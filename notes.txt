Prve stretnutie 11.11.2012
!! zapisovat, zapisovat – hned ten isty den (Misof, PG)

Geofrey Hinton – department of computer science Toronto, aplikacny
Randall O'Reilly – department of psychology Pittsburgh, prehlad, snazi sa simulovat ludsky mozog
v USA su takyto vedci vzdelany vseobecnym prehladom

ukazal som mu kaggle, 
andrew ng,
amsterdam – ML oriented

dal mi prefotene clanky – kde ich sakra mam? 

Oni sa na katedre zaoberaju nasadenim existujucich na ich problemy.
Ma jednu sikovnu doktorantku. 

Ma dobre skusenosti so spolupracou s teoretickymi informatikmi.

Moja predstava je trocha teorie – skumanie novych sieti, zistovanie domen, modifikovanie a znovuskumanie. Implementacie su (a preto nie su problem) 

!!Slubil som mu, ze o tyzden. dat si to ako buduco tyzdnovu prioritu

Neuronove siete su netrivialnymi zobrazeniami medzi domenami, je tam vela matiky a teorie a preto je tam co skumat / dokazovat / vymyslat

Mojim cielom bola praca na rozhrani vyskumu a implementacie pre vhodnu mnozinu praktickych problemov. 

24.10.2012
misof diplomovy seminar
prilis univerzalne kladivo (ked uz ine postupy zlyhavaju)
google ich ale pouzil na naozaj velkych datach a zacali funogvat dobre (napr. Youtube videa obsahujuce macky) 
najdolezitejsie je vediet dokazat vlastny prinos, ktory vyzadoval netrivialne mentalne usilie

7.11.2012
--15 stran v skuskovom s zakladnym prehladom tematiky – kvalita je na garantovi, prvy dojem na rovanovi 
UI – vela malych elementov co dava dokopy
formalizmus ak prinesie novy pohlad je super vec

14.11.2012 (Misof)
Ako pisat?
Latex
Vedieť dokázať vlastný prínos a netriviálne úsilie vedúce k nemu. 
Odvolávať sa na existujúcu lieteratúru. 
21.11.2012 nie

27.11.2012
https://www.ideals.illinois.edu/handle/2142/32512 – improve performance: Using feature construction to improve the performance of neural networks/1993: 149

predpoved ceny
data ming: harvesting reviews and grouping 
http://archive.ics.uci.edu/ml/ 


koncept:
zobrat existujuci model
potweakovat

12.12.2012
Hinton2012: 

Snazi riesit problem overfittingu na relativne malych problemoch

Je prevenciou co-adaptation a simuluje model viacerych neuronovych sieti natrenovanych na rovnaky problem

?Softmax output layer for computing the probabilities

-best result 160 errors from 10000 on test set
--130 with 50% dropout on hidden layer
--110 futher with 20% dropout on input set
Deep boltzman machine with dropout is the best result so far. 

?convolutional neural network
?pre-training extract useful features
?deep boltzman machine 
?hidden markov models
?viterbi algorithm
?max-pooling layer
?mixture of experts
?bayesian model averaging
?markov chain monte carlo
?mean net

We use the standard, stochastic gradient descent procedure for training the dropout neural 
networks on mini-batches of training cases, but we modify the penalty term that is normally 
used to prevent the weights from growing too large. Instead of penalizing the squared length 
(L2 norm) of the whole weight vector, we set an upper bound on the L2 norm of the incoming 
weight vector for each individual hidden unit. If a weight-update violates this constraint, we 
renormalize the weights of the hidden unit by division. Using a constraint rather than a penalty 
prevents weights from growing very large no matter how large the proposed weight-update is. 
This makes it possible to start with a very large learning rate which decays during learning, 
thus allowing a far more thorough search of the weight-space than methods that start with small 
weights and use a small learning rate. 

Performance on the test set can be greatly improved by
enhancing the training data with transformed images (3) or by wiring knowledge about spatial
transformations into a convolutional neural network (4) or by using generative pre-training to
extract useful features from the training images without using the labels (5).

Speech recognition systems use hidden Markov models (HMMs) to
deal with temporal variability and they need an acoustic model that determines how well a frame
of coefﬁcients extracted from the acoustic input ﬁts each possible state of each hidden Markov
model. 

For fully connected layers, dropout in all hidden layers works
better than dropout in only one hidden layer and more extreme probabilities tend to be worse,
which is why we have used 0.5 throughout this paper.

Zaujimave: For datasets in which the required input-output mapping has a
number of fairly different regimes, performance can probably be further improved by making
the dropout probabilities be a learned function of the input, thus creating a statistically efﬁcient
“mixture of experts” (13) in which there are combinatorially many experts, but each parameter
gets adapted on a large fraction of the training data.

Diplomovy seminar 12.12.2012

Too jitteri, dobra prezentacia, zabudol som meno skolitela, 
Super, ze to funguje. Zaujimava je este otazka preco to vlastne funguje – feature detectors. 

Prvy slide: nazov, meno, skolitel
Druhy slide : obsah
Dalsie slide-i: sucasny stav problematiky
Slide: co este porobime
5-7min = proof of work , uvod, 

Dat aktualny stav na web


================ Marec 2013 ====================
implementovany generec v C++ 

================ Jul 2013 ======================
Multilayer generec 42% - je to v slideoch

================ Oktober 2013 ==================
-implementovany BAL (zabudnute bias neuron, inicialicia divna)
-zrekonstruovane vysledky z clanku 
-meranie vlastnosti BAL (hidden layer distance, matrix similarity, ... ) /bal/data/... 
  reprezentacie na hidden absolutne rovnake (forward, backward), matice rozne: f_h_dist_0_m_sim_not_0.txt
-experimentovanie s init vahami aby sa zvysil predpoklad uspechu 
  data a grafy v /bal/data/...

-vyzera to tak, ze model pracuje lepsie ked init reprezentacie su dalej od seba (rapidminer/bal1_diagram) 
  -bipolarna sigmoida nepomaha
  -vybratie siete s veacsim hdist pomaha (62% - 70%) 
  
=============== November 2013 ===================
3 neurony na hidden ma:
 err sigma lambda
0.0 1.8 0.8 163/165 98.7878787878788%
0.0 1.8 1.0 160/162 98.76543209876543%
0.0 1.8 1.2 162/162 100.0%
0.0 1.8 1.4 156/158 98.73417721518987%
0.0 1.8 1.6 141/143 98.6013986013986%
0.0 1.8 1.8 149/154 96.75324675324676%
0.0 1.8 2.0 166/167 99.40119760479041%
0.0 1.8 2.2 160/162 98.76543209876543%
0.0 2.0 0.8 158/159 99.37106918238993%
0.0 2.0 1.0 150/154 97.40259740259741%
0.0 2.0 1.2 162/163 99.38650306748467%
0.0 2.0 1.4 164/166 98.79518072289156%
0.0 2.0 1.6 150/151 99.33774834437085%
0.0 2.0 1.8 152/154 98.7012987012987%
0.0 2.0 2.0 134/135 99.25925925925925%
0.0 2.0 2.2 160/163 98.15950920245399%
0.0 2.2 0.8 142/142 100.0%
0.0 2.2 1.0 151/153 98.69281045751634%
0.0 2.2 1.2 159/161 98.75776397515527%
0.0 2.2 1.4 156/156 100.0%
0.0 2.2 1.6 163/163 100.0%
0.0 2.2 1.8 155/156 99.35897435897436%
0.0 2.2 2.0 201/203 99.01477832512316%
0.0 2.2 2.2 134/136 98.52941176470588%
0.0 2.4 0.8 163/166 98.19277108433735%
0.0 2.4 1.0 163/164 99.39024390243902%
0.0 2.4 1.2 158/160 98.75%
0.0 2.4 1.4 143/144 99.30555555555556%
0.0 2.4 1.6 146/149 97.98657718120806%
0.0 2.4 1.8 162/164 98.78048780487805%
0.0 2.4 2.0 153/154 99.35064935064936%
0.0 2.4 2.2 149/155 96.12903225806451%
0.0 2.6 0.8 138/140 98.57142857142858%
0.0 2.6 1.0 140/141 99.29078014184397%
0.0 2.6 1.2 149/151 98.67549668874173%
0.0 2.6 1.4 165/167 98.80239520958084%
0.0 2.6 1.6 161/162 99.38271604938271%
0.0 2.6 1.8 161/162 99.38271604938271%
0.0 2.6 2.0 157/159 98.74213836477988%
0.0 2.6 2.2 130/134 97.01492537313433%
0.0 2.8 0.8 152/154 98.7012987012987%
0.0 2.8 1.0 168/171 98.24561403508771%
0.0 2.8 1.2 159/161 98.75776397515527%
0.0 2.8 1.4 136/140 97.14285714285714%
0.0 2.8 1.6 147/151 97.35099337748345%
0.0 2.8 1.8 167/168 99.40476190476191%
0.0 2.8 2.0 144/149 96.64429530201343%
0.0 2.8 2.2 148/153 96.73202614379085%
0.0 3.0 0.8 157/157 100.0%
0.0 3.0 1.0 168/172 97.67441860465115%
0.0 3.0 1.2 175/177 98.87005649717514%
0.0 3.0 1.4 150/152 98.68421052631578%
0.0 3.0 1.6 138/139 99.28057553956835%
0.0 3.0 1.8 151/151 100.0%
0.0 3.0 2.0 176/177 99.43502824858757%
0.0 3.0 2.2 158/163 96.93251533742331%
0.0 3.2 0.8 145/146 99.31506849315068%
0.0 3.2 1.0 154/162 95.06172839506173%
0.0 3.2 1.2 148/150 98.66666666666667%
0.0 3.2 1.4 148/154 96.1038961038961%
0.0 3.2 1.6 132/134 98.50746268656717%
0.0 3.2 1.8 166/167 99.40119760479041%
0.0 3.2 2.0 141/143 98.6013986013986%
0.0 3.2 2.2 144/149 96.64429530201343%
1.0 1.8 0.8 2/165 1.2121212121212122%
1.0 1.8 1.0 2/162 1.2345679012345678%
1.0 1.8 1.4 2/158 1.2658227848101267%
1.0 1.8 1.6 2/143 1.3986013986013985%
1.0 1.8 1.8 4/154 2.5974025974025974%
1.0 1.8 2.0 1/167 0.5988023952095809%
1.0 1.8 2.2 1/162 0.6172839506172839%
1.0 2.0 0.8 1/159 0.628930817610063%
1.0 2.0 1.0 3/154 1.948051948051948%
1.0 2.0 1.2 1/163 0.6134969325153374%
1.0 2.0 1.4 1/166 0.6024096385542169%
1.0 2.0 1.6 1/151 0.6622516556291391%
1.0 2.0 1.8 2/154 1.2987012987012987%
1.0 2.0 2.0 1/135 0.7407407407407408%
1.0 2.0 2.2 3/163 1.8404907975460123%
1.0 2.2 1.0 1/153 0.6535947712418301%
1.0 2.2 1.2 2/161 1.2422360248447204%
1.0 2.2 1.8 1/156 0.641025641025641%
1.0 2.2 2.0 2/203 0.9852216748768473%
1.0 2.2 2.2 2/136 1.4705882352941175%
1.0 2.4 0.8 3/166 1.8072289156626504%
1.0 2.4 1.0 1/164 0.6097560975609756%
1.0 2.4 1.2 2/160 1.25%
1.0 2.4 1.4 1/144 0.6944444444444444%
1.0 2.4 1.6 3/149 2.013422818791946%
1.0 2.4 1.8 2/164 1.2195121951219512%
1.0 2.4 2.0 1/154 0.6493506493506493%
1.0 2.4 2.2 4/155 2.5806451612903225%
1.0 2.6 0.8 2/140 1.4285714285714286%
1.0 2.6 1.0 1/141 0.7092198581560284%
1.0 2.6 1.2 1/151 0.6622516556291391%
1.0 2.6 1.4 2/167 1.1976047904191618%
1.0 2.6 1.6 1/162 0.6172839506172839%
1.0 2.6 1.8 1/162 0.6172839506172839%
1.0 2.6 2.0 2/159 1.257861635220126%
1.0 2.6 2.2 2/134 1.4925373134328357%
1.0 2.8 0.8 2/154 1.2987012987012987%
1.0 2.8 1.0 3/171 1.7543859649122806%
1.0 2.8 1.2 2/161 1.2422360248447204%
1.0 2.8 1.4 4/140 2.857142857142857%
1.0 2.8 1.6 4/151 2.6490066225165565%
1.0 2.8 1.8 1/168 0.5952380952380952%
1.0 2.8 2.0 5/149 3.3557046979865772%
1.0 2.8 2.2 5/153 3.2679738562091507%
1.0 3.0 1.0 4/172 2.3255813953488373%
1.0 3.0 1.2 2/177 1.1299435028248588%
1.0 3.0 1.4 2/152 1.3157894736842104%
1.0 3.0 2.0 1/177 0.5649717514124294%
1.0 3.0 2.2 4/163 2.4539877300613497%
1.0 3.2 0.8 1/146 0.684931506849315%
1.0 3.2 1.0 8/162 4.938271604938271%
1.0 3.2 1.2 2/150 1.3333333333333335%
1.0 3.2 1.4 5/154 3.2467532467532463%
1.0 3.2 1.6 2/134 1.4925373134328357%
1.0 3.2 1.8 1/167 0.5988023952095809%
1.0 3.2 2.0 2/143 1.3986013986013985%
1.0 3.2 2.2 4/149 2.684563758389262%
2.0 1.8 1.8 1/154 0.6493506493506493%
2.0 1.8 2.2 1/162 0.6172839506172839%
2.0 2.0 1.0 1/154 0.6493506493506493%
2.0 2.0 1.4 1/166 0.6024096385542169%
2.0 2.2 1.0 1/153 0.6535947712418301%
2.0 2.4 2.2 2/155 1.2903225806451613%
2.0 2.6 1.2 1/151 0.6622516556291391%
2.0 2.6 2.2 2/134 1.4925373134328357%
2.0 3.0 1.6 1/139 0.7194244604316548%
2.0 3.0 2.2 1/163 0.6134969325153374%
2.0 3.2 2.2 1/149 0.6711409395973155%
3.0 3.2 1.4 1/154 0.6493506493506493%

===CONVERGENCE EPSILON: 
	public static final int INIT_MAX_EPOCHS = 30000;
	public static final int INIT_RUNS = 1000; 
	public static final int INIT_CANDIDATES_COUNT = 100;
	
conv_eps=0.003 (ABS WEIGHT)
 err sigma lambda
0.0 2.3 0.7 655/1000 65.5
1.0 2.3 0.7 264/1000 26.400000000000002
2.0 2.3 0.7 81/1000 8.1
avg(epoch)=21416.3
avg(err)=0.426
avg(h_dist)=0.3377208768354886
avg(h_f_b_dist)=3.533469239792219E-8
avg(m_avg_w)=9.17389922619515
avg(m_sim)=0.6716337215384639
avg(first_second)=1191.0805852194187
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=1.2693376003173885E-4
	
conv_eps=0.01 (ABS WEIGHT)
 err sigma lambda
0.0 2.3 0.7 665/1000 66.5
1.0 2.3 0.7 282/1000 28.199999999999996
2.0 2.3 0.7 52/1000 5.2
3.0 2.3 0.7 1/1000 0.1
avg(epoch)=17174.083
avg(err)=0.389
avg(h_dist)=0.3402298305484247
avg(h_f_b_dist)=1.190306966621504E-6
avg(m_avg_w)=8.260697028353873
avg(m_sim)=0.6817923508697991
avg(first_second)=745.8397365477459
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=5.221918912727679E-4
	
conv_eps=0.03 (ABS WEIGHT)
 err sigma lambda
0.0 2.3 0.7 618/1000 61.8
1.0 2.3 0.7 303/1000 30.3
2.0 2.3 0.7 79/1000 7.9
avg(epoch)=12744.2
avg(err)=0.461
avg(h_dist)=0.34307441746879935
avg(h_f_b_dist)=3.730931210904762E-5
avg(m_avg_w)=6.559497108563506
avg(m_sim)=0.7192853722674895
avg(first_second)=461.7433056363167
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=0.003317823806944541

conv_eps=0.1 (ABS WEIGHT)
 err sigma lambda
0.0 2.3 0.7 91/1000 9.1
1.0 2.3 0.7 559/1000 55.900000000000006
2.0 2.3 0.7 278/1000 27.800000000000004
3.0 2.3 0.7 67/1000 6.7
4.0 2.3 0.7 5/1000 0.5
avg(epoch)=266.823
avg(err)=1.336
avg(h_dist)=0.34501134797359695
avg(h_f_b_dist)=0.004436944848110387
avg(m_avg_w)=2.4130610278020796
avg(m_sim)=1.08299640000091
avg(first_second)=15.024663218965912
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=0.06751217092624097


conv_eps=0.003 (REL WEIGHT)
 err sigma lambda
0.0 2.3 0.7 615/1000 61.5
1.0 2.3 0.7 296/1000 29.599999999999998
2.0 2.3 0.7 89/1000 8.9
avg(epoch)=11414.698
avg(err)=0.474
avg(h_dist)=0.3449922649825705
avg(h_f_b_dist)=8.357729465024385E-6
avg(m_avg_w)=6.3269007044953
avg(m_sim)=0.7109262416378718
avg(first_second)=342.3715979371622
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=0.0021038254278060134

