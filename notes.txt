Prve stretnutie 11.11.2012
!! zapisovat, zapisovat – hned ten isty den (Misof, PG)

Geofrey Hinton – department of computer science Toronto, aplikacny
Randall O'Reilly – department of psychology Pittsburgh, prehlad, snazi sa simulovat ludsky mozog
v USA su takyto vedci vzdelany vseobecnym prehladom

ukazal som mu kaggle, 
andrew ng,
amsterdam – ML oriented

dal mi prefotene clanky – kde ich sakra mam? 

Oni sa na katedre zaoberaju nasadenim existujucich na ich problemy.
Ma jednu sikovnu doktorantku. 

Ma dobre skusenosti so spolupracou s teoretickymi informatikmi.

Moja predstava je trocha teorie – skumanie novych sieti, zistovanie domen, modifikovanie a znovuskumanie. Implementacie su (a preto nie su problem) 

!!Slubil som mu, ze o tyzden. dat si to ako buduco tyzdnovu prioritu

Neuronove siete su netrivialnymi zobrazeniami medzi domenami, je tam vela matiky a teorie a preto je tam co skumat / dokazovat / vymyslat

Mojim cielom bola praca na rozhrani vyskumu a implementacie pre vhodnu mnozinu praktickych problemov. 

24.10.2012
misof diplomovy seminar
prilis univerzalne kladivo (ked uz ine postupy zlyhavaju)
google ich ale pouzil na naozaj velkych datach a zacali funogvat dobre (napr. Youtube videa obsahujuce macky) 
najdolezitejsie je vediet dokazat vlastny prinos, ktory vyzadoval netrivialne mentalne usilie

7.11.2012
--15 stran v skuskovom s zakladnym prehladom tematiky – kvalita je na garantovi, prvy dojem na rovanovi 
UI – vela malych elementov co dava dokopy
formalizmus ak prinesie novy pohlad je super vec

14.11.2012 (Misof)
Ako pisat?
Latex
Vedieť dokázať vlastný prínos a netriviálne úsilie vedúce k nemu. 
Odvolávať sa na existujúcu lieteratúru. 
21.11.2012 nie

27.11.2012
https://www.ideals.illinois.edu/handle/2142/32512 – improve performance: Using feature construction to improve the performance of neural networks/1993: 149

predpoved ceny
data ming: harvesting reviews and grouping 
http://archive.ics.uci.edu/ml/ 


koncept:
zobrat existujuci model
potweakovat

12.12.2012
Hinton2012: 

Snazi riesit problem overfittingu na relativne malych problemoch

Je prevenciou co-adaptation a simuluje model viacerych neuronovych sieti natrenovanych na rovnaky problem

?Softmax output layer for computing the probabilities

-best result 160 errors from 10000 on test set
--130 with 50% dropout on hidden layer
--110 futher with 20% dropout on input set
Deep boltzman machine with dropout is the best result so far. 

?convolutional neural network
?pre-training extract useful features
?deep boltzman machine 
?hidden markov models
?viterbi algorithm
?max-pooling layer
?mixture of experts
?bayesian model averaging
?markov chain monte carlo
?mean net

We use the standard, stochastic gradient descent procedure for training the dropout neural 
networks on mini-batches of training cases, but we modify the penalty term that is normally 
used to prevent the weights from growing too large. Instead of penalizing the squared length 
(L2 norm) of the whole weight vector, we set an upper bound on the L2 norm of the incoming 
weight vector for each individual hidden unit. If a weight-update violates this constraint, we 
renormalize the weights of the hidden unit by division. Using a constraint rather than a penalty 
prevents weights from growing very large no matter how large the proposed weight-update is. 
This makes it possible to start with a very large learning rate which decays during learning, 
thus allowing a far more thorough search of the weight-space than methods that start with small 
weights and use a small learning rate. 

Performance on the test set can be greatly improved by
enhancing the training data with transformed images (3) or by wiring knowledge about spatial
transformations into a convolutional neural network (4) or by using generative pre-training to
extract useful features from the training images without using the labels (5).

Speech recognition systems use hidden Markov models (HMMs) to
deal with temporal variability and they need an acoustic model that determines how well a frame
of coefﬁcients extracted from the acoustic input ﬁts each possible state of each hidden Markov
model. 

For fully connected layers, dropout in all hidden layers works
better than dropout in only one hidden layer and more extreme probabilities tend to be worse,
which is why we have used 0.5 throughout this paper.

Zaujimave: For datasets in which the required input-output mapping has a
number of fairly different regimes, performance can probably be further improved by making
the dropout probabilities be a learned function of the input, thus creating a statistically efﬁcient
“mixture of experts” (13) in which there are combinatorially many experts, but each parameter
gets adapted on a large fraction of the training data.

Diplomovy seminar 12.12.2012

Too jitteri, dobra prezentacia, zabudol som meno skolitela, 
Super, ze to funguje. Zaujimava je este otazka preco to vlastne funguje – feature detectors. 

Prvy slide: nazov, meno, skolitel
Druhy slide : obsah
Dalsie slide-i: sucasny stav problematiky
Slide: co este porobime
5-7min = proof of work , uvod, 

Dat aktualny stav na web


================ Marec 2013 ====================
implementovany generec v C++ 

================ Jul 2013 ======================
Multilayer generec 42% - je to v slideoch

================ Oktober 2013 ==================
-implementovany BAL (zabudnute bias neuron, inicialicia divna)
-zrekonstruovane vysledky z clanku 
-meranie vlastnosti BAL (hidden layer distance, matrix similarity, ... ) /bal/data/... 
  reprezentacie na hidden absolutne rovnake (forward, backward), matice rozne: f_h_dist_0_m_sim_not_0.txt
-experimentovanie s init vahami aby sa zvysil predpoklad uspechu 
  data a grafy v /bal/data/...

-vyzera to tak, ze model pracuje lepsie ked init reprezentacie su dalej od seba (rapidminer/bal1_diagram) 
  -bipolarna sigmoida nepomaha
  -vybratie siete s veacsim hdist pomaha (62% - 70%) 
  
=============== November 2013 ===================
3 neurony na hidden ma:
 err sigma lambda
0.0 1.8 0.8 163/165 98.7878787878788%
0.0 1.8 1.0 160/162 98.76543209876543%
0.0 1.8 1.2 162/162 100.0%
0.0 1.8 1.4 156/158 98.73417721518987%
0.0 1.8 1.6 141/143 98.6013986013986%
0.0 1.8 1.8 149/154 96.75324675324676%
0.0 1.8 2.0 166/167 99.40119760479041%
0.0 1.8 2.2 160/162 98.76543209876543%
0.0 2.0 0.8 158/159 99.37106918238993%
0.0 2.0 1.0 150/154 97.40259740259741%
0.0 2.0 1.2 162/163 99.38650306748467%
0.0 2.0 1.4 164/166 98.79518072289156%
0.0 2.0 1.6 150/151 99.33774834437085%
0.0 2.0 1.8 152/154 98.7012987012987%
0.0 2.0 2.0 134/135 99.25925925925925%
0.0 2.0 2.2 160/163 98.15950920245399%
0.0 2.2 0.8 142/142 100.0%
0.0 2.2 1.0 151/153 98.69281045751634%
0.0 2.2 1.2 159/161 98.75776397515527%
0.0 2.2 1.4 156/156 100.0%
0.0 2.2 1.6 163/163 100.0%
0.0 2.2 1.8 155/156 99.35897435897436%
0.0 2.2 2.0 201/203 99.01477832512316%
0.0 2.2 2.2 134/136 98.52941176470588%
0.0 2.4 0.8 163/166 98.19277108433735%
0.0 2.4 1.0 163/164 99.39024390243902%
0.0 2.4 1.2 158/160 98.75%
0.0 2.4 1.4 143/144 99.30555555555556%
0.0 2.4 1.6 146/149 97.98657718120806%
0.0 2.4 1.8 162/164 98.78048780487805%
0.0 2.4 2.0 153/154 99.35064935064936%
0.0 2.4 2.2 149/155 96.12903225806451%
0.0 2.6 0.8 138/140 98.57142857142858%
0.0 2.6 1.0 140/141 99.29078014184397%
0.0 2.6 1.2 149/151 98.67549668874173%
0.0 2.6 1.4 165/167 98.80239520958084%
0.0 2.6 1.6 161/162 99.38271604938271%
0.0 2.6 1.8 161/162 99.38271604938271%
0.0 2.6 2.0 157/159 98.74213836477988%
0.0 2.6 2.2 130/134 97.01492537313433%
0.0 2.8 0.8 152/154 98.7012987012987%
0.0 2.8 1.0 168/171 98.24561403508771%
0.0 2.8 1.2 159/161 98.75776397515527%
0.0 2.8 1.4 136/140 97.14285714285714%
0.0 2.8 1.6 147/151 97.35099337748345%
0.0 2.8 1.8 167/168 99.40476190476191%
0.0 2.8 2.0 144/149 96.64429530201343%
0.0 2.8 2.2 148/153 96.73202614379085%
0.0 3.0 0.8 157/157 100.0%
0.0 3.0 1.0 168/172 97.67441860465115%
0.0 3.0 1.2 175/177 98.87005649717514%
0.0 3.0 1.4 150/152 98.68421052631578%
0.0 3.0 1.6 138/139 99.28057553956835%
0.0 3.0 1.8 151/151 100.0%
0.0 3.0 2.0 176/177 99.43502824858757%
0.0 3.0 2.2 158/163 96.93251533742331%
0.0 3.2 0.8 145/146 99.31506849315068%
0.0 3.2 1.0 154/162 95.06172839506173%
0.0 3.2 1.2 148/150 98.66666666666667%
0.0 3.2 1.4 148/154 96.1038961038961%
0.0 3.2 1.6 132/134 98.50746268656717%
0.0 3.2 1.8 166/167 99.40119760479041%
0.0 3.2 2.0 141/143 98.6013986013986%
0.0 3.2 2.2 144/149 96.64429530201343%
1.0 1.8 0.8 2/165 1.2121212121212122%
1.0 1.8 1.0 2/162 1.2345679012345678%
1.0 1.8 1.4 2/158 1.2658227848101267%
1.0 1.8 1.6 2/143 1.3986013986013985%
1.0 1.8 1.8 4/154 2.5974025974025974%
1.0 1.8 2.0 1/167 0.5988023952095809%
1.0 1.8 2.2 1/162 0.6172839506172839%
1.0 2.0 0.8 1/159 0.628930817610063%
1.0 2.0 1.0 3/154 1.948051948051948%
1.0 2.0 1.2 1/163 0.6134969325153374%
1.0 2.0 1.4 1/166 0.6024096385542169%
1.0 2.0 1.6 1/151 0.6622516556291391%
1.0 2.0 1.8 2/154 1.2987012987012987%
1.0 2.0 2.0 1/135 0.7407407407407408%
1.0 2.0 2.2 3/163 1.8404907975460123%
1.0 2.2 1.0 1/153 0.6535947712418301%
1.0 2.2 1.2 2/161 1.2422360248447204%
1.0 2.2 1.8 1/156 0.641025641025641%
1.0 2.2 2.0 2/203 0.9852216748768473%
1.0 2.2 2.2 2/136 1.4705882352941175%
1.0 2.4 0.8 3/166 1.8072289156626504%
1.0 2.4 1.0 1/164 0.6097560975609756%
1.0 2.4 1.2 2/160 1.25%
1.0 2.4 1.4 1/144 0.6944444444444444%
1.0 2.4 1.6 3/149 2.013422818791946%
1.0 2.4 1.8 2/164 1.2195121951219512%
1.0 2.4 2.0 1/154 0.6493506493506493%
1.0 2.4 2.2 4/155 2.5806451612903225%
1.0 2.6 0.8 2/140 1.4285714285714286%
1.0 2.6 1.0 1/141 0.7092198581560284%
1.0 2.6 1.2 1/151 0.6622516556291391%
1.0 2.6 1.4 2/167 1.1976047904191618%
1.0 2.6 1.6 1/162 0.6172839506172839%
1.0 2.6 1.8 1/162 0.6172839506172839%
1.0 2.6 2.0 2/159 1.257861635220126%
1.0 2.6 2.2 2/134 1.4925373134328357%
1.0 2.8 0.8 2/154 1.2987012987012987%
1.0 2.8 1.0 3/171 1.7543859649122806%
1.0 2.8 1.2 2/161 1.2422360248447204%
1.0 2.8 1.4 4/140 2.857142857142857%
1.0 2.8 1.6 4/151 2.6490066225165565%
1.0 2.8 1.8 1/168 0.5952380952380952%
1.0 2.8 2.0 5/149 3.3557046979865772%
1.0 2.8 2.2 5/153 3.2679738562091507%
1.0 3.0 1.0 4/172 2.3255813953488373%
1.0 3.0 1.2 2/177 1.1299435028248588%
1.0 3.0 1.4 2/152 1.3157894736842104%
1.0 3.0 2.0 1/177 0.5649717514124294%
1.0 3.0 2.2 4/163 2.4539877300613497%
1.0 3.2 0.8 1/146 0.684931506849315%
1.0 3.2 1.0 8/162 4.938271604938271%
1.0 3.2 1.2 2/150 1.3333333333333335%
1.0 3.2 1.4 5/154 3.2467532467532463%
1.0 3.2 1.6 2/134 1.4925373134328357%
1.0 3.2 1.8 1/167 0.5988023952095809%
1.0 3.2 2.0 2/143 1.3986013986013985%
1.0 3.2 2.2 4/149 2.684563758389262%
2.0 1.8 1.8 1/154 0.6493506493506493%
2.0 1.8 2.2 1/162 0.6172839506172839%
2.0 2.0 1.0 1/154 0.6493506493506493%
2.0 2.0 1.4 1/166 0.6024096385542169%
2.0 2.2 1.0 1/153 0.6535947712418301%
2.0 2.4 2.2 2/155 1.2903225806451613%
2.0 2.6 1.2 1/151 0.6622516556291391%
2.0 2.6 2.2 2/134 1.4925373134328357%
2.0 3.0 1.6 1/139 0.7194244604316548%
2.0 3.0 2.2 1/163 0.6134969325153374%
2.0 3.2 2.2 1/149 0.6711409395973155%
3.0 3.2 1.4 1/154 0.6493506493506493%

===CONVERGENCE EPSILON: 
public static final int INIT_MAX_EPOCHS = 30000;
public static final int INIT_RUNS = 1000; 
public static final int INIT_CANDIDATES_COUNT = 100;
	
conv_eps=0.003 (ABS WEIGHT)
 err sigma lambda
0.0 2.3 0.7 655/1000 65.5
1.0 2.3 0.7 264/1000 26.400000000000002
2.0 2.3 0.7 81/1000 8.1
avg(epoch)=21416.3
avg(err)=0.426
avg(h_dist)=0.3377208768354886
 avg(h_f_b_dist)=3.533469239792219E-8
avg(m_avg_w)=9.17389922619515
avg(m_sim)=0.6716337215384639
avg(first_second)=1191.0805852194187
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=1.2693376003173885E-4
	
conv_eps=0.01 (ABS WEIGHT)
 err sigma lambda
0.0 2.3 0.7 665/1000 66.5
1.0 2.3 0.7 282/1000 28.199999999999996
2.0 2.3 0.7 52/1000 5.2
3.0 2.3 0.7 1/1000 0.1
avg(epoch)=17174.083
avg(err)=0.389
avg(h_dist)=0.3402298305484247
avg(h_f_b_dist)=1.190306966621504E-6
avg(m_avg_w)=8.260697028353873
avg(m_sim)=0.6817923508697991
avg(first_second)=745.8397365477459
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=5.221918912727679E-4
	
conv_eps=0.03 (ABS WEIGHT)
 err sigma lambda
0.0 2.3 0.7 618/1000 61.8
1.0 2.3 0.7 303/1000 30.3
2.0 2.3 0.7 79/1000 7.9
avg(epoch)=12744.2
avg(err)=0.461
avg(h_dist)=0.34307441746879935
avg(h_f_b_dist)=3.730931210904762E-5
avg(m_avg_w)=6.559497108563506
avg(m_sim)=0.7192853722674895
avg(first_second)=461.7433056363167
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=0.003317823806944541

conv_eps=0.1 (ABS WEIGHT)
 err sigma lambda
0.0 2.3 0.7 91/1000 9.1
1.0 2.3 0.7 559/1000 55.900000000000006
2.0 2.3 0.7 278/1000 27.800000000000004
3.0 2.3 0.7 67/1000 6.7
4.0 2.3 0.7 5/1000 0.5
avg(epoch)=266.823
avg(err)=1.336
avg(h_dist)=0.34501134797359695
avg(h_f_b_dist)=0.004436944848110387
avg(m_avg_w)=2.4130610278020796
avg(m_sim)=1.08299640000091
avg(first_second)=15.024663218965912
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=0.06751217092624097


conv_eps=0.003 (REL WEIGHT)
 err sigma lambda
0.0 2.3 0.7 615/1000 61.5
1.0 2.3 0.7 296/1000 29.599999999999998
2.0 2.3 0.7 89/1000 8.9
avg(epoch)=11414.698
avg(err)=0.474
avg(h_dist)=0.3449922649825705
avg(h_f_b_dist)=8.357729465024385E-6
avg(m_avg_w)=6.3269007044953
avg(m_sim)=0.7109262416378718
avg(first_second)=342.3715979371622
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=0.0021038254278060134

==========MOMENTUM 
=========================================================
err sigma lambda momentumsuccess sample_ratio

0.0 2.3 0.7 -0.0010 67.40412979351032 457/678
0.0 2.3 0.7 -0.0030 68.24712643678161 475/696
0.0 2.3 0.7 -0.01 66.51583710407239 441/663
0.0 2.3 0.7 -0.03 68.50746268656717 459/670
0.0 2.3 0.7 -0.1 66.31259484066769 437/659
0.0 2.3 0.7 -0.3 68.51851851851852 444/648
0.0 2.3 0.7 0.0 67.64705882352942 460/680
0.0 2.3 0.7 0.0010 66.57223796033995 470/706
0.0 2.3 0.7 0.0030 68.04123711340206 462/679
0.0 2.3 0.7 0.01 70.10785824345146 455/649
0.0 2.3 0.7 0.03 66.81481481481481 451/675
0.0 2.3 0.7 0.1 67.0605612998523 454/677
0.0 2.3 0.7 0.3 69.24198250728864 475/686
0.0 2.3 0.7 1.0 67.26384364820846 413/614
1.0 2.3 0.7 -0.0010 25.958702064896755 176/678
1.0 2.3 0.7 -0.0030 24.137931034482758 168/696
1.0 2.3 0.7 -0.01 26.546003016591253 176/663
1.0 2.3 0.7 -0.03 24.029850746268657 161/670
1.0 2.3 0.7 -0.1 26.403641881638844 174/659
1.0 2.3 0.7 -0.3 25.462962962962965 165/648
1.0 2.3 0.7 0.0 26.61764705882353 181/680
1.0 2.3 0.7 0.0010 25.35410764872521 179/706
1.0 2.3 0.7 0.0030 25.773195876288657 175/679
1.0 2.3 0.7 0.01 23.728813559322035 154/649
1.0 2.3 0.7 0.03 25.48148148148148 172/675
1.0 2.3 0.7 0.1 27.474150664697195 186/677
1.0 2.3 0.7 0.3 24.34402332361516 167/686
1.0 2.3 0.7 1.0 27.36156351791531 168/614
10.0 2.3 0.7 -1.0 8.870967741935484 55/620
11.0 2.3 0.7 -1.0 5.483870967741936 34/620
12.0 2.3 0.7 -1.0 3.225806451612903 20/620
13.0 2.3 0.7 -1.0 0.4838709677419355 3/620
2.0 2.3 0.7 -0.0010 6.637168141592921 45/678
2.0 2.3 0.7 -0.0030 7.614942528735632 53/696
2.0 2.3 0.7 -0.01 6.93815987933635 46/663
2.0 2.3 0.7 -0.03 7.313432835820896 49/670
2.0 2.3 0.7 -0.1 7.283763277693475 48/659
2.0 2.3 0.7 -0.3 6.018518518518518 39/648
2.0 2.3 0.7 -1.0 0.3225806451612903 2/620
2.0 2.3 0.7 0.0 5.735294117647059 39/680
2.0 2.3 0.7 0.0010 7.790368271954675 55/706
2.0 2.3 0.7 0.0030 6.185567010309279 42/679
2.0 2.3 0.7 0.01 6.163328197226503 40/649
2.0 2.3 0.7 0.03 7.555555555555555 51/675
2.0 2.3 0.7 0.1 5.465288035450517 37/677
2.0 2.3 0.7 0.3 6.41399416909621 44/686
2.0 2.3 0.7 1.0 5.374592833876222 33/614
3.0 2.3 0.7 -0.03 0.1492537313432836 1/670
3.0 2.3 0.7 -1.0 1.129032258064516 7/620
3.0 2.3 0.7 0.0010 0.28328611898017 2/706
3.0 2.3 0.7 0.03 0.14814814814814814 1/675
4.0 2.3 0.7 -1.0 2.258064516129032 14/620
5.0 2.3 0.7 -1.0 7.903225806451612 49/620
6.0 2.3 0.7 -1.0 15.806451612903224 98/620
7.0 2.3 0.7 -1.0 19.193548387096772 119/620
8.0 2.3 0.7 -1.0 22.258064516129032 138/620
9.0 2.3 0.7 -1.0 13.064516129032258 81/620
avg(epoch)=30000.0
avg(err)=0.8439
avg(h_dist)=0.3499103246224569
avg(h_f_b_dist)=0.041112550125614586
avg(m_avg_w)=9.636579583625638
avg(m_sim)=0.792297487186741
avg(first_second)=2673.609889300811
avg(sigma)=2.299999999999577
avg(lambda)=0.6999999999998807
avg(o_f_b_dist)=0.056782865777391126
avg(momentum)=7.186999999999984E-4

============================================== 30.10.2013 =================
public static final double NORMAL_DISTRIBUTION_SPAN = 15; 

	public static final String INPUT_FILEPATH = "auto4.in"; 
	public static final String OUTPUT_FILEPATH = "auto4.in"; 
	public static final int INIT_HIDDEN_LAYER_SIZE = 2; 
	public static final int INIT_MAX_EPOCHS = 30000;
	public static final int INIT_RUNS = 10000; 
	public static final double INIT_NORMAL_DISTRIBUTION_MU = 0; 
	public static final double TRY_NORMAL_DISTRIBUTION_SIGMA[] = {0.1, 0.3, 1, 1.3, 1.7, 2, 2.3}; 
	public static final double TRY_LAMBDA[] = {0.001, 0.003, 0.01, 0.03, 0.1, 0.3}; 
	public static final double TRY_NOISE_SPAN[] = {0.0, 0.003, 0.01, 0.03, 0.1, 0.3}; 

- 0.328 * h_dist
+ 0.140 * h_f_b_dist
- 0.100 * m_avg_w
+ 0.019 * m_sim
+ 0.000 * first_second
- 0.127 * sigma
+ 2.912 * noise_span
+ 3.610

============================================================

 err sigma lambda
0.0 0.85 0.03 83/388 21.391752577319586\%
0.0 0.85 0.1 147/411 35.76642335766424\%
0.0 0.85 0.2 168/393 42.74809160305343\%
0.0 0.85 0.3 198/402 49.25373134328358\%
0.0 0.85 0.5 215/398 54.020100502512555\%
0.0 1.0 0.03 123/410 30.0\%
0.0 1.0 0.1 159/397 40.050377833753146\%
0.0 1.0 0.2 191/390 48.97435897435897\%
0.0 1.0 0.3 193/391 49.36061381074169\%
0.0 1.0 0.5 217/409 53.0562347188264\%
0.0 1.15 0.03 135/420 32.142857142857146\%
0.0 1.15 0.1 175/402 43.53233830845771\%
0.0 1.15 0.2 205/398 51.507537688442206\%
0.0 1.15 0.3 202/391 51.66240409207161\%
0.0 1.15 0.5 215/387 55.55555555555556\%
0.0 1.3 0.03 134/406 33.004926108374384\%
0.0 1.3 0.1 188/431 43.619489559164734\%
0.0 1.3 0.2 205/396 51.76767676767676\%
0.0 1.3 0.3 230/427 53.86416861826698\%
0.0 1.3 0.5 231/401 57.605985037406484\%
0.0 1.5 0.03 149/403 36.972704714640194\%
0.0 1.5 0.1 182/380 47.89473684210526\%
0.0 1.5 0.2 203/407 49.877149877149876\%
0.0 1.5 0.3 201/374 53.7433155080214\%
0.0 1.5 0.5 237/388 61.08247422680413\%
1.0 0.85 0.03 222/388 57.21649484536082\%
1.0 0.85 0.1 230/411 55.961070559610704\%
1.0 0.85 0.2 194/393 49.36386768447837\%
1.0 0.85 0.3 180/402 44.776119402985074\%
1.0 0.85 0.5 158/398 39.698492462311556\%
1.0 1.0 0.03 224/410 54.63414634146342\%
1.0 1.0 0.1 203/397 51.13350125944584\%
1.0 1.0 0.2 177/390 45.38461538461539\%
1.0 1.0 0.3 166/391 42.45524296675192\%
1.0 1.0 0.5 163/409 39.85330073349633\%
1.0 1.15 0.03 224/420 53.333333333333336\%
1.0 1.15 0.1 199/402 49.50248756218906\%
1.0 1.15 0.2 171/398 42.96482412060301\%
1.0 1.15 0.3 158/391 40.40920716112532\%
1.0 1.15 0.5 142/387 36.69250645994832\%
1.0 1.3 0.03 215/406 52.95566502463054\%
1.0 1.3 0.1 204/431 47.33178654292343\%
1.0 1.3 0.2 167/396 42.17171717171717\%
1.0 1.3 0.3 171/427 40.04683840749414\%
1.0 1.3 0.5 138/401 34.413965087281795\%
1.0 1.5 0.03 201/403 49.87593052109181\%
1.0 1.5 0.1 171/380 45.0\%
1.0 1.5 0.2 179/407 43.980343980343974\%
1.0 1.5 0.3 152/374 40.64171122994652\%
1.0 1.5 0.5 124/388 31.958762886597935\%
2.0 0.85 0.03 74/388 19.072164948453608\%
2.0 0.85 0.1 32/411 7.785888077858881\%
2.0 0.85 0.2 31/393 7.888040712468193\%
2.0 0.85 0.3 23/402 5.721393034825871\%
2.0 0.85 0.5 25/398 6.281407035175879\%
2.0 1.0 0.03 55/410 13.414634146341465\%
2.0 1.0 0.1 33/397 8.312342569269521\%
2.0 1.0 0.2 22/390 5.641025641025641\%
2.0 1.0 0.3 30/391 7.672634271099745\%
2.0 1.0 0.5 28/409 6.845965770171149\%
2.0 1.15 0.03 54/420 12.857142857142856\%
2.0 1.15 0.1 27/402 6.7164179104477615\%
2.0 1.15 0.2 22/398 5.527638190954774\%
2.0 1.15 0.3 31/391 7.928388746803069\%
2.0 1.15 0.5 30/387 7.751937984496124\%
2.0 1.3 0.03 53/406 13.054187192118228\%
2.0 1.3 0.1 37/431 8.584686774941995\%
2.0 1.3 0.2 24/396 6.0606060606060606\%
2.0 1.3 0.3 26/427 6.0889929742388755\%
2.0 1.3 0.5 31/401 7.73067331670823\%
2.0 1.5 0.03 51/403 12.655086848635236\%
2.0 1.5 0.1 27/380 7.105263157894736\%
2.0 1.5 0.2 23/407 5.651105651105651\%
2.0 1.5 0.3 20/374 5.347593582887701\%
2.0 1.5 0.5 27/388 6.958762886597938\%
3.0 0.85 0.03 9/388 2.3195876288659796\%
3.0 0.85 0.1 2/411 0.48661800486618007\%
3.0 0.85 0.3 1/402 0.24875621890547264\%
3.0 1.0 0.03 8/410 1.951219512195122\%
3.0 1.0 0.1 2/397 0.5037783375314862\%
3.0 1.0 0.3 2/391 0.5115089514066496\%
3.0 1.0 0.5 1/409 0.24449877750611246\%
3.0 1.15 0.03 7/420 1.6666666666666667\%
3.0 1.15 0.1 1/402 0.24875621890547264\%
3.0 1.3 0.03 4/406 0.9852216748768473\%
3.0 1.3 0.1 1/431 0.23201856148491878\%
3.0 1.3 0.5 1/401 0.24937655860349126\%
3.0 1.5 0.03 2/403 0.49627791563275436\%
3.0 1.5 0.2 2/407 0.4914004914004914\%
3.0 1.5 0.3 1/374 0.267379679144385\%
4.0 1.3 0.1 1/431 0.23201856148491878\%


=========================================================
GNUPLOT: splot "./bal/data/hdist_stats_0.csv" using 1:2:3 with lines lt rgb "blue"
sample file: 
#sigma  lambda  success
1       0.001   0.4
1       0.003   8.75
1       0.01    30.8
1       0.03    47.03389830508475
1       0.1     54.112554112554115
1       0.3     56.97211155378486
1       1       57.3394495412844
1.3     0.001   0.8264462809917356
1.3     0.003   14.354066985645932
1.3     0.01    32.5


 err sigma lambda
0.0 1.5 0.3 66/131 50.38167938931297%
0.0 1.5 0.5 65/110 59.09090909090909%
0.0 1.5 0.7 72/122 59.01639344262295%
0.0 1.5 0.9 61/100 61.0%
0.0 1.5 1.1 75/124 60.483870967741936%
0.0 1.5 1.3 57/113 50.442477876106196%
0.0 1.5 1.5 65/114 57.01754385964912%
0.0 1.7 0.3 63/128 49.21875%
0.0 1.7 0.5 82/129 63.565891472868216%
0.0 1.7 0.7 63/122 51.63934426229508%
0.0 1.7 0.9 87/144 60.416666666666664%
0.0 1.7 1.1 60/118 50.847457627118644%
0.0 1.7 1.3 67/116 57.758620689655174%
0.0 1.7 1.5 62/118 52.54237288135594%
0.0 1.9 0.3 60/99 60.60606060606061%
0.0 1.9 0.5 69/116 59.48275862068966%
0.0 1.9 0.7 65/115 56.52173913043478%
0.0 1.9 0.9 68/112 60.71428571428571%
0.0 1.9 1.1 75/131 57.25190839694656%
0.0 1.9 1.3 79/136 58.08823529411765%
0.0 1.9 1.5 78/128 60.9375%
0.0 2.1 0.3 57/109 52.293577981651374%
0.0 2.1 0.5 60/98 61.224489795918366%
0.0 2.1 0.7 79/123 64.22764227642277%
0.0 2.1 0.9 65/110 59.09090909090909%
0.0 2.1 1.1 95/146 65.06849315068493%
0.0 2.1 1.3 71/109 65.13761467889908%
0.0 2.1 1.5 81/128 63.28125%
0.0 2.3 0.3 69/120 57.49999999999999%
0.0 2.3 0.5 73/133 54.88721804511278%
0.0 2.3 0.7 67/95 70.52631578947368%
0.0 2.3 0.9 68/117 58.119658119658126%
0.0 2.3 1.1 71/138 51.449275362318836%
0.0 2.3 1.3 80/136 58.82352941176471%
0.0 2.3 1.5 77/120 64.16666666666667%
0.0 2.5 0.3 53/110 48.18181818181818%
0.0 2.5 0.5 75/121 61.98347107438017%
0.0 2.5 0.7 63/109 57.798165137614674%
0.0 2.5 0.9 50/94 53.191489361702125%
0.0 2.5 1.1 62/103 60.19417475728155%
0.0 2.5 1.3 66/132 50.0%
0.0 2.5 1.5 75/123 60.97560975609756%
1.0 1.5 0.3 55/131 41.98473282442748%
1.0 1.5 0.5 35/110 31.818181818181817%
1.0 1.5 0.7 40/122 32.78688524590164%
1.0 1.5 0.9 28/100 28.000000000000004%
1.0 1.5 1.1 35/124 28.225806451612907%
1.0 1.5 1.3 41/113 36.283185840707965%
1.0 1.5 1.5 42/114 36.84210526315789%
1.0 1.7 0.3 57/128 44.53125%
1.0 1.7 0.5 40/129 31.007751937984494%
1.0 1.7 0.7 43/122 35.24590163934426%
1.0 1.7 0.9 44/144 30.555555555555557%
1.0 1.7 1.1 45/118 38.13559322033898%
1.0 1.7 1.3 38/116 32.758620689655174%
1.0 1.7 1.5 46/118 38.983050847457626%
1.0 1.9 0.3 27/99 27.27272727272727%
1.0 1.9 0.5 36/116 31.03448275862069%
1.0 1.9 0.7 39/115 33.91304347826087%
1.0 1.9 0.9 35/112 31.25%
1.0 1.9 1.1 44/131 33.587786259541986%
1.0 1.9 1.3 38/136 27.941176470588236%
1.0 1.9 1.5 37/128 28.90625%
1.0 2.1 0.3 44/109 40.36697247706422%
1.0 2.1 0.5 28/98 28.57142857142857%
1.0 2.1 0.7 32/123 26.01626016260163%
1.0 2.1 0.9 38/110 34.54545454545455%
1.0 2.1 1.1 42/146 28.767123287671232%
1.0 2.1 1.3 27/109 24.770642201834864%
1.0 2.1 1.5 31/128 24.21875%
1.0 2.3 0.3 38/120 31.666666666666664%
1.0 2.3 0.5 46/133 34.58646616541353%
1.0 2.3 0.7 24/95 25.263157894736842%
1.0 2.3 0.9 41/117 35.04273504273504%
1.0 2.3 1.1 52/138 37.68115942028986%
1.0 2.3 1.3 41/136 30.14705882352941%
1.0 2.3 1.5 33/120 27.500000000000004%
1.0 2.5 0.3 48/110 43.63636363636363%
1.0 2.5 0.5 36/121 29.75206611570248%
1.0 2.5 0.7 33/109 30.275229357798167%
1.0 2.5 0.9 33/94 35.1063829787234%
1.0 2.5 1.1 31/103 30.097087378640776%
1.0 2.5 1.3 54/132 40.909090909090914%
1.0 2.5 1.5 39/123 31.70731707317073%
2.0 1.5 0.3 10/131 7.633587786259542%
2.0 1.5 0.5 10/110 9.090909090909092%
2.0 1.5 0.7 10/122 8.19672131147541%
2.0 1.5 0.9 11/100 11.0%
2.0 1.5 1.1 13/124 10.483870967741936%
2.0 1.5 1.3 15/113 13.274336283185843%
2.0 1.5 1.5 7/114 6.140350877192982%
2.0 1.7 0.3 8/128 6.25%
2.0 1.7 0.5 7/129 5.426356589147287%
2.0 1.7 0.7 16/122 13.114754098360656%
2.0 1.7 0.9 12/144 8.333333333333332%
2.0 1.7 1.1 13/118 11.016949152542372%
2.0 1.7 1.3 11/116 9.482758620689655%
2.0 1.7 1.5 10/118 8.47457627118644%
2.0 1.9 0.3 12/99 12.121212121212121%
2.0 1.9 0.5 11/116 9.482758620689655%
2.0 1.9 0.7 11/115 9.565217391304348%
2.0 1.9 0.9 9/112 8.035714285714286%
2.0 1.9 1.1 11/131 8.396946564885496%
2.0 1.9 1.3 19/136 13.970588235294118%
2.0 1.9 1.5 12/128 9.375%
2.0 2.1 0.3 8/109 7.339449541284404%
2.0 2.1 0.5 10/98 10.204081632653061%
2.0 2.1 0.7 12/123 9.75609756097561%
2.0 2.1 0.9 7/110 6.363636363636363%
2.0 2.1 1.1 9/146 6.164383561643835%
2.0 2.1 1.3 10/109 9.174311926605505%
2.0 2.1 1.5 14/128 10.9375%
2.0 2.3 0.3 13/120 10.833333333333334%
2.0 2.3 0.5 13/133 9.774436090225564%
2.0 2.3 0.7 4/95 4.2105263157894735%
2.0 2.3 0.9 8/117 6.837606837606838%
2.0 2.3 1.1 14/138 10.144927536231885%
2.0 2.3 1.3 15/136 11.029411764705882%
2.0 2.3 1.5 10/120 8.333333333333332%
2.0 2.5 0.3 7/110 6.363636363636363%
2.0 2.5 0.5 8/121 6.6115702479338845%
2.0 2.5 0.7 12/109 11.009174311926607%
2.0 2.5 0.9 10/94 10.638297872340425%
2.0 2.5 1.1 10/103 9.70873786407767%
2.0 2.5 1.3 11/132 8.333333333333332%
2.0 2.5 1.5 7/123 5.691056910569105%
3.0 1.5 1.1 1/124 0.8064516129032258%
3.0 1.7 0.9 1/144 0.6944444444444444%
3.0 1.9 1.1 1/131 0.7633587786259541%
3.0 1.9 1.5 1/128 0.78125%
3.0 2.1 1.3 1/109 0.9174311926605505%
3.0 2.1 1.5 2/128 1.5625%
3.0 2.3 0.5 1/133 0.7518796992481203%
3.0 2.3 1.1 1/138 0.7246376811594203%
3.0 2.5 0.3 1/110 0.9090909090909091%
3.0 2.5 0.5 2/121 1.6528925619834711%
3.0 2.5 0.7 1/109 0.9174311926605505%
3.0 2.5 0.9 1/94 1.0638297872340425%
3.0 2.5 1.3 1/132 0.7575757575757576%
3.0 2.5 1.5 2/123 1.6260162601626018%
4.0 2.5 0.3 1/110 0.9090909090909091%


================= 09-12-2013 =========================
Convergence which depends on average weight change does not work. 
Max diff implemented. Convergence with similar results on average of 900 epochs (TODO measure more exactly)/

======== MORE-DIMENSIONAL STUFF =====================
		MEASURE_IS = true; 
		MEASURE_SAVE_AFTER_EACH_RUN = false; 
		MEASURE_RECORD_EACH = 1000;

		INPUT_FILEPATH = "k3.in"; 
		OUTPUT_FILEPATH = "k3.out"; 
		INIT_HIDDEN_LAYER_SIZE = 3; 

		CONVERGENCE_WEIGHT_EPSILON = 0.0; 
		
		CONVERGENCE_NO_CHANGE_FOR = 10; 
		CONVERGENCE_NO_CHANGE_EPSILON = 0.001;
		INIT_MAX_EPOCHS = 5000;

		INIT_RUNS = 1000; 
		INIT_CANDIDATES_COUNT = 1;

		PRINT_NETWORK_IS = true; 

		TRY_NORMAL_DISTRIBUTION_SIGMA = new double[] {2.3}; 
		TRY_LAMBDA = new double[] {0.7}; 
		TRY_MOMENTUM = new double[] {0.0}; 
		
=========== HIDDEN SIZE=3 =================
err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.0 79.5 795/1000
10.0 2.3 0.7 0.0 0.1 1/1000
12.0 2.3 0.7 0.0 4.7 47/1000
14.0 2.3 0.7 0.0 0.1 1/1000
16.0 2.3 0.7 0.0 3.4000000000000004 34/1000
20.0 2.3 0.7 0.0 0.8 8/1000
24.0 2.3 0.7 0.0 1.7000000000000002 17/1000
28.0 2.3 0.7 0.0 0.5 5/1000
3.0 2.3 0.7 0.0 0.2 2/1000
32.0 2.3 0.7 0.0 0.2 2/1000
4.0 2.3 0.7 0.0 3.2 32/1000
40.0 2.3 0.7 0.0 0.2 2/1000
7.0 2.3 0.7 0.0 0.1 1/1000
8.0 2.3 0.7 0.0 5.3 53/1000
avg(epoch)=956.739
avg(err)=2.549
avg(h_dist)=0.4284663233176611
avg(h_f_b_dist)=0.001433903013638078
avg(m_avg_w)=4.118199611625134
avg(m_sim)=8.968924274269243
avg(first_second)=16.816534206233154
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=1.6954451983027756
avg(momentum)=0.0

=========== HIDDEN SIZE=4 =================

err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.0 94.89999999999999 949/1000
12.0 2.3 0.7 0.0 1.7000000000000002 17/1000
13.0 2.3 0.7 0.0 0.1 1/1000
16.0 2.3 0.7 0.0 0.7000000000000001 7/1000
24.0 2.3 0.7 0.0 0.7000000000000001 7/1000
28.0 2.3 0.7 0.0 0.1 1/1000
4.0 2.3 0.7 0.0 0.3 3/1000
8.0 2.3 0.7 0.0 1.5 15/1000
avg(epoch)=601.497
avg(err)=0.657
avg(h_dist)=0.5135170749928729
avg(h_f_b_dist)=0.003096212365949686
avg(m_avg_w)=3.0549479613555426
avg(m_sim)=6.275420587570562
avg(first_second)=16.24367030549711
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=1.7258244262998257
avg(momentum)=0.0

=========== HIDDEN SIZE=5 =================

err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.0 98.5 985/1000
1.0 2.3 0.7 0.0 0.1 1/1000
10.0 2.3 0.7 0.0 0.1 1/1000
12.0 2.3 0.7 0.0 0.2 2/1000
13.0 2.3 0.7 0.0 0.1 1/1000
16.0 2.3 0.7 0.0 0.1 1/1000
2.0 2.3 0.7 0.0 0.1 1/1000
24.0 2.3 0.7 0.0 0.2 2/1000
4.0 2.3 0.7 0.0 0.3 3/1000
8.0 2.3 0.7 0.0 0.3 3/1000
avg(epoch)=410.62
avg(err)=0.15
avg(h_dist)=0.5745057716392946
avg(h_f_b_dist)=0.004810628841391281
avg(m_avg_w)=2.5324437064360206
avg(m_sim)=4.923161220350142
avg(first_second)=16.075237073653938
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=1.7378024156440908
avg(momentum)=0.0


=========== HIDDEN SIZE=6 =================

err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.0 99.5 995/1000
12.0 2.3 0.7 0.0 0.1 1/1000
16.0 2.3 0.7 0.0 0.2 2/1000
4.0 2.3 0.7 0.0 0.1 1/1000
8.0 2.3 0.7 0.0 0.1 1/1000
avg(epoch)=303.967
avg(err)=0.056
avg(h_dist)=0.6458562318373903
avg(h_f_b_dist)=0.006696401702940912
avg(m_avg_w)=2.1818656076530747
avg(m_sim)=4.079264257960105
avg(first_second)=16.05796072173848
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=1.7416806433833738
avg(momentum)=0.0

=========== HIDDEN SIZE=7 =================

err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.0 99.8 998/1000
10.0 2.3 0.7 0.0 0.1 1/1000
4.0 2.3 0.7 0.0 0.1 1/1000
avg(epoch)=247.087
avg(err)=0.014
avg(h_dist)=0.7042485805082631
avg(h_f_b_dist)=0.009082904298846018
avg(m_avg_w)=1.9839182199103171
avg(m_sim)=3.5810010763993825
avg(first_second)=16.04953024179868
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=1.7434258830153593
avg(momentum)=0.0

=========== HIDDEN SIZE=8 =================

err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.0 99.9 999/1000
4.0 2.3 0.7 0.0 0.1 1/1000
avg(epoch)=207.157
avg(err)=0.0040
avg(h_dist)=0.7565836343904825
avg(h_f_b_dist)=0.01119194262780024
avg(m_avg_w)=1.849205567627445
avg(m_sim)=3.2458319255933024
avg(first_second)=16.041103921687853
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=1.7435353993910458
avg(momentum)=0.0


=========== HIDDEN SIZE=9 =================

err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.0 100.0 1000/1000
avg(epoch)=185.197
avg(err)=0.0
avg(h_dist)=0.8073814583944268
avg(h_f_b_dist)=0.013189858683857341
avg(m_avg_w)=1.7555708577329958
avg(m_sim)=3.0310904279720203
avg(first_second)=16.03765067877136
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=1.743996359435468
avg(momentum)=0.0

=========== HIDDEN SIZE=10 =================

err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.0 100.0 1000/1000
avg(epoch)=164.982
avg(err)=0.0
avg(h_dist)=0.8611878976280017
avg(h_f_b_dist)=0.015753765441862518
avg(m_avg_w)=1.6859185923859807
avg(m_sim)=2.867064838367571
avg(first_second)=16.033555213297653
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=1.7466494363906515
avg(momentum)=0.0

=========== HIDDEN SIZE=11 =================

err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.0 100.0 1000/1000
avg(epoch)=147.927
avg(err)=0.0
avg(h_dist)=0.9045586193984213
avg(h_f_b_dist)=0.018396550045545114
avg(m_avg_w)=1.6349275275251605
avg(m_sim)=2.7458523470512652
avg(first_second)=16.030847684383325
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=1.7479673417663801
avg(momentum)=0.0

=========== HIDDEN SIZE=12 =================

err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.0 100.0 1000/1000
avg(epoch)=138.848
avg(err)=0.0
avg(h_dist)=0.9471818347636051
avg(h_f_b_dist)=0.02031140691623281
avg(m_avg_w)=1.5997988709857414
avg(m_sim)=2.660522928606756
avg(first_second)=16.030766939353732
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=1.7503840335932033
avg(momentum)=0.0

=========== HIDDEN SIZE=13 =================

err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.0 100.0 1000/1000
avg(epoch)=130.816
avg(err)=0.0
avg(h_dist)=0.9873039504065871
avg(h_f_b_dist)=0.022116791057825578
avg(m_avg_w)=1.5660344127061239
avg(m_sim)=2.5939002974820426
avg(first_second)=16.029348701690783
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=1.7527485904242674
avg(momentum)=0.0

=========== HIDDEN SIZE=14 =================

err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.0 100.0 1000/1000
avg(epoch)=119.83
avg(err)=0.0
avg(h_dist)=1.0321880386525344
avg(h_f_b_dist)=0.02511997582657276
avg(m_avg_w)=1.538464705880271
avg(m_sim)=2.5314972594130145
avg(first_second)=16.02742213450508
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=1.7558119843146454
avg(momentum)=0.0

=========== HIDDEN SIZE=15 =================

err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.0 100.0 1000/1000
avg(epoch)=116.484
avg(err)=0.0
avg(h_dist)=1.064706435083704
avg(h_f_b_dist)=0.02674199876019537
avg(m_avg_w)=1.5234933115862725
avg(m_sim)=2.498672916444305
avg(first_second)=16.025714388437642
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=1.7589998704589265
avg(momentum)=0.0

=========== HIDDEN SIZE=16 =================

0.0 2.3 0.7 0.0 100.0 1000/1000
avg(epoch)=108.227
avg(err)=0.0
avg(h_dist)=1.1046212031191365
avg(h_f_b_dist)=0.029585609889805232
avg(m_avg_w)=1.5037866439274676
avg(m_sim)=2.45579992947505
avg(first_second)=16.025085847056886
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=1.7633294687453425
avg(momentum)=0.0


======== MORE-DIMENSIONAL STUFF =====================
		MEASURE_IS = true; 
		MEASURE_SAVE_AFTER_EACH_RUN = false; 
		MEASURE_RECORD_EACH = 1000;

		INPUT_FILEPATH = "k3.in"; 
		OUTPUT_FILEPATH = "k3.out"; 
		INIT_HIDDEN_LAYER_SIZE = 3; 

		CONVERGENCE_WEIGHT_EPSILON = 0.0; 
		
		CONVERGENCE_NO_CHANGE_FOR = 10; 
		CONVERGENCE_NO_CHANGE_EPSILON = 0.001;
		INIT_MAX_EPOCHS = 5000;

		INIT_RUNS = 1000; 
		INIT_CANDIDATES_COUNT = 1;

		PRINT_NETWORK_IS = true; 

		TRY_NORMAL_DISTRIBUTION_SIGMA = new double[] {2.3}; 
		TRY_LAMBDA = new double[] {0.7}; 
		TRY_MOMENTUM = new double[] {0.0}; 

=========== HIDDEN SIZE=8 =================
1401.0 2.3 0.7 0.0 5.0 1/20
1413.0 2.3 0.7 0.0 5.0 1/20
1422.0 2.3 0.7 0.0 5.0 1/20
1435.0 2.3 0.7 0.0 5.0 1/20
1449.0 2.3 0.7 0.0 5.0 1/20
1454.0 2.3 0.7 0.0 5.0 1/20
1479.0 2.3 0.7 0.0 5.0 1/20
1482.0 2.3 0.7 0.0 5.0 1/20
1493.0 2.3 0.7 0.0 5.0 1/20
1503.0 2.3 0.7 0.0 5.0 1/20
1505.0 2.3 0.7 0.0 5.0 1/20
1510.0 2.3 0.7 0.0 5.0 1/20
1514.0 2.3 0.7 0.0 5.0 1/20
1517.0 2.3 0.7 0.0 5.0 1/20
1526.0 2.3 0.7 0.0 5.0 1/20
1530.0 2.3 0.7 0.0 5.0 1/20
1542.0 2.3 0.7 0.0 5.0 1/20
1568.0 2.3 0.7 0.0 5.0 1/20
1587.0 2.3 0.7 0.0 5.0 1/20
avg(epoch)=1000.0
avg(err)=1486.35
avg(sigma)=2.2999999999999994
avg(lambda)=0.6999999999999997
avg(momentum)=0.0
avg(h_dist)=1.482050343522324
avg(h_f_b_dist)=1.1821481107119525
avg(m_avg_w)=1.5473514737246634
avg(m_sim)=2.0984952848986156
avg(first_second)=113.64329536840708
avg(o_f_b_dist)=2.4694862735224534

=========== HIDDEN SIZE=10 =================
1396.0 2.3 0.7 0.0 5.0 1/20
1427.0 2.3 0.7 0.0 5.0 1/20
1443.0 2.3 0.7 0.0 5.0 1/20
1458.0 2.3 0.7 0.0 5.0 1/20
1484.0 2.3 0.7 0.0 5.0 1/20
1515.0 2.3 0.7 0.0 5.0 1/20
1532.0 2.3 0.7 0.0 5.0 1/20
1540.0 2.3 0.7 0.0 5.0 1/20
1560.0 2.3 0.7 0.0 5.0 1/20
1574.0 2.3 0.7 0.0 5.0 1/20
1577.0 2.3 0.7 0.0 10.0 2/20
1582.0 2.3 0.7 0.0 5.0 1/20
1586.0 2.3 0.7 0.0 5.0 1/20
1587.0 2.3 0.7 0.0 5.0 1/20
1588.0 2.3 0.7 0.0 5.0 1/20
1592.0 2.3 0.7 0.0 5.0 1/20
1604.0 2.3 0.7 0.0 5.0 1/20
1605.0 2.3 0.7 0.0 5.0 1/20
1626.0 2.3 0.7 0.0 5.0 1/20
avg(epoch)=1000.0
avg(err)=1542.65
avg(sigma)=2.2999999999999994
avg(lambda)=0.6999999999999997
avg(momentum)=0.0
avg(h_dist)=1.653701960729024
avg(h_f_b_dist)=1.3210189919432302
avg(m_avg_w)=1.551604141181616
avg(m_sim)=2.1575913747369815
avg(first_second)=110.68134873210727
avg(o_f_b_dist)=2.7006947140564233

=========== HIDDEN SIZE=12 =================
1484.0 2.3 0.7 0.0 5.0 1/20
1491.0 2.3 0.7 0.0 5.0 1/20
1504.0 2.3 0.7 0.0 5.0 1/20
1512.0 2.3 0.7 0.0 5.0 1/20
1539.0 2.3 0.7 0.0 5.0 1/20
1544.0 2.3 0.7 0.0 5.0 1/20
1559.0 2.3 0.7 0.0 5.0 1/20
1573.0 2.3 0.7 0.0 5.0 1/20
1581.0 2.3 0.7 0.0 5.0 1/20
1593.0 2.3 0.7 0.0 5.0 1/20
1595.0 2.3 0.7 0.0 5.0 1/20
1603.0 2.3 0.7 0.0 5.0 1/20
1612.0 2.3 0.7 0.0 5.0 1/20
1617.0 2.3 0.7 0.0 5.0 1/20
1649.0 2.3 0.7 0.0 5.0 1/20
1655.0 2.3 0.7 0.0 5.0 1/20
1671.0 2.3 0.7 0.0 5.0 1/20
1691.0 2.3 0.7 0.0 5.0 1/20
1695.0 2.3 0.7 0.0 5.0 1/20
avg(epoch)=1000.0
avg(err)=1582.25
avg(sigma)=2.2999999999999994
avg(lambda)=0.6999999999999997
avg(momentum)=0.0
avg(h_dist)=1.8572751545126276
avg(h_f_b_dist)=1.4793388997673078
avg(m_avg_w)=1.5600097743922352
avg(m_sim)=2.191738234150749
avg(first_second)=108.10211594774948
avg(o_f_b_dist)=2.94788637922129

=========== HIDDEN SIZE=14 =================
err sigma lambda momentum success sample_ratio
1463.0 2.3 0.7 0.0 5.0 1/20
1472.0 2.3 0.7 0.0 5.0 1/20
1508.0 2.3 0.7 0.0 5.0 1/20
1548.0 2.3 0.7 0.0 5.0 1/20
1567.0 2.3 0.7 0.0 10.0 2/20
1581.0 2.3 0.7 0.0 5.0 1/20
1583.0 2.3 0.7 0.0 5.0 1/20
1598.0 2.3 0.7 0.0 5.0 1/20
1614.0 2.3 0.7 0.0 10.0 2/20
1618.0 2.3 0.7 0.0 5.0 1/20
1622.0 2.3 0.7 0.0 5.0 1/20
1623.0 2.3 0.7 0.0 5.0 1/20
1628.0 2.3 0.7 0.0 5.0 1/20
1631.0 2.3 0.7 0.0 5.0 1/20
1694.0 2.3 0.7 0.0 5.0 1/20
1719.0 2.3 0.7 0.0 5.0 1/20
1753.0 2.3 0.7 0.0 5.0 1/20
1766.0 2.3 0.7 0.0 5.0 1/20

=========== HIDDEN SIZE=16 =================
err sigma lambda momentum success sample_ratio
1548.0 2.3 0.7 0.0 5.0 1/20
1571.0 2.3 0.7 0.0 5.0 1/20
1582.0 2.3 0.7 0.0 5.0 1/20
1586.0 2.3 0.7 0.0 5.0 1/20
1588.0 2.3 0.7 0.0 5.0 1/20
1590.0 2.3 0.7 0.0 5.0 1/20
1607.0 2.3 0.7 0.0 5.0 1/20
1613.0 2.3 0.7 0.0 15.0 3/20
1614.0 2.3 0.7 0.0 5.0 1/20
1617.0 2.3 0.7 0.0 5.0 1/20
1626.0 2.3 0.7 0.0 5.0 1/20
1631.0 2.3 0.7 0.0 5.0 1/20
1663.0 2.3 0.7 0.0 5.0 1/20
1666.0 2.3 0.7 0.0 5.0 1/20
1670.0 2.3 0.7 0.0 5.0 1/20
1672.0 2.3 0.7 0.0 5.0 1/20
1691.0 2.3 0.7 0.0 5.0 1/20
1698.0 2.3 0.7 0.0 5.0 1/20
avg(epoch)=1000.0
avg(err)=1622.95
avg(sigma)=2.2999999999999994
avg(lambda)=0.6999999999999997
avg(momentum)=0.0
avg(h_dist)=2.1245678066050195
avg(h_f_b_dist)=1.7271721585694397
avg(m_avg_w)=1.636067005755703
avg(m_sim)=2.4494895296666996
avg(first_second)=104.03792905577318
avg(o_f_b_dist)=3.4224612192761077



=========== HIDDEN SIZE=19 =================

=========== HIDDEN SIZE=22 =================

=========== HIDDEN SIZE=25 =================

=========== HIDDEN SIZE=29 =================

=========== HIDDEN SIZE=33 =================

=========== HIDDEN SIZE=38 =================

=========== HIDDEN SIZE=43 =================

=========== HIDDEN SIZE= =================

=========== HIDDEN SIZE= =================

=========== HIDDEN SIZE= =================

=========== HIDDEN SIZE= =================

=========== HIDDEN SIZE= =================

1386588826_4.log:avg(err)=0.657
1386588902_5.log:avg(err)=0.15
1386588960_6.log:avg(err)=0.056
1386589007_7.log:avg(err)=0.014
1386589049_8.log:avg(err)=0.0040
1386589086_9.log:avg(err)=0.0
1386589123_10.log:avg(err)=0.0
1386589158_11.log:avg(err)=0.0
1386589191_12.log:avg(err)=0.0
1386589225_13.log:avg(err)=0.0
1386589258_14.log:avg(err)=0.0
1386589290_15.log:avg(err)=0.0
1386589323_16.log:avg(err)=0.0

k12_1386591048_8.log:avg(err)=1486.35
k12_1386591204_10.log:avg(err)=1542.65
k12_1386591375_12.log:avg(err)=1582.25
k12_1386591565_14.log:avg(err)=1608.45
k12_1386591774_16.log:avg(err)=1622.95
k12_1386592002_19.log:avg(err)=1634.15
k12_1386592260_22.log:avg(err)=1583.85
k12_1386592546_25.log:avg(err)=1585.2
k12_1386592863_29.log:avg(err)=1471.15
k12_1386593289_33.log:avg(err)=1435.15
k12_1386593762_38.log:avg(err)=1339.3
k12_1386594301_43.log:avg(err)=1270.1
k12_1386594959_49.log:avg(err)=1149.35
k12_1386595621_56.log:avg(err)=1036.15
k12_1386596402_64.log:avg(err)=913.6
k12_1386597286_73.log:avg(err)=811.75
k12_1386598260_83.log:avg(err)=674.15
k12_1386599314_94.log:avg(err)=587.25
k12_1386600536_106.log:avg(err)=463.45
k12_1386602166_120.log:avg(err)=375.9
k12_1386603756_136.log:avg(err)=306.3





========================== 10-12-2013 =========================
Printing 4-2-4 hidden representations 
Generating plots bad/good

good: look like a lichobeznik or symmetric
138671883_.png - 3 on line
138671920_.png - 4 border close
138671944_.png - 3 corner close 
138671948_.png - 2 very close
138671986_.png - 4 left
138672005_.png - 4 left border
138672026_.png - 4 left top corner
138672076_.png - 4 right bottom corner
138672119_.png - 4 right top corner

bad: looks static or usually one inside a triangle (non-convex 4gon) 
138671976_.png - almost no diff with good
138671983_.png - why not
138671990_.png - 2 collision
138672053_.png - big jump line
138672070_.png - 2 zero 
138672013_.png - non-convergent 
138672133_.png - looks good 
138672146_.png - 1 total static zero
138672185_.png - 4 static looks good 
138672212_.png - 4 line
138672288_.png - 4 non-convergent
138672302_.png - 4 close 
138672376_.png - 4 top 
138672401_.png - 4 left top
13867_.png - 
13867_.png - 
13867_.png - 

TODO pouzivam error pri uceni? 0.2, 0.8  
TODO ako vyzeraju blizke body (moz
TODO konverguje pri zlych? 

============================== 16-12-2013 ========================
Saving networks before and after runs (bad / good).
Rerunning same configurations. 

All bad: 
err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.0 19.296918767507005 6889/35700
1.0 2.3 0.7 0.0 68.05602240896359 24296/35700
2.0 2.3 0.7 0.0 12.644257703081232 4514/35700
3.0 2.3 0.7 0.0 0.0028011204481792717 1/35700

All good: 
err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.0 99.98911353032659 64293/64300
1.0 2.3 0.7 0.0 0.01088646967340591 7/64300
//TODO overit ci zrobilo dobre good / bad
//TODO numericky potvrdit ci vnutri trojuholnika 
//TODO batch mode 
//TODO ==> ako rozlisit uspesne a neuspesne od inicializacie (v batch mode)
//TODO binarny klasifikator na good/bad vah 

//TODO zovseobecnenie siete

============================== 08-01-2014 ========================
Checking if some point inside others. 
Preliminary result: 
awk 'BEGIN{FS=" "}{print $2,$12}' auto4_1389182421458_2_post.dat | sort | uniq -c | sort -n
      1 2.0 0.0
      1 err in_triangle
      3 1.0 0.0
      3 2.0 1.0
     13 1.0 1.0
     30 0.0 0.0

awk 'BEGIN{FS=" "}{print $2,$12}' auto4_1389182421458_2_pre.dat | sort | uniq -c | sort -n
      1 err in_triangle
      4 0.0 1.0
      4 1.0 1.0
      4 2.0 0.0
     12 1.0 0.0
     26 0.0 0.0
     
awk 'BEGIN{FS=" "}{print $2,$12}' auto4_1389194211285_2_post.dat | sort | uniq -c | sort -n
      1 err in_triangle
     15 2.0 0.0
     69 2.0 1.0
    105 1.0 0.0
    244 1.0 1.0
    567 0.0 0.0
    
Chosing beginnings with convex hidden representations:
err sigma lambda success sample_ratio
0.0 2.3 0.7 68.30000000000001 683/1000
1.0 2.3 0.7 24.5 245/1000
2.0 2.3 0.7 7.199999999999999 72/1000

Start: 
      1 err in_triangle
     72 2.0 0.0
    245 1.0 0.0
    683 0.0 0.0

End: 
      1 err in_triangle
      7 2.0 0.0
     47 1.0 0.0
     65 2.0 1.0
    198 1.0 1.0
    683 0.0 0.0

Hmm, errors count match. 

awk 'BEGIN{FS=" "}{print $2,$12}' auto4_1389196250599_2_pre.dat | sort | uniq -c | sort -n
      1 err in_triangle
     69 2.0 0.0
    256 1.0 0.0
    675 0.0 0.0
petrzlen@Petobook3:~/diplomovka/bal/data$ awk 'BEGIN{FS=" "}{print $2,$12}' auto4_1389196250599_2_post.dat | sort | uniq -c | sort -n
      1 err in_triangle
     12 2.0 0.0
     43 1.0 0.0
     57 2.0 1.0
    213 1.0 1.0
    675 0.0 0.0

Hmm, again. 
So all the training is for nothing, even makes the things worse. Maybe a bug. 

awk 'BEGIN{FS=" "}{print $2}' auto4_1389196800513_2_pre.dat | sort | uniq -c | sort -n
      1 err
      8 3.0
    105 2.0
    289 1.0
    598 0.0
petrzlen@Petobook3:~/diplomovka/bal/data$ awk 'BEGIN{FS=" "}{print $2}' auto4_1389196800513_2_post.dat | sort | uniq -c | sort -n
      1 err
      8 3.0
    105 2.0
    289 1.0
    598 0.0
    
Without preselecting. 
SHIT.     

Some older: 
awk 'BEGIN{FS=" "}{print $2}' auto4_1387317905615_2_post.dat | sort | uniq -c | sort -n
      1 err
      4 3.0
    103 2.0
    296 1.0
    597 0.0
petrzlen@Petobook3:~/diplomovka/bal/data$ awk 'BEGIN{FS=" "}{print $2}' auto4_1387317905615_2_pre.dat | sort | uniq -c | sort -n
      1 err
      4 3.0
    103 2.0
    296 1.0
    597 0.0

FUCK THIS SHIT.  ]
Oh, ok. We are for some reason setting this only after run. 
Sure?
awk 'BEGIN{FS=" "}{print $2}' auto4_1389197605824_2_pre.dat | sort | uniq -c | sort -n
      1 err
      2 2.0
      3 3.0
     11 13.0
     33 12.0
     37 4.0
     44 11.0
     54 5.0
    126 10.0
    152 6.0
    158 9.0
    164 7.0
    216 8.0
petrzlen@Petobook3:~/diplomovka/bal/data$ awk 'BEGIN{FS=" "}{print $2}' auto4_1389197605824_2_post.dat | sort | uniq -c | sort -n
      1 err
     63 2.0
    249 1.0
    688 0.0


=================================================================================
========================== 4th February       ===================================
=================================================================================
Trying out different learning algorithms. 

There is something weird with the epochs, I should plot it ASAP. It looks like the more epochs the better results what is not in correspondence with Igors observations. 

tail -n +2 data/auto4_1391556235821_2_measure.dat | awk '{a[$2] += $3}END{for(epoch in a) print epoch,a[epoch]}' | sort -n | tail -n +100 > epochs_100000.dat

... even after 800,000 epochs there are some networks for which the error change 


=================================================================================
========================== 5th February       ===================================
=================================================================================
Implementing dropout 
  -- not helping, we should check out the implementation 
 
 
EPOCHS = 500,000
CANDIDATES = 1 

PreMeasure : GroupBy
err sigma lambda momentum success sample_ratio
10.0 2.3 0.7 0.1 14.000000000000002 14/100
11.0 2.3 0.7 0.1 4.0 4/100
12.0 2.3 0.7 0.1 4.0 4/100
4.0 2.3 0.7 0.1 6.0 6/100
5.0 2.3 0.7 0.1 7.000000000000001 7/100
6.0 2.3 0.7 0.1 21.0 21/100
7.0 2.3 0.7 0.1 16.0 16/100
8.0 2.3 0.7 0.1 20.0 20/100
9.0 2.3 0.7 0.1 8.0 8/100

PostMeasure : GroupBy
err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.1 60.0 60/100
1.0 2.3 0.7 0.1 28.999999999999996 29/100
2.0 2.3 0.7 0.1 11.0 11/100


EPOCHS = 500,000
CANDIDATES = 1000
PreMeasure : GroupBy
err sigma lambda momentum success sample_ratio
10.0 2.3 0.7 0.1 10.0 10/100
11.0 2.3 0.7 0.1 6.0 6/100
12.0 2.3 0.7 0.1 4.0 4/100
13.0 2.3 0.7 0.1 2.0 2/100
4.0 2.3 0.7 0.1 6.0 6/100
5.0 2.3 0.7 0.1 6.0 6/100
6.0 2.3 0.7 0.1 11.0 11/100
7.0 2.3 0.7 0.1 19.0 19/100
8.0 2.3 0.7 0.1 14.000000000000002 14/100
9.0 2.3 0.7 0.1 22.0 22/100

PostMeasure : GroupBy
err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.1 79.0 79/100
1.0 2.3 0.7 0.1 15.0 15/100
2.0 2.3 0.7 0.1 6.0 6/100

==EPOCH TO ERROR ==
tail -n +2 auto4_1391599344374_2_measure.dat | awk '{a[$2] += $3}END{for(epoch in a) print epoch,a[epoch]}' | sort -n | tail -n +10 | head -1000 > epochs_500000_can.dat
petrzlen@Petobook3:~/diplomovka/bal/data$ tail -n +2 auto4_1391598749349_2_measure.dat | awk '{a[$2] += $3}END{for(epoch in a) print epoch,a[epoch]}' | sort -n | tail -n +10 | head -1000 > epochs_500000_norm.dat

plot "./epochs_500000_norm.dat" using 1:2 with lines title "normal",  \
>"./epochs_500000_can.dat" using 1:2 with lines title "1000 canditates"


============================= 24th February ==============================
Mathematical background on convergence and learning rule: 

(Hopfield networks, Wikipedia) The requirement that weights be symmetric is typically used, as it will guarantee that the energy function decreases monotonically while following the activation rules, and the network may exhibit some periodic or chaotic behaviour if non-symmetric weights are used. However, Hopfield found that this chaotic behavior is confined to relatively small parts of the phase space, and does not impair the network's ability to act as a content-addressable associative memory system.

(O'Reilly 1996) The GeneRec Approximation to AP BP
The analysis presented earlier in the paper shows that GeneRec should compute the same error derivatives as the Almeida-Pineda version of error backpropagation in a recurrent network if the following conditions hold:
  The difference of the plus and minus phase activation terms in GeneRec, which are updated in separate iterative activation settling phases, can be used to compute a unit’s error term instead of the iterative update of the difference itself, which is what Almeida-Pineda uses.
  The reciprocal weights are symmetric. This enables the activation signals from the output to the hidden units (via the recurrent weights) to reflect the contribution that the hidden units made to the output error (via the forward-going weights).
  The difference of activations in the plus and minus phases is a reasonable approximation to the difference of net inputs times the derivative of the sigmoidal activation function. Note that this only affects the overall magnitude of the weight derivatives, not their direction.
  
(PINnc89.pdf) 

$$r_x\frac{dx_i}{dt} = -x_i + \sum_j w_{ij} f(x_j) + I_i$$
TODO: Frome where comes this equation? 

There are several ways to guarantee convergence. One way is to
impose structure on the connectivity of the network, such as requiring
the weight matrix to be lower triangular or symmetric. Symmetry, al-
though mathematically elegant, is quite stringent because it constrains
microscopic connectivity by requiring pairs of neurons to be symmetri-
cally connected. A less stringent constraint is to require that the Jacobian
matrix be diagonally dominant. For equation (2.11, the Jacobian matrix
has the form: 

$$L_{ij} = \delta_{ij} - w_{ij}f'(x_j)$$
Guez et al. (1988) 

(PINnc89.pdf) then the gradient descent dynamics will not change the stability of the network. The need
for this assumption can be eliminated by choosing a dynamical system
which admits only stable behavior, even under learning, as was done by
Barhen et al. (1989). TODO look up Barhen et al. (1989) article 

TODO - general theorem concerning staiblity of networks with symmetric weights Cohen and Grossberg 1983 

GLOBAL ASYMPTOTIC STABILITY - the network will settle for any input 
FIXED POINT =similar= MEMORY

================= 26th February ===================

pineda1987
Oscilation (on recurrent BP) could occur when substantial SELF-EXCITATION 


================= 04th March ======================
Trying to implement recirculation to BAL (non-symmetric weights)

== with 0.5 on hidden
		INIT_NORMAL_DISTRIBUTION_SIGMA = 2.3;  
		INIT_LAMBDA = 0.7; 
		INIT_MAX_EPOCHS = 10000;
		INIT_RUNS = 100; 

err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.1 21.0 21/100
1.0 2.3 0.7 0.1 32.0 32/100
2.0 2.3 0.7 0.1 20.0 20/100
3.0 2.3 0.7 0.1 8.0 8/100
4.0 2.3 0.7 0.1 18.0 18/100
5.0 2.3 0.7 0.1 1.0 1/100


== with 0.5 on hidden
		INIT_NORMAL_DISTRIBUTION_SIGMA = 2.3;  
		INIT_MAX_EPOCHS = 10000;
		INIT_RUNS = 100; 
	  TRY_LAMBDA[] = {0.03, 0.1, 0.3, 0.5, 0.7, 0.9, 1.1, 1.3, 1.5, 1.8, 2.2}; 
	  
err sigma lambda momentum success sample_ratio
0.0 2.3 0.03 0.1 11.11111111111111 11/99
0.0 2.3 0.1 0.1 17.857142857142858 15/84
0.0 2.3 0.3 0.1 33.6734693877551 33/98
0.0 2.3 0.5 0.1 26.923076923076923 28/104
0.0 2.3 0.7 0.1 25.24271844660194 26/103
0.0 2.3 0.9 0.1 19.51219512195122 16/82
0.0 2.3 1.1 0.1 19.047619047619047 16/84
0.0 2.3 1.3 0.1 25.555555555555554 23/90
0.0 2.3 1.5 0.1 15.11627906976744 13/86
0.0 2.3 1.8 0.1 14.864864864864865 11/74
0.0 2.3 2.2 0.1 9.375 9/96
1.0 2.3 0.03 0.1 36.36363636363637 36/99
1.0 2.3 0.1 0.1 47.61904761904761 40/84
1.0 2.3 0.3 0.1 42.857142857142854 42/98
1.0 2.3 0.5 0.1 41.34615384615385 43/104
1.0 2.3 0.7 0.1 31.06796116504854 32/103
1.0 2.3 0.9 0.1 31.70731707317073 26/82
1.0 2.3 1.1 0.1 27.380952380952383 23/84
1.0 2.3 1.3 0.1 25.555555555555554 23/90
1.0 2.3 1.5 0.1 23.25581395348837 20/86
1.0 2.3 1.8 0.1 20.27027027027027 15/74
1.0 2.3 2.2 0.1 23.958333333333336 23/96
2.0 2.3 0.03 0.1 40.4040404040404 40/99
2.0 2.3 0.1 0.1 26.190476190476193 22/84
2.0 2.3 0.3 0.1 13.26530612244898 13/98
2.0 2.3 0.5 0.1 20.192307692307693 21/104
2.0 2.3 0.7 0.1 25.24271844660194 26/103
2.0 2.3 0.9 0.1 17.073170731707318 14/82
2.0 2.3 1.1 0.1 25.0 21/84
2.0 2.3 1.3 0.1 16.666666666666664 15/90
2.0 2.3 1.5 0.1 16.27906976744186 14/86
2.0 2.3 1.8 0.1 25.675675675675674 19/74
2.0 2.3 2.2 0.1 21.875 21/96
3.0 2.3 0.03 0.1 8.080808080808081 8/99
3.0 2.3 0.1 0.1 4.761904761904762 4/84
3.0 2.3 0.3 0.1 3.061224489795918 3/98
3.0 2.3 0.5 0.1 1.9230769230769231 2/104
3.0 2.3 0.7 0.1 4.854368932038835 5/103
3.0 2.3 0.9 0.1 8.536585365853659 7/82
3.0 2.3 1.1 0.1 8.333333333333332 7/84
3.0 2.3 1.3 0.1 6.666666666666667 6/90
3.0 2.3 1.5 0.1 12.790697674418606 11/86
3.0 2.3 1.8 0.1 13.513513513513514 10/74
3.0 2.3 2.2 0.1 5.208333333333334 5/96
4.0 2.3 0.03 0.1 4.040404040404041 4/99
4.0 2.3 0.1 0.1 3.571428571428571 3/84
4.0 2.3 0.3 0.1 6.122448979591836 6/98
4.0 2.3 0.5 0.1 8.653846153846153 9/104
4.0 2.3 0.7 0.1 11.650485436893204 12/103
4.0 2.3 0.9 0.1 17.073170731707318 14/82
4.0 2.3 1.1 0.1 11.904761904761903 10/84
4.0 2.3 1.3 0.1 7.777777777777778 7/90
4.0 2.3 1.5 0.1 9.30232558139535 8/86
4.0 2.3 1.8 0.1 8.108108108108109 6/74
4.0 2.3 2.2 0.1 9.375 9/96
5.0 2.3 0.3 0.1 1.0204081632653061 1/98
5.0 2.3 0.5 0.1 0.9615384615384616 1/104
5.0 2.3 0.7 0.1 1.9417475728155338 2/103
5.0 2.3 0.9 0.1 1.2195121951219512 1/82
5.0 2.3 1.1 0.1 2.380952380952381 2/84
5.0 2.3 1.3 0.1 4.444444444444445 4/90
5.0 2.3 1.5 0.1 2.3255813953488373 2/86
5.0 2.3 2.2 0.1 1.0416666666666665 1/96
6.0 2.3 0.9 0.1 4.878048780487805 4/82
6.0 2.3 1.1 0.1 5.952380952380952 5/84
6.0 2.3 1.3 0.1 12.222222222222221 11/90
6.0 2.3 1.5 0.1 17.441860465116278 15/86
6.0 2.3 1.8 0.1 16.216216216216218 12/74
6.0 2.3 2.2 0.1 26.041666666666668 25/96
7.0 2.3 1.3 0.1 1.1111111111111112 1/90
7.0 2.3 1.5 0.1 1.1627906976744187 1/86
7.0 2.3 2.2 0.1 2.083333333333333 2/96
8.0 2.3 1.5 0.1 1.1627906976744187 1/86
8.0 2.3 1.8 0.1 1.3513513513513513 1/74
8.0 2.3 2.2 0.1 1.0416666666666665 1/96
9.0 2.3 1.5 0.1 1.1627906976744187 1/86

== with 1.0 on hidden
		INIT_NORMAL_DISTRIBUTION_SIGMA = 2.3;  
		INIT_LAMBDA = 0.7; 
		INIT_MAX_EPOCHS = 10000;
		INIT_RUNS = 100; 
PostMeasure : GroupBy
err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.1 12.0 12/100
1.0 2.3 0.7 0.1 23.0 23/100
2.0 2.3 0.7 0.1 24.0 24/100
3.0 2.3 0.7 0.1 20.0 20/100
4.0 2.3 0.7 0.1 21.0 21/100



================================== 5th of March ==========================================
How to generalize GeneRec to both ways? 
Experimenting with:
  a) classic generec - use only 3 matrices (which should work best for symmetric) 
  b) bothwards = f(hidden_net_from_input + hidden_net_from_output) 
  c) using GeneRec distinctly for forward and backward and combine them 

Getting to best \lambda \sigma (as now twice as much, i.e. N = ((I.size() + 1) + (O.size() + 1)) come to each hidden unit, probably is better to chose smaller sigma (\sqrt{N}) that's about \sqrt{2} times less. 

TODO: want to try symmetric weights 
  a) on init 
  b) allways and compare performance 


NOTE: hidden_repre_all could lead to big memory consumptions 

============= c), non-symmetric, generec rule, 4-2-4 
err sigma lambda momentum success sample_ratio
0.0 1.0 0.03 0.1 13.88888888888889 5/36
0.0 1.0 0.1 0.1 30.23255813953488 13/43
0.0 1.0 0.3 0.1 32.432432432432435 12/37
0.0 1.0 0.5 0.1 21.73913043478261 10/46
0.0 1.0 0.7 0.1 34.48275862068966 10/29
0.0 1.0 0.9 0.1 47.05882352941176 16/34
0.0 1.0 1.1 0.1 33.33333333333333 14/42
0.0 1.0 1.3 0.1 26.190476190476193 11/42
0.0 1.0 1.5 0.1 13.157894736842104 5/38
0.0 1.0 1.8 0.1 23.684210526315788 9/38
0.0 1.0 2.2 0.1 27.027027027027028 10/37
0.0 1.3 0.03 0.1 11.904761904761903 5/42
0.0 1.3 0.1 0.1 29.72972972972973 11/37
0.0 1.3 0.3 0.1 25.0 7/28
0.0 1.3 0.5 0.1 21.875 7/32
0.0 1.3 0.7 0.1 30.23255813953488 13/43
0.0 1.3 0.9 0.1 19.51219512195122 8/41
0.0 1.3 1.1 0.1 28.26086956521739 13/46
0.0 1.3 1.3 0.1 19.444444444444446 7/36
0.0 1.3 1.5 0.1 17.94871794871795 7/39
0.0 1.3 1.8 0.1 22.857142857142858 8/35
0.0 1.3 2.2 0.1 14.634146341463413 6/41
0.0 1.8 0.03 0.1 19.444444444444446 7/36
0.0 1.8 0.1 0.1 25.0 9/36
0.0 1.8 0.3 0.1 28.205128205128204 11/39
0.0 1.8 0.5 0.1 15.151515151515152 5/33
0.0 1.8 0.7 0.1 28.57142857142857 10/35
0.0 1.8 0.9 0.1 27.586206896551722 8/29
0.0 1.8 1.1 0.1 19.047619047619047 8/42
0.0 1.8 1.3 0.1 23.25581395348837 10/43
0.0 1.8 1.5 0.1 20.0 5/25
0.0 1.8 1.8 0.1 12.903225806451612 4/31
0.0 1.8 2.2 0.1 13.513513513513514 5/37
0.0 2.3 0.03 0.1 20.454545454545457 9/44
0.0 2.3 0.1 0.1 17.94871794871795 7/39
0.0 2.3 0.3 0.1 11.538461538461538 3/26
0.0 2.3 0.5 0.1 24.0 6/25
0.0 2.3 0.7 0.1 18.181818181818183 6/33
0.0 2.3 0.9 0.1 20.833333333333336 5/24
0.0 2.3 1.1 0.1 36.58536585365854 15/41
0.0 2.3 1.3 0.1 22.857142857142858 8/35
0.0 2.3 1.5 0.1 31.03448275862069 9/29
0.0 2.3 1.8 0.1 18.181818181818183 6/33
0.0 2.3 2.2 0.1 22.857142857142858 8/35
0.0 3.0 0.03 0.1 7.142857142857142 2/28
0.0 3.0 0.1 0.1 13.953488372093023 6/43
0.0 3.0 0.3 0.1 16.666666666666664 5/30
0.0 3.0 0.5 0.1 12.195121951219512 5/41
0.0 3.0 0.7 0.1 20.51282051282051 8/39
0.0 3.0 0.9 0.1 30.555555555555557 11/36
0.0 3.0 1.1 0.1 19.230769230769234 5/26
0.0 3.0 1.3 0.1 14.285714285714285 6/42
0.0 3.0 1.5 0.1 10.526315789473683 4/38
0.0 3.0 1.8 0.1 4.081632653061225 2/49
0.0 3.0 2.2 0.1 5.555555555555555 2/36
1.0 1.0 0.03 0.1 63.888888888888886 23/36
1.0 1.0 0.1 0.1 58.139534883720934 25/43
1.0 1.0 0.3 0.1 45.94594594594595 17/37
1.0 1.0 0.5 0.1 58.69565217391305 27/46
1.0 1.0 0.7 0.1 48.275862068965516 14/29
1.0 1.0 0.9 0.1 41.17647058823529 14/34
1.0 1.0 1.1 0.1 33.33333333333333 14/42
1.0 1.0 1.3 0.1 35.714285714285715 15/42
1.0 1.0 1.5 0.1 39.473684210526315 15/38
1.0 1.0 1.8 0.1 36.84210526315789 14/38
1.0 1.0 2.2 0.1 27.027027027027028 10/37
1.0 1.3 0.03 0.1 42.857142857142854 18/42
1.0 1.3 0.1 0.1 59.45945945945946 22/37
1.0 1.3 0.3 0.1 64.28571428571429 18/28
1.0 1.3 0.5 0.1 31.25 10/32
1.0 1.3 0.7 0.1 44.18604651162791 19/43
1.0 1.3 0.9 0.1 48.78048780487805 20/41
1.0 1.3 1.1 0.1 34.78260869565217 16/46
1.0 1.3 1.3 0.1 44.44444444444444 16/36
1.0 1.3 1.5 0.1 33.33333333333333 13/39
1.0 1.3 1.8 0.1 25.71428571428571 9/35
1.0 1.3 2.2 0.1 36.58536585365854 15/41
1.0 1.8 0.03 0.1 41.66666666666667 15/36
1.0 1.8 0.1 0.1 47.22222222222222 17/36
1.0 1.8 0.3 0.1 33.33333333333333 13/39
1.0 1.8 0.5 0.1 48.484848484848484 16/33
1.0 1.8 0.7 0.1 34.285714285714285 12/35
1.0 1.8 0.9 0.1 41.37931034482759 12/29
1.0 1.8 1.1 0.1 38.095238095238095 16/42
1.0 1.8 1.3 0.1 34.883720930232556 15/43
1.0 1.8 1.5 0.1 40.0 10/25
1.0 1.8 1.8 0.1 25.806451612903224 8/31
1.0 1.8 2.2 0.1 32.432432432432435 12/37
1.0 2.3 0.03 0.1 31.818181818181817 14/44
1.0 2.3 0.1 0.1 33.33333333333333 13/39
1.0 2.3 0.3 0.1 26.923076923076923 7/26
1.0 2.3 0.5 0.1 36.0 9/25
1.0 2.3 0.7 0.1 24.242424242424242 8/33
1.0 2.3 0.9 0.1 33.33333333333333 8/24
1.0 2.3 1.1 0.1 26.82926829268293 11/41
1.0 2.3 1.3 0.1 34.285714285714285 12/35
1.0 2.3 1.5 0.1 20.689655172413794 6/29
1.0 2.3 1.8 0.1 21.21212121212121 7/33
1.0 2.3 2.2 0.1 34.285714285714285 12/35
1.0 3.0 0.03 0.1 14.285714285714285 4/28
1.0 3.0 0.1 0.1 30.23255813953488 13/43
1.0 3.0 0.3 0.1 23.333333333333332 7/30
1.0 3.0 0.5 0.1 26.82926829268293 11/41
1.0 3.0 0.7 0.1 28.205128205128204 11/39
1.0 3.0 0.9 0.1 27.77777777777778 10/36
1.0 3.0 1.1 0.1 23.076923076923077 6/26
1.0 3.0 1.3 0.1 16.666666666666664 7/42
1.0 3.0 1.5 0.1 34.21052631578947 13/38
1.0 3.0 1.8 0.1 28.57142857142857 14/49
1.0 3.0 2.2 0.1 11.11111111111111 4/36

============= b), non-symmetric, generec rule, 4-2-4 
=> input / output activations tended to zero 

============= c), non-symmetric, generec rule, 4-3-4 
-> seems like smaller sigma works good 
err sigma lambda momentum success sample_ratio
0.0 0.4 0.7 0.1 100.0 3/3
0.0 0.4 0.8 0.1 100.0 10/10
0.0 0.4 0.9 0.1 83.33333333333334 5/6
0.0 0.4 1.0 0.1 100.0 4/4
0.0 0.4 1.1 0.1 100.0 8/8
0.0 0.4 1.2 0.1 100.0 15/15
0.0 0.7 0.7 0.1 100.0 6/6
0.0 0.7 0.8 0.1 66.66666666666666 6/9
0.0 0.7 0.9 0.1 72.72727272727273 8/11
0.0 0.7 1.0 0.1 100.0 5/5
0.0 0.7 1.1 0.1 80.0 4/5
0.0 0.7 1.2 0.1 100.0 6/6
0.0 1.0 0.7 0.1 83.33333333333334 5/6
0.0 1.0 0.8 0.1 100.0 7/7
0.0 1.0 0.9 0.1 100.0 9/9
0.0 1.0 1.0 0.1 60.0 3/5
0.0 1.0 1.1 0.1 75.0 3/4
0.0 1.0 1.2 0.1 100.0 5/5
0.0 1.5 0.7 0.1 100.0 5/5
0.0 1.5 0.8 0.1 75.0 3/4
0.0 1.5 0.9 0.1 57.14285714285714 4/7
0.0 1.5 1.0 0.1 100.0 7/7
0.0 1.5 1.1 0.1 71.42857142857143 5/7
0.0 1.5 1.2 0.1 87.5 7/8
0.0 2.3 0.7 0.1 80.0 4/5
0.0 2.3 0.8 0.1 66.66666666666666 4/6
0.0 2.3 0.9 0.1 62.5 5/8
0.0 2.3 1.0 0.1 66.66666666666666 2/3
0.0 2.3 1.1 0.1 75.0 6/8
0.0 2.3 1.2 0.1 50.0 4/8


============= c) symmetric, generec rule, 4-2-4, 200 runs 
err sigma lambda momentum success sample_ratio
0.0 0.3 0.7 0.1 29.411764705882355 15/51
0.0 0.5 0.7 0.1 17.02127659574468 8/47
0.0 0.7 0.7 0.1 27.659574468085108 13/47
0.0 1.0 0.7 0.1 23.636363636363637 13/55
1.0 0.3 0.7 0.1 50.98039215686274 26/51
1.0 0.5 0.7 0.1 59.57446808510638 28/47
1.0 0.7 0.7 0.1 53.191489361702125 25/47
1.0 1.0 0.7 0.1 58.18181818181818 32/55
==== non-symmetric
=> works better 
err sigma lambda momentum success sample_ratio
0.0 0.3 0.7 0.1 35.41666666666667 17/48
0.0 0.5 0.7 0.1 28.26086956521739 13/46
0.0 0.7 0.7 0.1 38.18181818181819 21/55
0.0 1.0 0.7 0.1 29.411764705882355 15/51
1.0 0.3 0.7 0.1 47.91666666666667 23/48
1.0 0.5 0.7 0.1 52.17391304347826 24/46
1.0 0.7 0.7 0.1 38.18181818181819 21/55
1.0 1.0 0.7 0.1 37.254901960784316 19/51

=============== Resurrecting old Generec (without bias) ===================
2 hidden -> 50%
3 hidden -> 75% 
4 hidden -> 100% 

=============== First implementation of a) in Java (modifying BAL - working only with 3 weight matrices) 
-> iterative recirculation tends to oscilate activations for h^+ 

starting with symmetric weights seems to help a lot with average recirculation length 
=> I guess symmetry will be crucial 
 => TODO: how to cope with biases? (which are not symmetric) 
 
 ============== 6th March ================
 Igor: 
 Implement original GeneRec in Java 
 Try "Symmetric GeneRec" - be sure it's implemented correctly we should the compare it with BAL and GeneRec 
 Measure "fluctuation" - oscilacia na aktivacii pri iterativnom vypocte  
 Measure epochs needed to converge to good results 
 
 ===== 12th March ========
 
 ==UVOD - pre citatela, ktory temu trocha chape, zavedenie zakladnej notacie 
   co je problem
   preco zaujimave,
   co je zname,
   my sme urobili toto
 
MIND PRVA VETA, primerane skromne a preco zaujimavo a riesitelny 
 
 ==ZAVER 
   spravili sme toto toto
   toto je otvorene
   toto vyzera tazko
   tymto by sme sa zaoberali dalej 
   
 ==OPONENT (konzultovat s veducim) 
   snazime sa z inej katedry 
   precita, svoj nazor, upozorni na chyby
   (o com to je, hodnotiaca cast, zaverecne hodnotenie - otazky)
   ! nemozem referencovat oponenta v prezentacii
     - po prezentacii sa prejdu pripomienky oponenta,
       idealne je mat to pripravene na slide-och 
       
 ==VEDUCI
   co sa spravilo,
   co sa malo spravit, 
   priestor na obranu diplomanta 
   
   
 ==OBHAJOBA:  
   ukazat, co sme sa naucili 
   a ako to vieme aplikovat 
   v prezentacii len to co sme urobil
   
   mame cas dopracovat "mal som cas vysledky dorobit a opravit"
     "nie je v praci" 
     pomoze
     dokument uz nemozem opravit
     
     
     
==== MEASURE FLUCTUATION ========
  INIT_SYMMETRIC_IS = false; 
  RECIRCULATION_USE_AVERAGE_WHEN_OSCILATING = false;	
  RECIRCULATION_ITERATIONS_MAX = 20; //maximum number of iterations to approximate the underlying dynamic system  
  
  we observe arbitrary big fluctuations on the output layer
  most of times around: 
    epoch=1000
    iteraction_count=MAX 
      sometimes even in the first 2-3 iterations 
      most recent in epoch 0 (bad init)  
      
  (treshold 0.95) 
  count = 18 

  hypothesis: 
    the greater fluctuation, the greater the divergence of individual measures
    IDEA: measure "fluctuation" on original BAL and correlate it with error 
  
epoch	err	sigma	lambda	momentum	h_dist	h_f_b_dist	m_avg_w	m_sim	first_second	o_f_b_dist	in_triangle	fluctuation
0	10	0.7	0.7	0.1	0.132211218	0.2786445585	0.5318242002	0.8139507816	4.7267311977	0.5891841024	1	9.793428239062707E-4
0	10	0.7	0.7	0.1	0.132211218	0.2786445585	0.5318242002	0.8139507816	4.7267311977	0.5891841024	1	9.793428239062707E-4
0	10	0.7	0.7	0.1	0.132211218	0.2786445585	0.5318242002	0.8139507816	4.7267311977	0.5891841024	1	9.793428239062707E-4
100	3	0.7	0.7	0.1	0.1209385913	0.0113024925	1.2274326065	0.7514313501	7.5104735297	0.0333910783	0	9.317528770297345E-4
200	2	0.7	0.7	0.1	0.1420464756	0.0111035955	1.8032223422	0.7494629871	7.3325257821	0.0284364855	0	0.0053145144
300	3	0.7	0.7	0.1	0.1270916853	0.0394511929	2.0549802623	0.7472554726	6.5619444128	0.1221931011	0	0.0033510671
400	3	0.7	0.7	0.1	0.1819305524	0.0433490504	2.2132336836	0.7470032846	7.0955398322	0.1478754807	0	0.0031762655
500	2	0.7	0.7	0.1	0.1693042418	0.037170989	2.3602187835	0.7717293129	6.5010481075	0.1091902052	1	0.0011389738
600	3	0.7	0.7	0.1	0.1415687149	0.0293507132	2.5285077486	0.8153394761	6.3544653975	0.1186287764	0	0.0049907399
700	2	0.7	0.7	0.1	0.1721807988	0.0208394457	2.7222702773	0.834317071	6.6935473275	0.0594553391	0	0.0026436319
800	2	0.7	0.7	0.1	0.1541663713	0.0139460481	2.9794722817	0.8283186809	7.3871295089	0.060604712	0	0.0082798183
900	2	0.7	0.7	0.1	0.1632124244	0.0055052794	3.2314481091	0.7862282912	7.9522387158	0.0220200817	0	0.005377339
1000	2	0.7	0.7	0.1	0.1615607721	0.0110292484	3.5097357792	0.8300733922	8.4602597085	0.0453888231	1	0.0099210204
1100	4	0.7	0.7	0.1	0.0118677935	1.4021950622	51.5238922832	68.1585851667	5.6807073484	0.0300233623	1	0.8439187311
1200	4	0.7	0.7	0.1	0.0033354825	1.4106679228	127.5302128374	169.5187046699	5.8130019187	0.0066839711	2	0.8424965005
1300	4	0.7	0.7	0.1	0.0020642798	1.4120331267	203.74159324	271.1403557261	5.7825624507	0.0041940168	2	0.8423298202
1400	4	0.7	0.7	0.1	0.0015129277	1.4125831617	280.0152162847	372.8422619688	5.7702790427	0.0032419408	2	0.8422186796
1500	4	0.7	0.7	0.1	0.0012077319	1.4129110982	356.3107510981	474.5766832466	5.7636454264	0.0025663611	2	0.8421543591
1600	4	0.7	0.7	0.1	8.349306589485936E-4	1.4133468115	432.4523979842	576.1135563944	5.7559883786	0.0014687606	2	0.8420148303
1700	4	0.7	0.7	0.1	7.594608009742811E-4	1.4134233884	508.7930877742	677.9047447987	5.7544373695	0.001349028	2	0.8420030895
1800	4	0.7	0.7	0.1	6.972108359333767E-4	1.4134866862	585.1353853217	779.6978280699	5.7531528077	0.0012490294	2	0.8419934755
1900	4	0.7	0.7	0.1	6.448809749368036E-4	1.413540005	661.4790889035	881.4925678872	5.7520699633	0.0011640431	2	0.8419854785
2000	4	0.7	0.7	0.1	5.993387226058031E-4	1.41358259	737.8233984693	983.2879914517	5.7511450812	0.0011324904	2	0.8420225658

============================================================
  INIT_SYMMETRIC_IS = false; 	
  RECIRCULATION_USE_AVERAGE_WHEN_OSCILATING = true;
  RECIRCULATION_ITERATIONS_MAX = 20; //maximum number of iterations to approximate the underlying dynamic system  
  
  (treshold 0.95) 
    20
    20
    20
    20
    20
    Iteration recirc avg=12 sum=64 count=5
  (treshold 0.05) 
    Iteration recirc avg=20 sum=2500 count=125
    => observed not enough iterations , and most off time not enough iterations 
      it tend to have exponential activation behaviour 
  (treshold 0.05, ITERATIONS_MAX = 200) 
    Iteration recirc avg=200 sum=4800 count=24
    => comparable run-time 
    INTERESTING: oscilation in iteration could occur randomly (just in some epochs and it will completely change the network) 
  
  INTERESTING: when using averages, it's less probable that a fluctuation will occur 
  
  === example 1: === (big fluctuation) 
    backwardPassWithRecirculation : 1.0 0.0 0.0 0.0
    RUN_ID: auto4_1394657816069_2
    Epoch:  1837
    Max fluctuation: 0.9591299483462391
    Iteration count: 20
    Recirc epsilon : 0.0010
    Hidden activations: 0.7514025929910703 0.5148801189699082 1.0
    Hidden activations: 0.4293029835273872 0.15393029053282475 1.0
    Hidden activations: 0.5655477811843503 0.256450483494839 1.0
    Hidden activations: 0.7039438851758654 0.4390921587251962 1.0
    Hidden activations: 0.4443628418531371 0.16487080024433703 1.0
    Hidden activations: 0.5831931106819522 0.27227958640382005 1.0
    Hidden activations: 0.6885008940286396 0.41672696750330646 1.0
    Hidden activations: 0.45670220588404203 0.17405889296969265 1.0
    Hidden activations: 0.5992499612259417 0.2887819782938486 1.0
    Hidden activations: 0.6629722780669469 0.38126878518201696 1.0
    Hidden activations: 0.49299146237429653 0.20278963554799992 1.0
    Hidden activations: 0.6585407094943209 0.3647572713182468 1.0
    Hidden activations: 0.5086897349140999 0.21512136384622532 1.0
    Hidden activations: 0.6825533316491553 0.40124634179929564 1.0
    Hidden activations: 0.4655392951353097 0.18046633810395 1.0
    Hidden activations: 0.611779647883295 0.30285864163362153 1.0
    Hidden activations: 0.6353237623467424 0.34563034768331147 1.0
    Hidden activations: 0.5582526996854738 0.2622971274246442 1.0
    Hidden activations: 0.7065867012069129 0.4452047158095246 1.0
    Hidden activations: 0.44246688992796107 0.16353058373614882 1.0
    Input activations: 0.9944651927610066 0.002791343676802622 2.2421597597631187E-7 0.9817174678203647
    Input activations: 0.004622159761878204 0.13161537521933872 0.9266610280419548 0.001597262712867166
    Input activations: 0.12946553217819015 0.013657962750518924 0.06019676964229493 0.1237814581961272
    Input activations: 0.960200937926683 0.0036557497667021053 8.628624693378726E-6 0.9227734215345949
    Input activations: 0.006713476442023315 0.10370176300253019 0.877616145603742 0.002619850109321028
    Input activations: 0.19690092624905486 0.010417908693358034 0.027986050175275872 0.20093141326297165
    Input activations: 0.9292878329581585 0.004121227686684533 2.551285378659993E-5 0.8795663638869882
    Input activations: 0.00914936865110522 0.08511172178084442 0.8169492828865814 0.003926541182181483
    Input activations: 0.28721193517010724 0.008378369614674551 0.012492827526767168 0.29758637077746364
    Input activations: 0.8322664612722026 0.005130937458324137 1.4300352034621693E-4 0.7635614497704052
    Input activations: 0.0233596783506766 0.04777953239195229 0.5059889759367543 0.01280971603764583
    Input activations: 0.7730966838844642 0.004611573278026752 3.0782307081212806E-4 0.7399406139828288
    Input activations: 0.03477991031437265 0.03694706348717198 0.352429331104904 0.021266369608348032
    Input activations: 0.9006707458124611 0.0039205771128386436 5.281147646165486E-5 0.8592022464836002
    Input activations: 0.011375330712306632 0.07354630356407722 0.7620558175220664 0.005245736010955506
    Input activations: 0.3781674414274391 0.007185608213140369 0.006268712450624729 0.3885327969892769
    Input activations: 0.6461294291264378 0.006759221808279777 8.159414187480456E-4 0.570651845960926
    Input activations: 0.13412962746549656 0.018122093752548835 0.0484798494040271 0.09854950995122283
    Input activations: 0.9655402615329624 0.003697896081493577 6.464892192531853E-6 0.928347550051083
    Input activations: 0.0064103131867232705 0.10695386580429578 0.8849009924540654 0.0024615646186834377
    
    Network: 4 2 4
    #IH
    5 2
    -0.8171727808639266 -0.9028794212918296
    -1.439670071128536 -1.193574194263798
    -0.6995050620802388 -1.037454526552041
    -0.5848257987551353 -0.8784868694774733
    -1.7120970399349165 0.1592894583753889
    #HO
    3 4
    10.223384129453361 -27.888484201910554 -0.9846704321926415 32.63056815222108
    19.78091421855231 13.84278180659581 -37.80160232960633 -1.8506399956871178
    -13.333171109912616 8.291077463281864 9.759257961555308 -20.515461413796675
    #OH
    5 2
    -0.03464567840787709 -0.049256827526341584
    -0.3469060488314887 0.5269954286980081
    -0.27067776592899506 -0.7933265523800431
    0.31968861027927153 0.012253930282992445
    2.8528495537086562 -0.0504945737575051
    #HI
    3 4
    10.416944580086014 -28.13221736372574 -4.81331751550509 33.97824298384033
    19.969852695626884 14.045415087138482 -45.14972042960504 -1.4493868589488141
    -12.91825094203023 8.028479134891404 11.55277684136181 -20.8017219272834

  === example 2: ==== (not enough iterations) 
    backwardPassWithRecirculation : 0.0 0.0 0.0 1.0
    RUN_ID: auto4_1394659338449_2
    Epoch:  1017
    Max fluctuation: 0.07981192045322621
    Iteration count: 20
    Recirc epsilon : 0.0010
    Hidden activations: 0.47594578152917394 0.970390045232073 1.0
    Hidden activations: 0.48131157321414375 0.894808632145404 1.0
    Hidden activations: 0.48304398910381946 0.8926403553692763 1.0
    Hidden activations: 0.4847732509523974 0.8914838491876135 1.0
    Hidden activations: 0.48658567233598016 0.8902668689066595 1.0
    Hidden activations: 0.4885245967125305 0.8889489977816366 1.0
    Hidden activations: 0.49064309782905846 0.8874901336755067 1.0
    Hidden activations: 0.49301024641163893 0.8858366352808646 1.0
    Hidden activations: 0.49572010553296436 0.8839133697006313 1.0
    Hidden activations: 0.4989063154687979 0.881610431973861 1.0
    Hidden activations: 0.5027669235639085 0.8787595619397891 1.0
    Hidden activations: 0.5076087462245324 0.8750898206160953 1.0
    Hidden activations: 0.5139307044214692 0.8701387788736785 1.0
    Hidden activations: 0.5225884761041375 0.8630611595287786 1.0
    Hidden activations: 0.5351334975497106 0.8521827811194121 1.0
    Hidden activations: 0.554507891519271 0.8338881444323303 1.0
    Hidden activations: 0.5861868700834111 0.799926584969023 1.0
    Hidden activations: 0.6374746335251835 0.7347630504547686 1.0
    Hidden activations: 0.7032572038525674 0.6498916741217261 1.0
    Hidden activations: 0.7519574663484819 0.6440519992253361 1.0
    Input activations: 9.781281087898542E-6 0.9975177910878938 0.06640083314558627 0.1655444860153801
    Input activations: 1.4123104638236708E-5 0.9914055376280678 0.07239856144998966 0.17328992879623067
    Input activations: 1.3809863370799495E-5 0.9910243998199415 0.07207583126636036 0.17803203396888445
    Input activations: 1.3418070420018144E-5 0.9907772296268622 0.07164996857398771 0.1829677481190695
    Input activations: 1.3019761899327701E-5 0.990510196755561 0.07120662259718397 0.18825360099040764
    Input activations: 1.260800868456603E-5 0.9902135481112123 0.0707367789709498 0.1940357209535036
    Input activations: 1.2174454329230969E-5 0.9898758566168914 0.07022863166079323 0.2005043517179501
    Input activations: 1.1709364387382958E-5 0.9894809391241691 0.06966721233344897 0.2079191594966661
    Input activations: 1.1200869079658161E-5 0.9890047745349372 0.06903266768021013 0.21664987500161081
    Input activations: 1.0633966296999813E-5 0.9884098930097167 0.06829757070227625 0.2272456968328944
    Input activations: 9.989209449208035E-6 0.9876344223928505 0.06742255878561473 0.24055976741137083
    Input activations: 9.241077739348413E-6 0.986568936110909 0.06634900131256197 0.2579842390983894
    Input activations: 8.356380182190558E-6 0.9850024419431498 0.0649863304497723 0.28191844979436737
    Input activations: 7.294321221195173E-6 0.9824793840570103 0.06318996415613815 0.31674632063082636
    Input activations: 6.0139840802287135E-6 0.9778521029198356 0.06072470803325819 0.37089897861977517
    Input activations: 4.506334688385422E-6 0.9675239276184154 0.05721966521558519 0.460558876170297
    Input activations: 2.8842544928199103E-6 0.9361731736466616 0.05220804747581589 0.6093559483092945
    Input activations: 1.493944314532945E-6 0.7980152742423021 0.04565386948478164 0.8043187302654795
    Input activations: 6.47812124546015E-7 0.4184045178583433 0.038463764701518105 0.9343411880003744
    Input activations: 2.431072051248428E-7 0.3385925974051171 0.03105382132094775 0.9737409431823713
    
    Network: 4 2 4
    #IH
    5 2
    -0.7364658580179757 -1.7107271078061996
    -0.1563096103768036 -0.8282141827600178
    -0.8008779530999657 -1.1876845486723824
    1.3930421259175396 -2.680370106440119
    -0.6529693735945469 -2.4670958617383048
    #HO
    3 4
    -21.137466464182616 -4.414053853229933 -4.040931974702346 19.426235299461673
    -5.7746508878037766 13.96949441983745 -3.068150023862361 1.6821845756951619
    4.376641018480419 -6.410090046665403 1.6549997186103766 -12.09442879513631
    #OH
    5 2
    -1.5820073789462286 0.3592983118498677
    -0.2498110809795749 -0.5007282688924879
    -1.150707920022549 -0.24706869367114642
    0.026913056196237448 2.4293471797696853
    0.5297651114591365 3.527336166610928
    #HI
    3 4
    -20.885714491936078 -5.049370841890192 -4.73952127914221 19.74728250584106
    -6.343019170012237 16.15505218293986 -1.5659027443337816 0.6735558295772592
    4.560640055371324 -7.27735393678636 1.131954009384598 -11.669787153970704
 
  ==== Example 3: =============
    ITERATIONS_MAX_COUNT: 200 
    Max fluctuation still could be arbitrary and oscilating: 
    Max fluctuation: 0.05092955742474292
    Max fluctuation: 0.051553244332624565
    Max fluctuation: 0.05346136161210835
    Max fluctuation: 0.054174860556223725
    Max fluctuation: 0.05426257449369554
    Max fluctuation: 0.0576316938104105
    Max fluctuation: 0.05955840624439024
    Max fluctuation: 0.061599122214289426
    Max fluctuation: 0.06255860181952605
    Max fluctuation: 0.06526186126468048
    Max fluctuation: 0.06615001375803692
    Max fluctuation: 0.06725885255447184
    Max fluctuation: 0.07815891848279999
    Max fluctuation: 0.08409050293586162
    Max fluctuation: 0.09933932087774155
    Max fluctuation: 0.10788427593695982
    Max fluctuation: 0.16750475832410128
    Max fluctuation: 0.17357008332976598
    Max fluctuation: 0.23901025923711422
    Max fluctuation: 0.2937524082077899
    Max fluctuation: 0.3524379664813937
    Max fluctuation: 0.39542680913068207
    Max fluctuation: 0.4056852107085285
    Max fluctuation: 0.4329001094174422
    Max fluctuation: 0.7737428278602867
    Max fluctuation: 0.8051458889923122
    Max fluctuation: 0.9110664442238358
      Hidden activations: 0.03465186843478441 0.6093857541346955 1.0
      Hidden activations: 0.29281230851121 0.2892777779051186 1.0
      Hidden activations: 0.03465186843389666 0.6093857541318218 1.0
      Hidden activations: 0.2928123085030569 0.28927777790564996 1.0
      Hidden activations: 0.034651868433166255 0.6093857541294567 1.0
      Input activations: 0.2960729585399346 0.5718192802211801 0.08661728626245098 1.6482627457028477E-4
      Input activations: 0.8638523751407984 0.043446734369799116 2.1210090585001447E-6 0.9112312705193157
      Input activations: 0.2960729585552001 0.5718192802206216 0.08661728625624097 1.6482627457019226E-4
      Input activations: 0.863852375154124 0.0434467343728438 2.121009058627981E-6 0.9112312704984059
      Input activations: 0.29607295856776306 0.5718192802201613 0.08661728625112997 1.6482627457011764E-4
      
      epoch	err	sigma	lambda	momentum	h_dist	h_f_b_dist	m_avg_w	m_sim	in_triangle	fluctuation
      0	10	1	0.7	0.1	0.1687193585	0.4530546258	0.6808077641	1.4268498681	0	3.756703179919052E-4
      100	4	1	0.7	0.1	0.1506214792	0.0336149143	1.6104627627	1.3771077702	0	8.823902116354976E-4
      200	3	1	0.7	0.1	0.1746237865	0.0408102866	2.1414962975	1.3862310384	0	9.594626938489448E-4
      300	3	1	0.7	0.1	0.1820660063	0.0560339186	2.6240493276	1.3868269376	0	9.187654854000415E-4
      400	3	1	0.7	0.1	0.1845848069	0.056410632	3.0275817156	1.4243211844	0	9.367949996583746E-4
      500	1	1	0.7	0.1	0.1885709668	0.0851536396	3.3309012099	1.5005885247	0	9.432381709987836E-4
      600	3	1	0.7	0.1	0.1862156103	0.1150466341	3.5415581303	1.515184561	0	8.642271582453009E-4
      700	3	1	0.7	0.1	0.1875551694	0.1208563885	3.7164360044	1.5475629441	0	9.743687891316244E-4
      800	1	1	0.7	0.1	0.1918434923	0.0864105559	3.9090318752	1.6396062485	0	7.833316617016223E-4
      900	3	1	0.7	0.1	0.1880508613	0.124445158	4.0548708688	1.7084418204	0	7.423079929776422E-4
      1000	1	1	0.7	0.1	0.1925623465	0.0871298926	4.2042209381	1.7840792779	1	9.81835948200116E-4
      1100	1	1	0.7	0.1	0.1921349364	0.084698029	4.3633076618	1.8760276892	0	8.349892535689518E-4
      1200	3	1	0.7	0.1	0.1555342985	0.1469412468	4.4911273691	1.9458147867	0	9.737801059146456E-4
      1300	3	1	0.7	0.1	0.1556650443	0.1497758708	4.6050591955	2.018219444	0	9.085415843591571E-4
      1400	1	1	0.7	0.1	0.1936006225	0.0121851493	4.6749312978	2.0521271879	1	8.67602320155314E-4
      1500	3	1	0.7	0.1	0.157241957	0.0827518074	4.5250960327	1.807450061	0	9.973148091556028E-4
      1600	5	1	0.7	0.1	0.1574327958	0.1158262081	4.5597284912	1.8215161955	0	7.756440853018964E-4
      1700	1	1	0.7	0.1	0.192365326	0.0101481658	4.6780532852	1.9150121161	0	7.422606949347849E-4
      1800	1	1	0.7	0.1	0.1929826454	0.0093968428	4.7548486358	1.9595409169	0	7.483873404116222E-4
      1900	3	1	0.7	0.1	0.1860024941	0.0443897768	4.7867422573	1.9708070039	0	7.310310516153162E-4
      2000	1	1	0.7	0.1	0.1929063788	0.0058621785	4.8805279723	2.0448403848	0	9.778628005294998E-4

      Network: 4 2 4
        #IH
        5 2
        -3.382064196231074 0.9552198739601303
        -0.8152527154889451 0.04874745732108573
        -1.9001594097281804 1.9276377784754586
        -1.2298183122868296 1.09101874817068
        -2.8101561976579017 3.011857485203263
        #HO
        3 4
        -18.694170061673173 -8.583692373324809 -3.830792670528463 30.119817471134688
        -16.913236498903213 3.274929204256274 28.805848665164895 -8.278839655166943
        11.340252859544027 -1.421792775257793 -20.254020040528143 -4.527016074789195
        #OH
        5 2
        1.583683934318635 -2.127682641665579
        -0.22737983295343242 -0.42266897566549666
        1.9030471712390107 -1.0509303698069399
        1.8018621806648243 -1.2701435259333402
        1.9770123066318896 -2.260905206994192
        #HI
        3 4
        -15.250926634766934 -8.757673237286788 -5.501665207791351 31.09284323832752
        -20.77710446049136 3.4994243814713264 29.01411179876515 -9.410180618166903
        12.323675762656713 -1.5397519760735414 -19.845798989404948 -4.053448780877543
        
        
=================== Implementing GeneRec in Java ========================= 
  We decided that the most straighforward solution is to make the matrices symmetric (copy values after learning) 
  The main questions were: 
    How to implement bias (it is not in the article) 
    Parameters (lambda, sigma, ...) 
    
    
  INTERESTING: when bias only on input:
    4-2-4 best network error = 3.0 (tended to have two very profiled (0.95+, 0, 0, 0) and two very even (0.5, 0.5, 0.5, 0.5) 
    4-3-4 about 75% success rate 
    
             : when adding bias on hidden to output: 
    4-2-4 0.0 1.0 0.5 0.1 91.74311926605505 100/109, almost no 1.0 errors 
          epochs ranged from 9 to 4500
    4-3-4 about 100% succes rate with about 20 epochs (ranged from 5 to 700) 
    
             : when bias on each matrix: 
    4-2-4: seems to add a little chaos with a slighty worse performance (but maybe bad sigma) 
      err sigma lambda momentum success sample_ratio
      0.0 1.0 0.03 0.1 84.15841584158416 85/101
      0.0 1.0 0.1 0.1 88.17204301075269 82/93
      0.0 1.0 0.3 0.1 86.3013698630137 63/73
      0.0 1.0 0.5 0.1 75.49019607843137 77/102
      0.0 1.0 0.7 0.1 75.30864197530865 61/81
      0.0 1.0 0.9 0.1 88.23529411764706 90/102
      0.0 1.0 1.1 0.1 73.5632183908046 64/87
      0.0 1.0 1.3 0.1 75.0 60/80
      0.0 1.0 1.5 0.1 68.57142857142857 48/70
      0.0 1.0 1.8 0.1 68.93203883495146 71/103
      0.0 1.0 2.2 0.1 49.074074074074076 53/108

=================== Trying ForBackWard GeneRec (after having confidence in implementation of Generec)       
Symmetric weights haven't helped BAL (35%) - no param selection
Using bothward GeneRec (60%) - no param selection 
  Forgot to symmetric init -> no perceivable change 
In both cases (almost) no fluctuation 

Non-symmetric case, fluctuation in about 1/5 cases 
  About 30% success 
  About 3-33 iterations needed to settle, very network dependent 
  About 50 - 5000 epochs needed 

IDEA: maybe bad implementation of backward
      using iterative activation has almost no reason (bothward is the generec idea) 
      
      
===================== 17th March ======================
CHL success rate: 75.4% 
Avg epochs: 689 (10,000 max) 

=========================
Seems like "a_post_other.getEntry(j) - a_post_self.getEntry(j)" is not changeing after

== Basic - Trying dynamic lambda (49.6% basic) 
err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.1 49.6 248/500
1.0 2.3 0.7 0.1 40.2 201/500
2.0 2.3 0.7 0.1 9.4 47/500
3.0 2.3 0.7 0.1 0.6 3/500
4.0 2.3 0.7 0.1 0.2 1/500

Avg epochs: 1909

==		return init_lambda * Math.max(1.0, (100 / (epochs + 50))); 
err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.1 48.8 244/500
1.0 2.3 0.7 0.1 38.2 191/500
2.0 2.3 0.7 0.1 11.600000000000001 58/500
3.0 2.3 0.7 0.1 1.0 5/500
4.0 2.3 0.7 0.1 0.4 2/500

Avg epochs: 1742

==		return init_lambda * Math.max(1.0, (500 / (epochs + 250))); 
err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.1 47.0 235/500
1.0 2.3 0.7 0.1 38.6 193/500
2.0 2.3 0.7 0.1 14.000000000000002 70/500
3.0 2.3 0.7 0.1 0.2 1/500
4.0 2.3 0.7 0.1 0.2 1/500

Avg epochs: 1578

==		return init_lambda * 0.1 * Math.max(1.0, 20 * last_err); 
err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.1 19.8 99/500
1.0 2.3 0.7 0.1 39.0 195/500
2.0 2.3 0.7 0.1 26.0 130/500
3.0 2.3 0.7 0.1 11.200000000000001 56/500
4.0 2.3 0.7 0.1 4.0 20/500

Avg epochs: 3785

==    return init_lambda * 0.1 * Math.max(1.0, 20 * Math.abs(last_err)); 
err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.1 25.0 125/500
1.0 2.3 0.7 0.1 35.4 177/500
2.0 2.3 0.7 0.1 29.599999999999998 148/500
3.0 2.3 0.7 0.1 7.8 39/500
4.0 2.3 0.7 0.1 2.1999999999999997 11/500

==  	return init_lambda * Math.max(1.0, 20 * Math.abs(last_err)); 
err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.1 1.4000000000000001 7/500
1.0 2.3 0.7 0.1 3.8 19/500
2.0 2.3 0.7 0.1 15.8 79/500
3.0 2.3 0.7 0.1 20.8 104/500
4.0 2.3 0.7 0.1 54.400000000000006 272/500
5.0 2.3 0.7 0.1 2.8000000000000003 14/500
6.0 2.3 0.7 0.1 0.8 4/500
8.0 2.3 0.7 0.1 0.2 1/500

Avg epochs: 2194

==		return init_lambda * Math.max(0.5, 2 * Math.abs(last_err)); 
err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.1 47.4 237/500
1.0 2.3 0.7 0.1 38.4 192/500
2.0 2.3 0.7 0.1 12.6 63/500
3.0 2.3 0.7 0.1 1.4000000000000001 7/500
4.0 2.3 0.7 0.1 0.2 1/500

Avg epochs: 2100

========================= 24th of March ======================
Text: 
- BAL (roughly) 
- Datasets
- notes -> methodology & results & analysis 

Tried training only on samples which are not learned:
INIT_TRAIN_ONLY_ON_ERROR = true 

And it lead to significant improvement (over 10\%) 

err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.1 62.0 124/200
1.0 2.3 0.7 0.1 21.5 43/200
2.0 2.3 0.7 0.1 11.0 22/200
3.0 2.3 0.7 0.1 4.5 9/200
4.0 2.3 0.7 0.1 1.0 2/200

Avg epochs: 2251

Vau, such result. TODO: find citation "neural networks training on error samples" - not found. 


TODO: Analyse why stop learning in error state (there is also difference between expected and given on HO and HI) 
- but activations on hidden seems settled (print weight update matrices) 



=========================== 25th of March (36d to go) ====================
Examining activations closely. 

Activations on the hidden layer tend to be same 
and IH \approx OH and HI \approx HO (bias makes the difference) 

IH, OH tend to have low weights, but HI and HO arbitrary big (pushing to 0.9999... and 0.000... resp.) 

When SHUFFLE_IS = false && INIT_TRAIN_ONLY_ON_ERROR = true then we observed oscialtion between several states (when activations on the hidden layer where settled) 

====== Without shuffle and INIT_TRAIN_ONLY_ON_ERROR = true: 
PostMeasure : GroupBy
err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.0 59.099999999999994 591/1000
1.0 2.3 0.7 0.0 29.299999999999997 293/1000
2.0 2.3 0.7 0.0 8.9 89/1000
3.0 2.3 0.7 0.0 2.5 25/1000
4.0 2.3 0.7 0.0 0.2 2/1000

Avg epochs: 2146

====== With shuffle and INIT_TRAIN_ONLY_ON_ERROR = true: 
PostMeasure : GroupBy
err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.0 61.5 615/1000
1.0 2.3 0.7 0.0 23.0 230/1000
2.0 2.3 0.7 0.0 12.0 120/1000
3.0 2.3 0.7 0.0 3.4000000000000004 34/1000
4.0 2.3 0.7 0.0 0.1 1/1000

Avg epochs: 2052

====== Without shuffle and INIT_TRAIN_ONLY_ON_ERROR = false: 
err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.0 57.199999999999996 572/1000
1.0 2.3 0.7 0.0 32.1 321/1000
2.0 2.3 0.7 0.0 10.2 102/1000
3.0 2.3 0.7 0.0 0.4 4/1000
4.0 2.3 0.7 0.0 0.1 1/1000

Avg epochs: 3014


====== With shuffle and INIT_TRAIN_ONLY_ON_ERROR = false: 
PostMeasure : GroupBy
err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.0 62.4 624/1000
1.0 2.3 0.7 0.0 30.2 302/1000
2.0 2.3 0.7 0.0 7.1 71/1000
3.0 2.3 0.7 0.0 0.3 3/1000

Avg epochs: 2182

INTERESTING => Shuffle helps success and epochs, INIT_TRAIN_ONLY_ON_ERROR helps epochs

TODO: more investigation on why not learned 
====== With shuffle and INIT_TRAIN_ONLY_ON_ERROR = false === 
== How last activations looked like 
