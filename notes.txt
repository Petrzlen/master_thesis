Prve stretnutie 11.11.2012
!! zapisovat, zapisovat – hned ten isty den (Misof, PG)

Geofrey Hinton – department of computer science Toronto, aplikacny
Randall O'Reilly – department of psychology Pittsburgh, prehlad, snazi sa simulovat ludsky mozog
v USA su takyto vedci vzdelany vseobecnym prehladom

ukazal som mu kaggle, 
andrew ng,
amsterdam – ML oriented

dal mi prefotene clanky – kde ich sakra mam? 

Oni sa na katedre zaoberaju nasadenim existujucich na ich problemy.
Ma jednu sikovnu doktorantku. 

Ma dobre skusenosti so spolupracou s teoretickymi informatikmi.

Moja predstava je trocha teorie – skumanie novych sieti, zistovanie domen, modifikovanie a znovuskumanie. Implementacie su (a preto nie su problem) 

!!Slubil som mu, ze o tyzden. dat si to ako buduco tyzdnovu prioritu

Neuronove siete su netrivialnymi zobrazeniami medzi domenami, je tam vela matiky a teorie a preto je tam co skumat / dokazovat / vymyslat

Mojim cielom bola praca na rozhrani vyskumu a implementacie pre vhodnu mnozinu praktickych problemov. 

24.10.2012
misof diplomovy seminar
prilis univerzalne kladivo (ked uz ine postupy zlyhavaju)
google ich ale pouzil na naozaj velkych datach a zacali funogvat dobre (napr. Youtube videa obsahujuce macky) 
najdolezitejsie je vediet dokazat vlastny prinos, ktory vyzadoval netrivialne mentalne usilie

7.11.2012
--15 stran v skuskovom s zakladnym prehladom tematiky – kvalita je na garantovi, prvy dojem na rovanovi 
UI – vela malych elementov co dava dokopy
formalizmus ak prinesie novy pohlad je super vec

14.11.2012 (Misof)
Ako pisat?
Latex
Vedieť dokázať vlastný prínos a netriviálne úsilie vedúce k nemu. 
Odvolávať sa na existujúcu lieteratúru. 
21.11.2012 nie

27.11.2012
https://www.ideals.illinois.edu/handle/2142/32512 – improve performance: Using feature construction to improve the performance of neural networks/1993: 149

predpoved ceny
data ming: harvesting reviews and grouping 
http://archive.ics.uci.edu/ml/ 


koncept:
zobrat existujuci model
potweakovat

12.12.2012
Hinton2012: 

Snazi riesit problem overfittingu na relativne malych problemoch

Je prevenciou co-adaptation a simuluje model viacerych neuronovych sieti natrenovanych na rovnaky problem

?Softmax output layer for computing the probabilities

-best result 160 errors from 10000 on test set
--130 with 50% dropout on hidden layer
--110 futher with 20% dropout on input set
Deep boltzman machine with dropout is the best result so far. 

?convolutional neural network
?pre-training extract useful features
?deep boltzman machine 
?hidden markov models
?viterbi algorithm
?max-pooling layer
?mixture of experts
?bayesian model averaging
?markov chain monte carlo
?mean net

We use the standard, stochastic gradient descent procedure for training the dropout neural 
networks on mini-batches of training cases, but we modify the penalty term that is normally 
used to prevent the weights from growing too large. Instead of penalizing the squared length 
(L2 norm) of the whole weight vector, we set an upper bound on the L2 norm of the incoming 
weight vector for each individual hidden unit. If a weight-update violates this constraint, we 
renormalize the weights of the hidden unit by division. Using a constraint rather than a penalty 
prevents weights from growing very large no matter how large the proposed weight-update is. 
This makes it possible to start with a very large learning rate which decays during learning, 
thus allowing a far more thorough search of the weight-space than methods that start with small 
weights and use a small learning rate. 

Performance on the test set can be greatly improved by
enhancing the training data with transformed images (3) or by wiring knowledge about spatial
transformations into a convolutional neural network (4) or by using generative pre-training to
extract useful features from the training images without using the labels (5).

Speech recognition systems use hidden Markov models (HMMs) to
deal with temporal variability and they need an acoustic model that determines how well a frame
of coefﬁcients extracted from the acoustic input ﬁts each possible state of each hidden Markov
model. 

For fully connected layers, dropout in all hidden layers works
better than dropout in only one hidden layer and more extreme probabilities tend to be worse,
which is why we have used 0.5 throughout this paper.

Zaujimave: For datasets in which the required input-output mapping has a
number of fairly different regimes, performance can probably be further improved by making
the dropout probabilities be a learned function of the input, thus creating a statistically efﬁcient
“mixture of experts” (13) in which there are combinatorially many experts, but each parameter
gets adapted on a large fraction of the training data.

Diplomovy seminar 12.12.2012

Too jitteri, dobra prezentacia, zabudol som meno skolitela, 
Super, ze to funguje. Zaujimava je este otazka preco to vlastne funguje – feature detectors. 

Prvy slide: nazov, meno, skolitel
Druhy slide : obsah
Dalsie slide-i: sucasny stav problematiky
Slide: co este porobime
5-7min = proof of work , uvod, 

Dat aktualny stav na web


================ Marec 2013 ====================
implementovany generec v C++ 

================ Jul 2013 ======================
Multilayer generec 42% - je to v slideoch

================ Oktober 2013 ==================
-implementovany BAL (zabudnute bias neuron, inicialicia divna)
-zrekonstruovane vysledky z clanku 
-meranie vlastnosti BAL (hidden layer distance, matrix similarity, ... ) /bal/data/... 
  reprezentacie na hidden absolutne rovnake (forward, backward), matice rozne: f_h_dist_0_m_sim_not_0.txt
-experimentovanie s init vahami aby sa zvysil predpoklad uspechu 
  data a grafy v /bal/data/...

-vyzera to tak, ze model pracuje lepsie ked init reprezentacie su dalej od seba (rapidminer/bal1_diagram) 
  -bipolarna sigmoida nepomaha
  -vybratie siete s veacsim hdist pomaha (62% - 70%) 
  
=============== November 2013 ===================
3 neurony na hidden ma:
 err sigma lambda
0.0 1.8 0.8 163/165 98.7878787878788%
0.0 1.8 1.0 160/162 98.76543209876543%
0.0 1.8 1.2 162/162 100.0%
0.0 1.8 1.4 156/158 98.73417721518987%
0.0 1.8 1.6 141/143 98.6013986013986%
0.0 1.8 1.8 149/154 96.75324675324676%
0.0 1.8 2.0 166/167 99.40119760479041%
0.0 1.8 2.2 160/162 98.76543209876543%
0.0 2.0 0.8 158/159 99.37106918238993%
0.0 2.0 1.0 150/154 97.40259740259741%
0.0 2.0 1.2 162/163 99.38650306748467%
0.0 2.0 1.4 164/166 98.79518072289156%
0.0 2.0 1.6 150/151 99.33774834437085%
0.0 2.0 1.8 152/154 98.7012987012987%
0.0 2.0 2.0 134/135 99.25925925925925%
0.0 2.0 2.2 160/163 98.15950920245399%
0.0 2.2 0.8 142/142 100.0%
0.0 2.2 1.0 151/153 98.69281045751634%
0.0 2.2 1.2 159/161 98.75776397515527%
0.0 2.2 1.4 156/156 100.0%
0.0 2.2 1.6 163/163 100.0%
0.0 2.2 1.8 155/156 99.35897435897436%
0.0 2.2 2.0 201/203 99.01477832512316%
0.0 2.2 2.2 134/136 98.52941176470588%
0.0 2.4 0.8 163/166 98.19277108433735%
0.0 2.4 1.0 163/164 99.39024390243902%
0.0 2.4 1.2 158/160 98.75%
0.0 2.4 1.4 143/144 99.30555555555556%
0.0 2.4 1.6 146/149 97.98657718120806%
0.0 2.4 1.8 162/164 98.78048780487805%
0.0 2.4 2.0 153/154 99.35064935064936%
0.0 2.4 2.2 149/155 96.12903225806451%
0.0 2.6 0.8 138/140 98.57142857142858%
0.0 2.6 1.0 140/141 99.29078014184397%
0.0 2.6 1.2 149/151 98.67549668874173%
0.0 2.6 1.4 165/167 98.80239520958084%
0.0 2.6 1.6 161/162 99.38271604938271%
0.0 2.6 1.8 161/162 99.38271604938271%
0.0 2.6 2.0 157/159 98.74213836477988%
0.0 2.6 2.2 130/134 97.01492537313433%
0.0 2.8 0.8 152/154 98.7012987012987%
0.0 2.8 1.0 168/171 98.24561403508771%
0.0 2.8 1.2 159/161 98.75776397515527%
0.0 2.8 1.4 136/140 97.14285714285714%
0.0 2.8 1.6 147/151 97.35099337748345%
0.0 2.8 1.8 167/168 99.40476190476191%
0.0 2.8 2.0 144/149 96.64429530201343%
0.0 2.8 2.2 148/153 96.73202614379085%
0.0 3.0 0.8 157/157 100.0%
0.0 3.0 1.0 168/172 97.67441860465115%
0.0 3.0 1.2 175/177 98.87005649717514%
0.0 3.0 1.4 150/152 98.68421052631578%
0.0 3.0 1.6 138/139 99.28057553956835%
0.0 3.0 1.8 151/151 100.0%
0.0 3.0 2.0 176/177 99.43502824858757%
0.0 3.0 2.2 158/163 96.93251533742331%
0.0 3.2 0.8 145/146 99.31506849315068%
0.0 3.2 1.0 154/162 95.06172839506173%
0.0 3.2 1.2 148/150 98.66666666666667%
0.0 3.2 1.4 148/154 96.1038961038961%
0.0 3.2 1.6 132/134 98.50746268656717%
0.0 3.2 1.8 166/167 99.40119760479041%
0.0 3.2 2.0 141/143 98.6013986013986%
0.0 3.2 2.2 144/149 96.64429530201343%
1.0 1.8 0.8 2/165 1.2121212121212122%
1.0 1.8 1.0 2/162 1.2345679012345678%
1.0 1.8 1.4 2/158 1.2658227848101267%
1.0 1.8 1.6 2/143 1.3986013986013985%
1.0 1.8 1.8 4/154 2.5974025974025974%
1.0 1.8 2.0 1/167 0.5988023952095809%
1.0 1.8 2.2 1/162 0.6172839506172839%
1.0 2.0 0.8 1/159 0.628930817610063%
1.0 2.0 1.0 3/154 1.948051948051948%
1.0 2.0 1.2 1/163 0.6134969325153374%
1.0 2.0 1.4 1/166 0.6024096385542169%
1.0 2.0 1.6 1/151 0.6622516556291391%
1.0 2.0 1.8 2/154 1.2987012987012987%
1.0 2.0 2.0 1/135 0.7407407407407408%
1.0 2.0 2.2 3/163 1.8404907975460123%
1.0 2.2 1.0 1/153 0.6535947712418301%
1.0 2.2 1.2 2/161 1.2422360248447204%
1.0 2.2 1.8 1/156 0.641025641025641%
1.0 2.2 2.0 2/203 0.9852216748768473%
1.0 2.2 2.2 2/136 1.4705882352941175%
1.0 2.4 0.8 3/166 1.8072289156626504%
1.0 2.4 1.0 1/164 0.6097560975609756%
1.0 2.4 1.2 2/160 1.25%
1.0 2.4 1.4 1/144 0.6944444444444444%
1.0 2.4 1.6 3/149 2.013422818791946%
1.0 2.4 1.8 2/164 1.2195121951219512%
1.0 2.4 2.0 1/154 0.6493506493506493%
1.0 2.4 2.2 4/155 2.5806451612903225%
1.0 2.6 0.8 2/140 1.4285714285714286%
1.0 2.6 1.0 1/141 0.7092198581560284%
1.0 2.6 1.2 1/151 0.6622516556291391%
1.0 2.6 1.4 2/167 1.1976047904191618%
1.0 2.6 1.6 1/162 0.6172839506172839%
1.0 2.6 1.8 1/162 0.6172839506172839%
1.0 2.6 2.0 2/159 1.257861635220126%
1.0 2.6 2.2 2/134 1.4925373134328357%
1.0 2.8 0.8 2/154 1.2987012987012987%
1.0 2.8 1.0 3/171 1.7543859649122806%
1.0 2.8 1.2 2/161 1.2422360248447204%
1.0 2.8 1.4 4/140 2.857142857142857%
1.0 2.8 1.6 4/151 2.6490066225165565%
1.0 2.8 1.8 1/168 0.5952380952380952%
1.0 2.8 2.0 5/149 3.3557046979865772%
1.0 2.8 2.2 5/153 3.2679738562091507%
1.0 3.0 1.0 4/172 2.3255813953488373%
1.0 3.0 1.2 2/177 1.1299435028248588%
1.0 3.0 1.4 2/152 1.3157894736842104%
1.0 3.0 2.0 1/177 0.5649717514124294%
1.0 3.0 2.2 4/163 2.4539877300613497%
1.0 3.2 0.8 1/146 0.684931506849315%
1.0 3.2 1.0 8/162 4.938271604938271%
1.0 3.2 1.2 2/150 1.3333333333333335%
1.0 3.2 1.4 5/154 3.2467532467532463%
1.0 3.2 1.6 2/134 1.4925373134328357%
1.0 3.2 1.8 1/167 0.5988023952095809%
1.0 3.2 2.0 2/143 1.3986013986013985%
1.0 3.2 2.2 4/149 2.684563758389262%
2.0 1.8 1.8 1/154 0.6493506493506493%
2.0 1.8 2.2 1/162 0.6172839506172839%
2.0 2.0 1.0 1/154 0.6493506493506493%
2.0 2.0 1.4 1/166 0.6024096385542169%
2.0 2.2 1.0 1/153 0.6535947712418301%
2.0 2.4 2.2 2/155 1.2903225806451613%
2.0 2.6 1.2 1/151 0.6622516556291391%
2.0 2.6 2.2 2/134 1.4925373134328357%
2.0 3.0 1.6 1/139 0.7194244604316548%
2.0 3.0 2.2 1/163 0.6134969325153374%
2.0 3.2 2.2 1/149 0.6711409395973155%
3.0 3.2 1.4 1/154 0.6493506493506493%

===CONVERGENCE EPSILON: 
public static final int INIT_MAX_EPOCHS = 30000;
public static final int INIT_RUNS = 1000; 
public static final int INIT_CANDIDATES_COUNT = 100;
	
conv_eps=0.003 (ABS WEIGHT)
 err sigma lambda
0.0 2.3 0.7 655/1000 65.5
1.0 2.3 0.7 264/1000 26.400000000000002
2.0 2.3 0.7 81/1000 8.1
avg(epoch)=21416.3
avg(err)=0.426
avg(h_dist)=0.3377208768354886
 avg(h_f_b_dist)=3.533469239792219E-8
avg(m_avg_w)=9.17389922619515
avg(m_sim)=0.6716337215384639
avg(first_second)=1191.0805852194187
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=1.2693376003173885E-4
	
conv_eps=0.01 (ABS WEIGHT)
 err sigma lambda
0.0 2.3 0.7 665/1000 66.5
1.0 2.3 0.7 282/1000 28.199999999999996
2.0 2.3 0.7 52/1000 5.2
3.0 2.3 0.7 1/1000 0.1
avg(epoch)=17174.083
avg(err)=0.389
avg(h_dist)=0.3402298305484247
avg(h_f_b_dist)=1.190306966621504E-6
avg(m_avg_w)=8.260697028353873
avg(m_sim)=0.6817923508697991
avg(first_second)=745.8397365477459
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=5.221918912727679E-4
	
conv_eps=0.03 (ABS WEIGHT)
 err sigma lambda
0.0 2.3 0.7 618/1000 61.8
1.0 2.3 0.7 303/1000 30.3
2.0 2.3 0.7 79/1000 7.9
avg(epoch)=12744.2
avg(err)=0.461
avg(h_dist)=0.34307441746879935
avg(h_f_b_dist)=3.730931210904762E-5
avg(m_avg_w)=6.559497108563506
avg(m_sim)=0.7192853722674895
avg(first_second)=461.7433056363167
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=0.003317823806944541

conv_eps=0.1 (ABS WEIGHT)
 err sigma lambda
0.0 2.3 0.7 91/1000 9.1
1.0 2.3 0.7 559/1000 55.900000000000006
2.0 2.3 0.7 278/1000 27.800000000000004
3.0 2.3 0.7 67/1000 6.7
4.0 2.3 0.7 5/1000 0.5
avg(epoch)=266.823
avg(err)=1.336
avg(h_dist)=0.34501134797359695
avg(h_f_b_dist)=0.004436944848110387
avg(m_avg_w)=2.4130610278020796
avg(m_sim)=1.08299640000091
avg(first_second)=15.024663218965912
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=0.06751217092624097


conv_eps=0.003 (REL WEIGHT)
 err sigma lambda
0.0 2.3 0.7 615/1000 61.5
1.0 2.3 0.7 296/1000 29.599999999999998
2.0 2.3 0.7 89/1000 8.9
avg(epoch)=11414.698
avg(err)=0.474
avg(h_dist)=0.3449922649825705
avg(h_f_b_dist)=8.357729465024385E-6
avg(m_avg_w)=6.3269007044953
avg(m_sim)=0.7109262416378718
avg(first_second)=342.3715979371622
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=0.0021038254278060134

==========MOMENTUM 
=========================================================
err sigma lambda momentumsuccess sample_ratio

0.0 2.3 0.7 -0.0010 67.40412979351032 457/678
0.0 2.3 0.7 -0.0030 68.24712643678161 475/696
0.0 2.3 0.7 -0.01 66.51583710407239 441/663
0.0 2.3 0.7 -0.03 68.50746268656717 459/670
0.0 2.3 0.7 -0.1 66.31259484066769 437/659
0.0 2.3 0.7 -0.3 68.51851851851852 444/648
0.0 2.3 0.7 0.0 67.64705882352942 460/680
0.0 2.3 0.7 0.0010 66.57223796033995 470/706
0.0 2.3 0.7 0.0030 68.04123711340206 462/679
0.0 2.3 0.7 0.01 70.10785824345146 455/649
0.0 2.3 0.7 0.03 66.81481481481481 451/675
0.0 2.3 0.7 0.1 67.0605612998523 454/677
0.0 2.3 0.7 0.3 69.24198250728864 475/686
0.0 2.3 0.7 1.0 67.26384364820846 413/614
1.0 2.3 0.7 -0.0010 25.958702064896755 176/678
1.0 2.3 0.7 -0.0030 24.137931034482758 168/696
1.0 2.3 0.7 -0.01 26.546003016591253 176/663
1.0 2.3 0.7 -0.03 24.029850746268657 161/670
1.0 2.3 0.7 -0.1 26.403641881638844 174/659
1.0 2.3 0.7 -0.3 25.462962962962965 165/648
1.0 2.3 0.7 0.0 26.61764705882353 181/680
1.0 2.3 0.7 0.0010 25.35410764872521 179/706
1.0 2.3 0.7 0.0030 25.773195876288657 175/679
1.0 2.3 0.7 0.01 23.728813559322035 154/649
1.0 2.3 0.7 0.03 25.48148148148148 172/675
1.0 2.3 0.7 0.1 27.474150664697195 186/677
1.0 2.3 0.7 0.3 24.34402332361516 167/686
1.0 2.3 0.7 1.0 27.36156351791531 168/614
10.0 2.3 0.7 -1.0 8.870967741935484 55/620
11.0 2.3 0.7 -1.0 5.483870967741936 34/620
12.0 2.3 0.7 -1.0 3.225806451612903 20/620
13.0 2.3 0.7 -1.0 0.4838709677419355 3/620
2.0 2.3 0.7 -0.0010 6.637168141592921 45/678
2.0 2.3 0.7 -0.0030 7.614942528735632 53/696
2.0 2.3 0.7 -0.01 6.93815987933635 46/663
2.0 2.3 0.7 -0.03 7.313432835820896 49/670
2.0 2.3 0.7 -0.1 7.283763277693475 48/659
2.0 2.3 0.7 -0.3 6.018518518518518 39/648
2.0 2.3 0.7 -1.0 0.3225806451612903 2/620
2.0 2.3 0.7 0.0 5.735294117647059 39/680
2.0 2.3 0.7 0.0010 7.790368271954675 55/706
2.0 2.3 0.7 0.0030 6.185567010309279 42/679
2.0 2.3 0.7 0.01 6.163328197226503 40/649
2.0 2.3 0.7 0.03 7.555555555555555 51/675
2.0 2.3 0.7 0.1 5.465288035450517 37/677
2.0 2.3 0.7 0.3 6.41399416909621 44/686
2.0 2.3 0.7 1.0 5.374592833876222 33/614
3.0 2.3 0.7 -0.03 0.1492537313432836 1/670
3.0 2.3 0.7 -1.0 1.129032258064516 7/620
3.0 2.3 0.7 0.0010 0.28328611898017 2/706
3.0 2.3 0.7 0.03 0.14814814814814814 1/675
4.0 2.3 0.7 -1.0 2.258064516129032 14/620
5.0 2.3 0.7 -1.0 7.903225806451612 49/620
6.0 2.3 0.7 -1.0 15.806451612903224 98/620
7.0 2.3 0.7 -1.0 19.193548387096772 119/620
8.0 2.3 0.7 -1.0 22.258064516129032 138/620
9.0 2.3 0.7 -1.0 13.064516129032258 81/620
avg(epoch)=30000.0
avg(err)=0.8439
avg(h_dist)=0.3499103246224569
avg(h_f_b_dist)=0.041112550125614586
avg(m_avg_w)=9.636579583625638
avg(m_sim)=0.792297487186741
avg(first_second)=2673.609889300811
avg(sigma)=2.299999999999577
avg(lambda)=0.6999999999998807
avg(o_f_b_dist)=0.056782865777391126
avg(momentum)=7.186999999999984E-4

============================================== 30.10.2013 =================
public static final double NORMAL_DISTRIBUTION_SPAN = 15; 

	public static final String INPUT_FILEPATH = "auto4.in"; 
	public static final String OUTPUT_FILEPATH = "auto4.in"; 
	public static final int INIT_HIDDEN_LAYER_SIZE = 2; 
	public static final int INIT_MAX_EPOCHS = 30000;
	public static final int INIT_RUNS = 10000; 
	public static final double INIT_NORMAL_DISTRIBUTION_MU = 0; 
	public static final double TRY_NORMAL_DISTRIBUTION_SIGMA[] = {0.1, 0.3, 1, 1.3, 1.7, 2, 2.3}; 
	public static final double TRY_LAMBDA[] = {0.001, 0.003, 0.01, 0.03, 0.1, 0.3}; 
	public static final double TRY_NOISE_SPAN[] = {0.0, 0.003, 0.01, 0.03, 0.1, 0.3}; 

- 0.328 * h_dist
+ 0.140 * h_f_b_dist
- 0.100 * m_avg_w
+ 0.019 * m_sim
+ 0.000 * first_second
- 0.127 * sigma
+ 2.912 * noise_span
+ 3.610

============================================================

 err sigma lambda
0.0 0.85 0.03 83/388 21.391752577319586\%
0.0 0.85 0.1 147/411 35.76642335766424\%
0.0 0.85 0.2 168/393 42.74809160305343\%
0.0 0.85 0.3 198/402 49.25373134328358\%
0.0 0.85 0.5 215/398 54.020100502512555\%
0.0 1.0 0.03 123/410 30.0\%
0.0 1.0 0.1 159/397 40.050377833753146\%
0.0 1.0 0.2 191/390 48.97435897435897\%
0.0 1.0 0.3 193/391 49.36061381074169\%
0.0 1.0 0.5 217/409 53.0562347188264\%
0.0 1.15 0.03 135/420 32.142857142857146\%
0.0 1.15 0.1 175/402 43.53233830845771\%
0.0 1.15 0.2 205/398 51.507537688442206\%
0.0 1.15 0.3 202/391 51.66240409207161\%
0.0 1.15 0.5 215/387 55.55555555555556\%
0.0 1.3 0.03 134/406 33.004926108374384\%
0.0 1.3 0.1 188/431 43.619489559164734\%
0.0 1.3 0.2 205/396 51.76767676767676\%
0.0 1.3 0.3 230/427 53.86416861826698\%
0.0 1.3 0.5 231/401 57.605985037406484\%
0.0 1.5 0.03 149/403 36.972704714640194\%
0.0 1.5 0.1 182/380 47.89473684210526\%
0.0 1.5 0.2 203/407 49.877149877149876\%
0.0 1.5 0.3 201/374 53.7433155080214\%
0.0 1.5 0.5 237/388 61.08247422680413\%
1.0 0.85 0.03 222/388 57.21649484536082\%
1.0 0.85 0.1 230/411 55.961070559610704\%
1.0 0.85 0.2 194/393 49.36386768447837\%
1.0 0.85 0.3 180/402 44.776119402985074\%
1.0 0.85 0.5 158/398 39.698492462311556\%
1.0 1.0 0.03 224/410 54.63414634146342\%
1.0 1.0 0.1 203/397 51.13350125944584\%
1.0 1.0 0.2 177/390 45.38461538461539\%
1.0 1.0 0.3 166/391 42.45524296675192\%
1.0 1.0 0.5 163/409 39.85330073349633\%
1.0 1.15 0.03 224/420 53.333333333333336\%
1.0 1.15 0.1 199/402 49.50248756218906\%
1.0 1.15 0.2 171/398 42.96482412060301\%
1.0 1.15 0.3 158/391 40.40920716112532\%
1.0 1.15 0.5 142/387 36.69250645994832\%
1.0 1.3 0.03 215/406 52.95566502463054\%
1.0 1.3 0.1 204/431 47.33178654292343\%
1.0 1.3 0.2 167/396 42.17171717171717\%
1.0 1.3 0.3 171/427 40.04683840749414\%
1.0 1.3 0.5 138/401 34.413965087281795\%
1.0 1.5 0.03 201/403 49.87593052109181\%
1.0 1.5 0.1 171/380 45.0\%
1.0 1.5 0.2 179/407 43.980343980343974\%
1.0 1.5 0.3 152/374 40.64171122994652\%
1.0 1.5 0.5 124/388 31.958762886597935\%
2.0 0.85 0.03 74/388 19.072164948453608\%
2.0 0.85 0.1 32/411 7.785888077858881\%
2.0 0.85 0.2 31/393 7.888040712468193\%
2.0 0.85 0.3 23/402 5.721393034825871\%
2.0 0.85 0.5 25/398 6.281407035175879\%
2.0 1.0 0.03 55/410 13.414634146341465\%
2.0 1.0 0.1 33/397 8.312342569269521\%
2.0 1.0 0.2 22/390 5.641025641025641\%
2.0 1.0 0.3 30/391 7.672634271099745\%
2.0 1.0 0.5 28/409 6.845965770171149\%
2.0 1.15 0.03 54/420 12.857142857142856\%
2.0 1.15 0.1 27/402 6.7164179104477615\%
2.0 1.15 0.2 22/398 5.527638190954774\%
2.0 1.15 0.3 31/391 7.928388746803069\%
2.0 1.15 0.5 30/387 7.751937984496124\%
2.0 1.3 0.03 53/406 13.054187192118228\%
2.0 1.3 0.1 37/431 8.584686774941995\%
2.0 1.3 0.2 24/396 6.0606060606060606\%
2.0 1.3 0.3 26/427 6.0889929742388755\%
2.0 1.3 0.5 31/401 7.73067331670823\%
2.0 1.5 0.03 51/403 12.655086848635236\%
2.0 1.5 0.1 27/380 7.105263157894736\%
2.0 1.5 0.2 23/407 5.651105651105651\%
2.0 1.5 0.3 20/374 5.347593582887701\%
2.0 1.5 0.5 27/388 6.958762886597938\%
3.0 0.85 0.03 9/388 2.3195876288659796\%
3.0 0.85 0.1 2/411 0.48661800486618007\%
3.0 0.85 0.3 1/402 0.24875621890547264\%
3.0 1.0 0.03 8/410 1.951219512195122\%
3.0 1.0 0.1 2/397 0.5037783375314862\%
3.0 1.0 0.3 2/391 0.5115089514066496\%
3.0 1.0 0.5 1/409 0.24449877750611246\%
3.0 1.15 0.03 7/420 1.6666666666666667\%
3.0 1.15 0.1 1/402 0.24875621890547264\%
3.0 1.3 0.03 4/406 0.9852216748768473\%
3.0 1.3 0.1 1/431 0.23201856148491878\%
3.0 1.3 0.5 1/401 0.24937655860349126\%
3.0 1.5 0.03 2/403 0.49627791563275436\%
3.0 1.5 0.2 2/407 0.4914004914004914\%
3.0 1.5 0.3 1/374 0.267379679144385\%
4.0 1.3 0.1 1/431 0.23201856148491878\%


=========================================================
GNUPLOT: splot "./bal/data/hdist_stats_0.csv" using 1:2:3 with lines lt rgb "blue"
sample file: 
#sigma  lambda  success
1       0.001   0.4
1       0.003   8.75
1       0.01    30.8
1       0.03    47.03389830508475
1       0.1     54.112554112554115
1       0.3     56.97211155378486
1       1       57.3394495412844
1.3     0.001   0.8264462809917356
1.3     0.003   14.354066985645932
1.3     0.01    32.5


 err sigma lambda
0.0 1.5 0.3 66/131 50.38167938931297%
0.0 1.5 0.5 65/110 59.09090909090909%
0.0 1.5 0.7 72/122 59.01639344262295%
0.0 1.5 0.9 61/100 61.0%
0.0 1.5 1.1 75/124 60.483870967741936%
0.0 1.5 1.3 57/113 50.442477876106196%
0.0 1.5 1.5 65/114 57.01754385964912%
0.0 1.7 0.3 63/128 49.21875%
0.0 1.7 0.5 82/129 63.565891472868216%
0.0 1.7 0.7 63/122 51.63934426229508%
0.0 1.7 0.9 87/144 60.416666666666664%
0.0 1.7 1.1 60/118 50.847457627118644%
0.0 1.7 1.3 67/116 57.758620689655174%
0.0 1.7 1.5 62/118 52.54237288135594%
0.0 1.9 0.3 60/99 60.60606060606061%
0.0 1.9 0.5 69/116 59.48275862068966%
0.0 1.9 0.7 65/115 56.52173913043478%
0.0 1.9 0.9 68/112 60.71428571428571%
0.0 1.9 1.1 75/131 57.25190839694656%
0.0 1.9 1.3 79/136 58.08823529411765%
0.0 1.9 1.5 78/128 60.9375%
0.0 2.1 0.3 57/109 52.293577981651374%
0.0 2.1 0.5 60/98 61.224489795918366%
0.0 2.1 0.7 79/123 64.22764227642277%
0.0 2.1 0.9 65/110 59.09090909090909%
0.0 2.1 1.1 95/146 65.06849315068493%
0.0 2.1 1.3 71/109 65.13761467889908%
0.0 2.1 1.5 81/128 63.28125%
0.0 2.3 0.3 69/120 57.49999999999999%
0.0 2.3 0.5 73/133 54.88721804511278%
0.0 2.3 0.7 67/95 70.52631578947368%
0.0 2.3 0.9 68/117 58.119658119658126%
0.0 2.3 1.1 71/138 51.449275362318836%
0.0 2.3 1.3 80/136 58.82352941176471%
0.0 2.3 1.5 77/120 64.16666666666667%
0.0 2.5 0.3 53/110 48.18181818181818%
0.0 2.5 0.5 75/121 61.98347107438017%
0.0 2.5 0.7 63/109 57.798165137614674%
0.0 2.5 0.9 50/94 53.191489361702125%
0.0 2.5 1.1 62/103 60.19417475728155%
0.0 2.5 1.3 66/132 50.0%
0.0 2.5 1.5 75/123 60.97560975609756%
1.0 1.5 0.3 55/131 41.98473282442748%
1.0 1.5 0.5 35/110 31.818181818181817%
1.0 1.5 0.7 40/122 32.78688524590164%
1.0 1.5 0.9 28/100 28.000000000000004%
1.0 1.5 1.1 35/124 28.225806451612907%
1.0 1.5 1.3 41/113 36.283185840707965%
1.0 1.5 1.5 42/114 36.84210526315789%
1.0 1.7 0.3 57/128 44.53125%
1.0 1.7 0.5 40/129 31.007751937984494%
1.0 1.7 0.7 43/122 35.24590163934426%
1.0 1.7 0.9 44/144 30.555555555555557%
1.0 1.7 1.1 45/118 38.13559322033898%
1.0 1.7 1.3 38/116 32.758620689655174%
1.0 1.7 1.5 46/118 38.983050847457626%
1.0 1.9 0.3 27/99 27.27272727272727%
1.0 1.9 0.5 36/116 31.03448275862069%
1.0 1.9 0.7 39/115 33.91304347826087%
1.0 1.9 0.9 35/112 31.25%
1.0 1.9 1.1 44/131 33.587786259541986%
1.0 1.9 1.3 38/136 27.941176470588236%
1.0 1.9 1.5 37/128 28.90625%
1.0 2.1 0.3 44/109 40.36697247706422%
1.0 2.1 0.5 28/98 28.57142857142857%
1.0 2.1 0.7 32/123 26.01626016260163%
1.0 2.1 0.9 38/110 34.54545454545455%
1.0 2.1 1.1 42/146 28.767123287671232%
1.0 2.1 1.3 27/109 24.770642201834864%
1.0 2.1 1.5 31/128 24.21875%
1.0 2.3 0.3 38/120 31.666666666666664%
1.0 2.3 0.5 46/133 34.58646616541353%
1.0 2.3 0.7 24/95 25.263157894736842%
1.0 2.3 0.9 41/117 35.04273504273504%
1.0 2.3 1.1 52/138 37.68115942028986%
1.0 2.3 1.3 41/136 30.14705882352941%
1.0 2.3 1.5 33/120 27.500000000000004%
1.0 2.5 0.3 48/110 43.63636363636363%
1.0 2.5 0.5 36/121 29.75206611570248%
1.0 2.5 0.7 33/109 30.275229357798167%
1.0 2.5 0.9 33/94 35.1063829787234%
1.0 2.5 1.1 31/103 30.097087378640776%
1.0 2.5 1.3 54/132 40.909090909090914%
1.0 2.5 1.5 39/123 31.70731707317073%
2.0 1.5 0.3 10/131 7.633587786259542%
2.0 1.5 0.5 10/110 9.090909090909092%
2.0 1.5 0.7 10/122 8.19672131147541%
2.0 1.5 0.9 11/100 11.0%
2.0 1.5 1.1 13/124 10.483870967741936%
2.0 1.5 1.3 15/113 13.274336283185843%
2.0 1.5 1.5 7/114 6.140350877192982%
2.0 1.7 0.3 8/128 6.25%
2.0 1.7 0.5 7/129 5.426356589147287%
2.0 1.7 0.7 16/122 13.114754098360656%
2.0 1.7 0.9 12/144 8.333333333333332%
2.0 1.7 1.1 13/118 11.016949152542372%
2.0 1.7 1.3 11/116 9.482758620689655%
2.0 1.7 1.5 10/118 8.47457627118644%
2.0 1.9 0.3 12/99 12.121212121212121%
2.0 1.9 0.5 11/116 9.482758620689655%
2.0 1.9 0.7 11/115 9.565217391304348%
2.0 1.9 0.9 9/112 8.035714285714286%
2.0 1.9 1.1 11/131 8.396946564885496%
2.0 1.9 1.3 19/136 13.970588235294118%
2.0 1.9 1.5 12/128 9.375%
2.0 2.1 0.3 8/109 7.339449541284404%
2.0 2.1 0.5 10/98 10.204081632653061%
2.0 2.1 0.7 12/123 9.75609756097561%
2.0 2.1 0.9 7/110 6.363636363636363%
2.0 2.1 1.1 9/146 6.164383561643835%
2.0 2.1 1.3 10/109 9.174311926605505%
2.0 2.1 1.5 14/128 10.9375%
2.0 2.3 0.3 13/120 10.833333333333334%
2.0 2.3 0.5 13/133 9.774436090225564%
2.0 2.3 0.7 4/95 4.2105263157894735%
2.0 2.3 0.9 8/117 6.837606837606838%
2.0 2.3 1.1 14/138 10.144927536231885%
2.0 2.3 1.3 15/136 11.029411764705882%
2.0 2.3 1.5 10/120 8.333333333333332%
2.0 2.5 0.3 7/110 6.363636363636363%
2.0 2.5 0.5 8/121 6.6115702479338845%
2.0 2.5 0.7 12/109 11.009174311926607%
2.0 2.5 0.9 10/94 10.638297872340425%
2.0 2.5 1.1 10/103 9.70873786407767%
2.0 2.5 1.3 11/132 8.333333333333332%
2.0 2.5 1.5 7/123 5.691056910569105%
3.0 1.5 1.1 1/124 0.8064516129032258%
3.0 1.7 0.9 1/144 0.6944444444444444%
3.0 1.9 1.1 1/131 0.7633587786259541%
3.0 1.9 1.5 1/128 0.78125%
3.0 2.1 1.3 1/109 0.9174311926605505%
3.0 2.1 1.5 2/128 1.5625%
3.0 2.3 0.5 1/133 0.7518796992481203%
3.0 2.3 1.1 1/138 0.7246376811594203%
3.0 2.5 0.3 1/110 0.9090909090909091%
3.0 2.5 0.5 2/121 1.6528925619834711%
3.0 2.5 0.7 1/109 0.9174311926605505%
3.0 2.5 0.9 1/94 1.0638297872340425%
3.0 2.5 1.3 1/132 0.7575757575757576%
3.0 2.5 1.5 2/123 1.6260162601626018%
4.0 2.5 0.3 1/110 0.9090909090909091%


================= 09-12-2013 =========================
Convergence which depends on average weight change does not work. 
Max diff implemented. Convergence with similar results on average of 900 epochs (TODO measure more exactly)/

======== MORE-DIMENSIONAL STUFF =====================
		MEASURE_IS = true; 
		MEASURE_SAVE_AFTER_EACH_RUN = false; 
		MEASURE_RECORD_EACH = 1000;

		INPUT_FILEPATH = "k3.in"; 
		OUTPUT_FILEPATH = "k3.out"; 
		INIT_HIDDEN_LAYER_SIZE = 3; 

		CONVERGENCE_WEIGHT_EPSILON = 0.0; 
		
		CONVERGENCE_NO_CHANGE_FOR = 10; 
		CONVERGENCE_NO_CHANGE_EPSILON = 0.001;
		INIT_MAX_EPOCHS = 5000;

		INIT_RUNS = 1000; 
		INIT_CANDIDATES_COUNT = 1;

		PRINT_NETWORK_IS = true; 

		TRY_NORMAL_DISTRIBUTION_SIGMA = new double[] {2.3}; 
		TRY_LAMBDA = new double[] {0.7}; 
		TRY_MOMENTUM = new double[] {0.0}; 
		
=========== HIDDEN SIZE=3 =================
err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.0 79.5 795/1000
10.0 2.3 0.7 0.0 0.1 1/1000
12.0 2.3 0.7 0.0 4.7 47/1000
14.0 2.3 0.7 0.0 0.1 1/1000
16.0 2.3 0.7 0.0 3.4000000000000004 34/1000
20.0 2.3 0.7 0.0 0.8 8/1000
24.0 2.3 0.7 0.0 1.7000000000000002 17/1000
28.0 2.3 0.7 0.0 0.5 5/1000
3.0 2.3 0.7 0.0 0.2 2/1000
32.0 2.3 0.7 0.0 0.2 2/1000
4.0 2.3 0.7 0.0 3.2 32/1000
40.0 2.3 0.7 0.0 0.2 2/1000
7.0 2.3 0.7 0.0 0.1 1/1000
8.0 2.3 0.7 0.0 5.3 53/1000
avg(epoch)=956.739
avg(err)=2.549
avg(h_dist)=0.4284663233176611
avg(h_f_b_dist)=0.001433903013638078
avg(m_avg_w)=4.118199611625134
avg(m_sim)=8.968924274269243
avg(first_second)=16.816534206233154
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=1.6954451983027756
avg(momentum)=0.0

=========== HIDDEN SIZE=4 =================

err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.0 94.89999999999999 949/1000
12.0 2.3 0.7 0.0 1.7000000000000002 17/1000
13.0 2.3 0.7 0.0 0.1 1/1000
16.0 2.3 0.7 0.0 0.7000000000000001 7/1000
24.0 2.3 0.7 0.0 0.7000000000000001 7/1000
28.0 2.3 0.7 0.0 0.1 1/1000
4.0 2.3 0.7 0.0 0.3 3/1000
8.0 2.3 0.7 0.0 1.5 15/1000
avg(epoch)=601.497
avg(err)=0.657
avg(h_dist)=0.5135170749928729
avg(h_f_b_dist)=0.003096212365949686
avg(m_avg_w)=3.0549479613555426
avg(m_sim)=6.275420587570562
avg(first_second)=16.24367030549711
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=1.7258244262998257
avg(momentum)=0.0

=========== HIDDEN SIZE=5 =================

err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.0 98.5 985/1000
1.0 2.3 0.7 0.0 0.1 1/1000
10.0 2.3 0.7 0.0 0.1 1/1000
12.0 2.3 0.7 0.0 0.2 2/1000
13.0 2.3 0.7 0.0 0.1 1/1000
16.0 2.3 0.7 0.0 0.1 1/1000
2.0 2.3 0.7 0.0 0.1 1/1000
24.0 2.3 0.7 0.0 0.2 2/1000
4.0 2.3 0.7 0.0 0.3 3/1000
8.0 2.3 0.7 0.0 0.3 3/1000
avg(epoch)=410.62
avg(err)=0.15
avg(h_dist)=0.5745057716392946
avg(h_f_b_dist)=0.004810628841391281
avg(m_avg_w)=2.5324437064360206
avg(m_sim)=4.923161220350142
avg(first_second)=16.075237073653938
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=1.7378024156440908
avg(momentum)=0.0


=========== HIDDEN SIZE=6 =================

err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.0 99.5 995/1000
12.0 2.3 0.7 0.0 0.1 1/1000
16.0 2.3 0.7 0.0 0.2 2/1000
4.0 2.3 0.7 0.0 0.1 1/1000
8.0 2.3 0.7 0.0 0.1 1/1000
avg(epoch)=303.967
avg(err)=0.056
avg(h_dist)=0.6458562318373903
avg(h_f_b_dist)=0.006696401702940912
avg(m_avg_w)=2.1818656076530747
avg(m_sim)=4.079264257960105
avg(first_second)=16.05796072173848
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=1.7416806433833738
avg(momentum)=0.0

=========== HIDDEN SIZE=7 =================

err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.0 99.8 998/1000
10.0 2.3 0.7 0.0 0.1 1/1000
4.0 2.3 0.7 0.0 0.1 1/1000
avg(epoch)=247.087
avg(err)=0.014
avg(h_dist)=0.7042485805082631
avg(h_f_b_dist)=0.009082904298846018
avg(m_avg_w)=1.9839182199103171
avg(m_sim)=3.5810010763993825
avg(first_second)=16.04953024179868
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=1.7434258830153593
avg(momentum)=0.0

=========== HIDDEN SIZE=8 =================

err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.0 99.9 999/1000
4.0 2.3 0.7 0.0 0.1 1/1000
avg(epoch)=207.157
avg(err)=0.0040
avg(h_dist)=0.7565836343904825
avg(h_f_b_dist)=0.01119194262780024
avg(m_avg_w)=1.849205567627445
avg(m_sim)=3.2458319255933024
avg(first_second)=16.041103921687853
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=1.7435353993910458
avg(momentum)=0.0


=========== HIDDEN SIZE=9 =================

err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.0 100.0 1000/1000
avg(epoch)=185.197
avg(err)=0.0
avg(h_dist)=0.8073814583944268
avg(h_f_b_dist)=0.013189858683857341
avg(m_avg_w)=1.7555708577329958
avg(m_sim)=3.0310904279720203
avg(first_second)=16.03765067877136
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=1.743996359435468
avg(momentum)=0.0

=========== HIDDEN SIZE=10 =================

err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.0 100.0 1000/1000
avg(epoch)=164.982
avg(err)=0.0
avg(h_dist)=0.8611878976280017
avg(h_f_b_dist)=0.015753765441862518
avg(m_avg_w)=1.6859185923859807
avg(m_sim)=2.867064838367571
avg(first_second)=16.033555213297653
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=1.7466494363906515
avg(momentum)=0.0

=========== HIDDEN SIZE=11 =================

err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.0 100.0 1000/1000
avg(epoch)=147.927
avg(err)=0.0
avg(h_dist)=0.9045586193984213
avg(h_f_b_dist)=0.018396550045545114
avg(m_avg_w)=1.6349275275251605
avg(m_sim)=2.7458523470512652
avg(first_second)=16.030847684383325
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=1.7479673417663801
avg(momentum)=0.0

=========== HIDDEN SIZE=12 =================

err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.0 100.0 1000/1000
avg(epoch)=138.848
avg(err)=0.0
avg(h_dist)=0.9471818347636051
avg(h_f_b_dist)=0.02031140691623281
avg(m_avg_w)=1.5997988709857414
avg(m_sim)=2.660522928606756
avg(first_second)=16.030766939353732
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=1.7503840335932033
avg(momentum)=0.0

=========== HIDDEN SIZE=13 =================

err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.0 100.0 1000/1000
avg(epoch)=130.816
avg(err)=0.0
avg(h_dist)=0.9873039504065871
avg(h_f_b_dist)=0.022116791057825578
avg(m_avg_w)=1.5660344127061239
avg(m_sim)=2.5939002974820426
avg(first_second)=16.029348701690783
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=1.7527485904242674
avg(momentum)=0.0

=========== HIDDEN SIZE=14 =================

err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.0 100.0 1000/1000
avg(epoch)=119.83
avg(err)=0.0
avg(h_dist)=1.0321880386525344
avg(h_f_b_dist)=0.02511997582657276
avg(m_avg_w)=1.538464705880271
avg(m_sim)=2.5314972594130145
avg(first_second)=16.02742213450508
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=1.7558119843146454
avg(momentum)=0.0

=========== HIDDEN SIZE=15 =================

err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.0 100.0 1000/1000
avg(epoch)=116.484
avg(err)=0.0
avg(h_dist)=1.064706435083704
avg(h_f_b_dist)=0.02674199876019537
avg(m_avg_w)=1.5234933115862725
avg(m_sim)=2.498672916444305
avg(first_second)=16.025714388437642
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=1.7589998704589265
avg(momentum)=0.0

=========== HIDDEN SIZE=16 =================

0.0 2.3 0.7 0.0 100.0 1000/1000
avg(epoch)=108.227
avg(err)=0.0
avg(h_dist)=1.1046212031191365
avg(h_f_b_dist)=0.029585609889805232
avg(m_avg_w)=1.5037866439274676
avg(m_sim)=2.45579992947505
avg(first_second)=16.025085847056886
avg(sigma)=2.2999999999999914
avg(lambda)=0.7000000000000064
avg(o_f_b_dist)=1.7633294687453425
avg(momentum)=0.0


======== MORE-DIMENSIONAL STUFF =====================
		MEASURE_IS = true; 
		MEASURE_SAVE_AFTER_EACH_RUN = false; 
		MEASURE_RECORD_EACH = 1000;

		INPUT_FILEPATH = "k3.in"; 
		OUTPUT_FILEPATH = "k3.out"; 
		INIT_HIDDEN_LAYER_SIZE = 3; 

		CONVERGENCE_WEIGHT_EPSILON = 0.0; 
		
		CONVERGENCE_NO_CHANGE_FOR = 10; 
		CONVERGENCE_NO_CHANGE_EPSILON = 0.001;
		INIT_MAX_EPOCHS = 5000;

		INIT_RUNS = 1000; 
		INIT_CANDIDATES_COUNT = 1;

		PRINT_NETWORK_IS = true; 

		TRY_NORMAL_DISTRIBUTION_SIGMA = new double[] {2.3}; 
		TRY_LAMBDA = new double[] {0.7}; 
		TRY_MOMENTUM = new double[] {0.0}; 

=========== HIDDEN SIZE=8 =================
1401.0 2.3 0.7 0.0 5.0 1/20
1413.0 2.3 0.7 0.0 5.0 1/20
1422.0 2.3 0.7 0.0 5.0 1/20
1435.0 2.3 0.7 0.0 5.0 1/20
1449.0 2.3 0.7 0.0 5.0 1/20
1454.0 2.3 0.7 0.0 5.0 1/20
1479.0 2.3 0.7 0.0 5.0 1/20
1482.0 2.3 0.7 0.0 5.0 1/20
1493.0 2.3 0.7 0.0 5.0 1/20
1503.0 2.3 0.7 0.0 5.0 1/20
1505.0 2.3 0.7 0.0 5.0 1/20
1510.0 2.3 0.7 0.0 5.0 1/20
1514.0 2.3 0.7 0.0 5.0 1/20
1517.0 2.3 0.7 0.0 5.0 1/20
1526.0 2.3 0.7 0.0 5.0 1/20
1530.0 2.3 0.7 0.0 5.0 1/20
1542.0 2.3 0.7 0.0 5.0 1/20
1568.0 2.3 0.7 0.0 5.0 1/20
1587.0 2.3 0.7 0.0 5.0 1/20
avg(epoch)=1000.0
avg(err)=1486.35
avg(sigma)=2.2999999999999994
avg(lambda)=0.6999999999999997
avg(momentum)=0.0
avg(h_dist)=1.482050343522324
avg(h_f_b_dist)=1.1821481107119525
avg(m_avg_w)=1.5473514737246634
avg(m_sim)=2.0984952848986156
avg(first_second)=113.64329536840708
avg(o_f_b_dist)=2.4694862735224534

=========== HIDDEN SIZE=10 =================
1396.0 2.3 0.7 0.0 5.0 1/20
1427.0 2.3 0.7 0.0 5.0 1/20
1443.0 2.3 0.7 0.0 5.0 1/20
1458.0 2.3 0.7 0.0 5.0 1/20
1484.0 2.3 0.7 0.0 5.0 1/20
1515.0 2.3 0.7 0.0 5.0 1/20
1532.0 2.3 0.7 0.0 5.0 1/20
1540.0 2.3 0.7 0.0 5.0 1/20
1560.0 2.3 0.7 0.0 5.0 1/20
1574.0 2.3 0.7 0.0 5.0 1/20
1577.0 2.3 0.7 0.0 10.0 2/20
1582.0 2.3 0.7 0.0 5.0 1/20
1586.0 2.3 0.7 0.0 5.0 1/20
1587.0 2.3 0.7 0.0 5.0 1/20
1588.0 2.3 0.7 0.0 5.0 1/20
1592.0 2.3 0.7 0.0 5.0 1/20
1604.0 2.3 0.7 0.0 5.0 1/20
1605.0 2.3 0.7 0.0 5.0 1/20
1626.0 2.3 0.7 0.0 5.0 1/20
avg(epoch)=1000.0
avg(err)=1542.65
avg(sigma)=2.2999999999999994
avg(lambda)=0.6999999999999997
avg(momentum)=0.0
avg(h_dist)=1.653701960729024
avg(h_f_b_dist)=1.3210189919432302
avg(m_avg_w)=1.551604141181616
avg(m_sim)=2.1575913747369815
avg(first_second)=110.68134873210727
avg(o_f_b_dist)=2.7006947140564233

=========== HIDDEN SIZE=12 =================
1484.0 2.3 0.7 0.0 5.0 1/20
1491.0 2.3 0.7 0.0 5.0 1/20
1504.0 2.3 0.7 0.0 5.0 1/20
1512.0 2.3 0.7 0.0 5.0 1/20
1539.0 2.3 0.7 0.0 5.0 1/20
1544.0 2.3 0.7 0.0 5.0 1/20
1559.0 2.3 0.7 0.0 5.0 1/20
1573.0 2.3 0.7 0.0 5.0 1/20
1581.0 2.3 0.7 0.0 5.0 1/20
1593.0 2.3 0.7 0.0 5.0 1/20
1595.0 2.3 0.7 0.0 5.0 1/20
1603.0 2.3 0.7 0.0 5.0 1/20
1612.0 2.3 0.7 0.0 5.0 1/20
1617.0 2.3 0.7 0.0 5.0 1/20
1649.0 2.3 0.7 0.0 5.0 1/20
1655.0 2.3 0.7 0.0 5.0 1/20
1671.0 2.3 0.7 0.0 5.0 1/20
1691.0 2.3 0.7 0.0 5.0 1/20
1695.0 2.3 0.7 0.0 5.0 1/20
avg(epoch)=1000.0
avg(err)=1582.25
avg(sigma)=2.2999999999999994
avg(lambda)=0.6999999999999997
avg(momentum)=0.0
avg(h_dist)=1.8572751545126276
avg(h_f_b_dist)=1.4793388997673078
avg(m_avg_w)=1.5600097743922352
avg(m_sim)=2.191738234150749
avg(first_second)=108.10211594774948
avg(o_f_b_dist)=2.94788637922129

=========== HIDDEN SIZE=14 =================
err sigma lambda momentum success sample_ratio
1463.0 2.3 0.7 0.0 5.0 1/20
1472.0 2.3 0.7 0.0 5.0 1/20
1508.0 2.3 0.7 0.0 5.0 1/20
1548.0 2.3 0.7 0.0 5.0 1/20
1567.0 2.3 0.7 0.0 10.0 2/20
1581.0 2.3 0.7 0.0 5.0 1/20
1583.0 2.3 0.7 0.0 5.0 1/20
1598.0 2.3 0.7 0.0 5.0 1/20
1614.0 2.3 0.7 0.0 10.0 2/20
1618.0 2.3 0.7 0.0 5.0 1/20
1622.0 2.3 0.7 0.0 5.0 1/20
1623.0 2.3 0.7 0.0 5.0 1/20
1628.0 2.3 0.7 0.0 5.0 1/20
1631.0 2.3 0.7 0.0 5.0 1/20
1694.0 2.3 0.7 0.0 5.0 1/20
1719.0 2.3 0.7 0.0 5.0 1/20
1753.0 2.3 0.7 0.0 5.0 1/20
1766.0 2.3 0.7 0.0 5.0 1/20

=========== HIDDEN SIZE=16 =================
err sigma lambda momentum success sample_ratio
1548.0 2.3 0.7 0.0 5.0 1/20
1571.0 2.3 0.7 0.0 5.0 1/20
1582.0 2.3 0.7 0.0 5.0 1/20
1586.0 2.3 0.7 0.0 5.0 1/20
1588.0 2.3 0.7 0.0 5.0 1/20
1590.0 2.3 0.7 0.0 5.0 1/20
1607.0 2.3 0.7 0.0 5.0 1/20
1613.0 2.3 0.7 0.0 15.0 3/20
1614.0 2.3 0.7 0.0 5.0 1/20
1617.0 2.3 0.7 0.0 5.0 1/20
1626.0 2.3 0.7 0.0 5.0 1/20
1631.0 2.3 0.7 0.0 5.0 1/20
1663.0 2.3 0.7 0.0 5.0 1/20
1666.0 2.3 0.7 0.0 5.0 1/20
1670.0 2.3 0.7 0.0 5.0 1/20
1672.0 2.3 0.7 0.0 5.0 1/20
1691.0 2.3 0.7 0.0 5.0 1/20
1698.0 2.3 0.7 0.0 5.0 1/20
avg(epoch)=1000.0
avg(err)=1622.95
avg(sigma)=2.2999999999999994
avg(lambda)=0.6999999999999997
avg(momentum)=0.0
avg(h_dist)=2.1245678066050195
avg(h_f_b_dist)=1.7271721585694397
avg(m_avg_w)=1.636067005755703
avg(m_sim)=2.4494895296666996
avg(first_second)=104.03792905577318
avg(o_f_b_dist)=3.4224612192761077



=========== HIDDEN SIZE=19 =================

=========== HIDDEN SIZE=22 =================

=========== HIDDEN SIZE=25 =================

=========== HIDDEN SIZE=29 =================

=========== HIDDEN SIZE=33 =================

=========== HIDDEN SIZE=38 =================

=========== HIDDEN SIZE=43 =================

=========== HIDDEN SIZE= =================

=========== HIDDEN SIZE= =================

=========== HIDDEN SIZE= =================

=========== HIDDEN SIZE= =================

=========== HIDDEN SIZE= =================

1386588826_4.log:avg(err)=0.657
1386588902_5.log:avg(err)=0.15
1386588960_6.log:avg(err)=0.056
1386589007_7.log:avg(err)=0.014
1386589049_8.log:avg(err)=0.0040
1386589086_9.log:avg(err)=0.0
1386589123_10.log:avg(err)=0.0
1386589158_11.log:avg(err)=0.0
1386589191_12.log:avg(err)=0.0
1386589225_13.log:avg(err)=0.0
1386589258_14.log:avg(err)=0.0
1386589290_15.log:avg(err)=0.0
1386589323_16.log:avg(err)=0.0

k12_1386591048_8.log:avg(err)=1486.35
k12_1386591204_10.log:avg(err)=1542.65
k12_1386591375_12.log:avg(err)=1582.25
k12_1386591565_14.log:avg(err)=1608.45
k12_1386591774_16.log:avg(err)=1622.95
k12_1386592002_19.log:avg(err)=1634.15
k12_1386592260_22.log:avg(err)=1583.85
k12_1386592546_25.log:avg(err)=1585.2
k12_1386592863_29.log:avg(err)=1471.15
k12_1386593289_33.log:avg(err)=1435.15
k12_1386593762_38.log:avg(err)=1339.3
k12_1386594301_43.log:avg(err)=1270.1
k12_1386594959_49.log:avg(err)=1149.35
k12_1386595621_56.log:avg(err)=1036.15
k12_1386596402_64.log:avg(err)=913.6
k12_1386597286_73.log:avg(err)=811.75
k12_1386598260_83.log:avg(err)=674.15
k12_1386599314_94.log:avg(err)=587.25
k12_1386600536_106.log:avg(err)=463.45
k12_1386602166_120.log:avg(err)=375.9
k12_1386603756_136.log:avg(err)=306.3





========================== 10-12-2013 =========================
Printing 4-2-4 hidden representations 
Generating plots bad/good

good: look like a lichobeznik or symmetric
138671883_.png - 3 on line
138671920_.png - 4 border close
138671944_.png - 3 corner close 
138671948_.png - 2 very close
138671986_.png - 4 left
138672005_.png - 4 left border
138672026_.png - 4 left top corner
138672076_.png - 4 right bottom corner
138672119_.png - 4 right top corner

bad: looks static or usually one inside a triangle (non-convex 4gon) 
138671976_.png - almost no diff with good
138671983_.png - why not
138671990_.png - 2 collision
138672053_.png - big jump line
138672070_.png - 2 zero 
138672013_.png - non-convergent 
138672133_.png - looks good 
138672146_.png - 1 total static zero
138672185_.png - 4 static looks good 
138672212_.png - 4 line
138672288_.png - 4 non-convergent
138672302_.png - 4 close 
138672376_.png - 4 top 
138672401_.png - 4 left top
13867_.png - 
13867_.png - 
13867_.png - 

TODO pouzivam error pri uceni? 0.2, 0.8  
TODO ako vyzeraju blizke body (moz
TODO konverguje pri zlych? 

============================== 16-12-2013 ========================
Saving networks before and after runs (bad / good).
Rerunning same configurations. 

All bad: 
err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.0 19.296918767507005 6889/35700
1.0 2.3 0.7 0.0 68.05602240896359 24296/35700
2.0 2.3 0.7 0.0 12.644257703081232 4514/35700
3.0 2.3 0.7 0.0 0.0028011204481792717 1/35700

All good: 
err sigma lambda momentum success sample_ratio
0.0 2.3 0.7 0.0 99.98911353032659 64293/64300
1.0 2.3 0.7 0.0 0.01088646967340591 7/64300
//TODO overit ci zrobilo dobre good / bad
//TODO numericky potvrdit ci vnutri trojuholnika 
//TODO batch mode 
//TODO ==> ako rozlisit uspesne a neuspesne od inicializacie (v batch mode)
//TODO binarny klasifikator na good/bad vah 

//TODO zovseobecnenie siete

============================== 08-01-2014 ========================
Checking if some point inside others. 
Preliminary result: 
awk 'BEGIN{FS=" "}{print $2,$12}' auto4_1389182421458_2_post.dat | sort | uniq -c | sort -n
      1 2.0 0.0
      1 err in_triangle
      3 1.0 0.0
      3 2.0 1.0
     13 1.0 1.0
     30 0.0 0.0

awk 'BEGIN{FS=" "}{print $2,$12}' auto4_1389182421458_2_pre.dat | sort | uniq -c | sort -n
      1 err in_triangle
      4 0.0 1.0
      4 1.0 1.0
      4 2.0 0.0
     12 1.0 0.0
     26 0.0 0.0
     
awk 'BEGIN{FS=" "}{print $2,$12}' auto4_1389194211285_2_post.dat | sort | uniq -c | sort -n
      1 err in_triangle
     15 2.0 0.0
     69 2.0 1.0
    105 1.0 0.0
    244 1.0 1.0
    567 0.0 0.0
    
Chosing beginnings with convex hidden representations:
err sigma lambda success sample_ratio
0.0 2.3 0.7 68.30000000000001 683/1000
1.0 2.3 0.7 24.5 245/1000
2.0 2.3 0.7 7.199999999999999 72/1000

Start: 
      1 err in_triangle
     72 2.0 0.0
    245 1.0 0.0
    683 0.0 0.0

End: 
      1 err in_triangle
      7 2.0 0.0
     47 1.0 0.0
     65 2.0 1.0
    198 1.0 1.0
    683 0.0 0.0

Hmm, errors count match. 

awk 'BEGIN{FS=" "}{print $2,$12}' auto4_1389196250599_2_pre.dat | sort | uniq -c | sort -n
      1 err in_triangle
     69 2.0 0.0
    256 1.0 0.0
    675 0.0 0.0
petrzlen@Petobook3:~/diplomovka/bal/data$ awk 'BEGIN{FS=" "}{print $2,$12}' auto4_1389196250599_2_post.dat | sort | uniq -c | sort -n
      1 err in_triangle
     12 2.0 0.0
     43 1.0 0.0
     57 2.0 1.0
    213 1.0 1.0
    675 0.0 0.0

Hmm, again. 
So all the training is for nothing, even makes the things worse. Maybe a bug. 

awk 'BEGIN{FS=" "}{print $2}' auto4_1389196800513_2_pre.dat | sort | uniq -c | sort -n
      1 err
      8 3.0
    105 2.0
    289 1.0
    598 0.0
petrzlen@Petobook3:~/diplomovka/bal/data$ awk 'BEGIN{FS=" "}{print $2}' auto4_1389196800513_2_post.dat | sort | uniq -c | sort -n
      1 err
      8 3.0
    105 2.0
    289 1.0
    598 0.0
    
Without preselecting. 
SHIT.     

Some older: 
awk 'BEGIN{FS=" "}{print $2}' auto4_1387317905615_2_post.dat | sort | uniq -c | sort -n
      1 err
      4 3.0
    103 2.0
    296 1.0
    597 0.0
petrzlen@Petobook3:~/diplomovka/bal/data$ awk 'BEGIN{FS=" "}{print $2}' auto4_1387317905615_2_pre.dat | sort | uniq -c | sort -n
      1 err
      4 3.0
    103 2.0
    296 1.0
    597 0.0

FUCK THIS SHIT.  ]
Oh, ok. We are for some reason setting this only after run. 
Sure?
awk 'BEGIN{FS=" "}{print $2}' auto4_1389197605824_2_pre.dat | sort | uniq -c | sort -n
      1 err
      2 2.0
      3 3.0
     11 13.0
     33 12.0
     37 4.0
     44 11.0
     54 5.0
    126 10.0
    152 6.0
    158 9.0
    164 7.0
    216 8.0
petrzlen@Petobook3:~/diplomovka/bal/data$ awk 'BEGIN{FS=" "}{print $2}' auto4_1389197605824_2_post.dat | sort | uniq -c | sort -n
      1 err
     63 2.0
    249 1.0
    688 0.0


=================================================================================
========================== 4th February       ===================================
=================================================================================
Trying out different learning algorithms. 

There is something weird with the epochs, I should plot it ASAP. It looks like the more epochs the better results what is not in correspondence with Igors observations. 

tail -n +2 data/auto4_1391556235821_2_measure.dat | awk '{a[$2] += $3}END{for(epoch in a) print epoch,a[epoch]}' | sort -n | tail -n +100 > epochs_100000.dat




=================================================================================
========================== 5th February       ===================================
=================================================================================
Implementing dropout 
