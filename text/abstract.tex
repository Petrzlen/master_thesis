%Abstract is very important part of the thesis. It will be most read by people and should be written with a great care. The abstract should mention:
% 1) About the problem you want to solve
% 2) About your solution – how you solve the problem
% 3) Highlights about how good is your solution (e.g. achieves 70\% better performance) referring to the results you obtained in your experiments (e.g. achieves 70\% better performance).
% 4) Possible impacts of your work into the field (e.g. “The proposed solution can be used to offload the CPU by executing data parallel computation intensive code on GPUs and thus obtaining additional Speedup for no cost”).

\section*{Abstract}

% 1) About the problem you want to solve
We analysed artificial neural networks without error backpropagation (BP), based on the Generalized recirculation algorithm (GeneRec) by~\citet{o1996bio} and the Bidirectional Activation-based Learning algorithm (BAL) by~\citet{farkas2013bal}. The main idea of both algorithms is to update weights based on the difference between forward and backward activations. This is argued to be a biologically plausible update rule. But both have problems learning even low dimensional mappings which could be easily learned by BP. In our work we aim to fill this gap. 

% 2) About your solution – how you solve the problem
We have deeply analysed several modifications of BAL. After that we eventually came up with a Two learning rates (TLR) version of BAL. TLR uses different learning rates for different weight matrices. Our simulations prove increase in success rate while having a smooth relation between success and both learning rates. We observed that for the networks with highest success rate the two learning rates could differ even by $10^6$. We further apply the idea of TLR to GeneRec and BIA and experiment with momentum, weight initialization, hidden activations and dynamic learning rate. 

% 4) Possible impacts of your work into the field (e.g. “The proposed solution can be used to of
We believe that using the idea of TLR could lead to performance increase in other artificial neural network models and even several layered networks. The generalized idea of different parameters for different weight matrices could be used also for momentum or weight initialization. We outline further experiments which should be done. 

\begin{flushleft}
  \textbf{Keywords:} supervised learning, artificial neural network, heteroassociative mapping, dynamic learning rate, activation based learning, handwritten digits. 
\end{flushleft}

%keywords={ Internet; TCP streams; Tor network;}

