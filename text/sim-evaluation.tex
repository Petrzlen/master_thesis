
\subsection{Evaluation methods} 
\label{sec:sim-evaluation} 

Following~\citet{farkas2013bal}, we measured two key properties of a BAL-like network. The more important being \emph{success rate} and second being \emph{convergence time}. 

\paragraph{Success rate.}  
Before comparing \emph{given} outputs on both visible layers the activations $Y^F$ and $X^B$ are classified by a treshold: 
\begin{equation} 
  g_k =
  \left\{
	  \begin{array}{ll}
		  1 & \mbox{if } x_k^B > 0.5 \\
		  0 & \mbox{otherwise}
	  \end{array}
  \right.  
\end{equation} 
By having a set of given vectors $G^I$ and the target vectors $T^I$ for inputs $I \in \mathbb{S}$, we distinguish two main success measures: 
\begin{itemize}
  \item \emph{Bit success (bitSucc)} defined as $bitSucc = avg_{I \in \mathbb{S}} \sum_i |T_i^I - G^I_i|$ and 
  \item \emph{Pattern success (patSucc)} defined as 
    \begin{equation}
      patSucc = avg_{I \in \mathbb{S}} \left\{
	      \begin{array}{ll}
		      1 & \mbox{if } T^I = G^I \\
		      0 & \mbox{otherwise}
	      \end{array}
      \right.
    \end{equation} 
\end{itemize} 

\paragraph{Convergence time.} The are several possibilities when to stop the learning algorithm. \emph{Converge time} is the number of epochs before the stop. Usually, training could be stopped for two reasons. The network could either reach the stopping criterion~(\ref{sec:our-stopping-criteria}) or the maximum epoch is reached. We see that lower convergence time makes the model more convenient for training.  
