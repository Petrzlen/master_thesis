
\subsection{Evaluation methods} 
\label{sec:sim-evaluation} 

Following~\citet{farkas2013bal}, we measured two key properties of a BAL-like network. The more important being \emph{success rate} and second being \emph{convergence time}. 

\paragraph{Success rate.}  
Before comparing \emph{given} outputs on both visible layers the activations $Y^F$ and $X^B$ are classified by a treshold: 
\begin{equation} 
  g_k =
  \left\{
	  \begin{array}{ll}
		  1 & \mbox{if } x_k^B > 0.5 \\
		  0 & \mbox{otherwise}
	  \end{array}
  \right.  
\end{equation} 
By having a set of given vectors $G^I$ and the target vectors $T^I$ for inputs $I \in \mathbb{S}$, we distinguish two main success measures: 

%TODO remove itemize
\begin{itemize}
  \item \emph{Bit success (bitSucc)} defined as $bitSucc = avg_{I \in \mathbb{S}} \sum_i |T_i^I - G^I_i|$ and 
  \item \emph{Pattern success (patSucc)} defined as 
    \begin{equation}
      patSucc = avg_{I \in \mathbb{S}} \left\{
	      \begin{array}{ll}
		      1 & \mbox{if } T^I = G^I \\
		      0 & \mbox{otherwise}
	      \end{array}
      \right.
    \end{equation} 
\end{itemize} 

\paragraph{Convergence time.} The are several possibilities when to stop the learning algorithm. \emph{Converge time} is the number of epochs before the stop. Usually, training could be stopped for two reasons. The network could either reach the \emph{stopping criteria} or the maximum epoch is reached. Given by nature of used datasets we trained the neural networks while $patSucc^F \neq 1$. In case of the \emph{digits}~(\ref{sec:datasets-digits}) dataset we decided to stop the training if $patSucc^F$ was not increased for 3 epochs. Note that we are motivated to decrease the convergence time as it makes the training process faster.  
