%==========================================================
\subsubsection{Momentum}
\label{sec:our-momentum}

\emph{Momentum} introduced by \citet{jacobs1988increased} as an special case of dynamic learning rate \ref{sec:our-dynamic-lambda}. Momentum is an extension to any learning rule for any artificial neural network by adding a \emph{momentum} term: 
\begin{equation} 
  \Delta w_{ij}(t) = \mbox{learning rule} + \mu \Delta w_{ij}(t-1), \nonumber
\end{equation} 
where $\mu$ is a real valued parameter. 

It is argued that momentum could overcome settling in local minima by using the second derivate  \citep{phansalkar1994analysis}. Its also \enquote{believed that momentum could render the learning procedure more stable and accelarate convergence} but \enquote{momentum setting is as practice shows problem dependend} \citep{riedmiller1993direct}. As learning rate has adaptive versions \ref{sec:our-dynamic-lambda} also momentum has an adaptive version \citep{miniani1990acceleration}. 
