%==========================================================
\subsubsection{Momentum}
\label{sec:our-momentum}

\emph{Momentum} was introduced by~\citet{jacobs1988increased} as an special case of dynamic learning rate~(\ref{sec:our-dynamic-lambda}). It is an extension to any learning rule for any artificial neural network by adding a \emph{momentum} term: 
\begin{equation} 
  \Delta w_{ij}(t) = \mbox{learning rule} + \mu \Delta w_{ij}(t-1), \nonumber
\end{equation} 
where $\mu$ is a real--valued parameter. 

It is argued that momentum could overcome settling in local minima by leveraging the second derivate ~\citep{phansalkar1994analysis}. It is also \enquote{believed that momentum could render the learning procedure more stable and accelarate convergence} but \enquote{momentum setting is as practice shows problem dependend}~\citep{riedmiller1993direct}. As learning rate has an adaptive dynamic version~(\ref{sec:our-dynamic-lambda}), momentum also has an adaptive dynamic version~\citep{miniani1990acceleration}. 
