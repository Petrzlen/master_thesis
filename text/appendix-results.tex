%At the end of your thesis you can attach resources such as source code (or something like ASCII code table) that would improve the completeness of your thesis.

\section*{Appendix B - Other Results}
\appendix
\addcontentsline{toc}{section}{Appendix B - Other Results}
\markboth{Appendix B}{}
\label{sec:appendix-results}

This section contains results observed through our work which usually need to be extended. It could be treated as an idea list backuped with some simulations. 

\subsection*{Recirculation BAL}
\ref{sec:our-bal-recirc}

\subsection*{BAL}
\label{sec:results-auto4-bal-matrix-sim} 
Matrix similarity tends to zero.  

\subsubsection*{Measure - Weight decay}
\label{sec:weight-decay} 
TODO rewrite and reference in future work

A commonly-used bias or regularizing function is weight decay (e.g., Hinton, 1989a; Weigend et al., 1991).
We implemented two commonly-used forms of weight decay in the Bp and GeneRec networks, simple
weight decay and weight-elimination weight decay (Weigend et al., 1991). In simple weight decay, a con-
stant fraction of the weight value is subtracted at each weight update, and weight-elimination is similar
except that the rate of decay is a more complex function of the weight such that larger weights suffer rela-
tively less decay than smaller ones (supporting a prior assumption of a bimodal distribution of weight values
— one population of larger weights that are actually useful and another of near-zero weights that are not
useful; see Weigend et al., 1991 for details).
The results with these forms of weight decay for the 100 hidden unit network are shown in Figure 5.
Although a small amount (.002; smaller amounts had progressively smaller effects) of simple weight de-
cay appears to reliably improve generalization performance in both Bp and GeneRec, the difference is not
substantial. The weight-elimination version of weight decay always appears to impair, rather than improve,
performance. Although the specific forms of weight decay explored here were not overly successful in
this task, it is possible that other forms might perform better. Nevertheless, most forms of weight decay
\citet{o2001generalization} 


\subsubsection*{Convergence} 
\label{sec:convergence} 

TODO make it shorter, a lot

24-02-2014
TODO: Measure in how many cases the learning ends with convergence (no weight change) and divergence (oscialtion of seveal states). 

 Mathematical background on convergence and learning rule: 

(Hopfield networks, Wikipedia) The requirement that weights be symmetric is typically used, as it will guarantee that the energy function decreases monotonically while following the activation rules, and the network may exhibit some periodic or chaotic behaviour if non-symmetric weights are used. However, Hopfield found that this chaotic behavior is confined to relatively small parts of the phase space, and does not impair the network's ability to act as a content-addressable associative memory system.

(O'Reilly 1996) The GeneRec Approximation to AP BP
The analysis presented earlier in the paper shows that GeneRec should compute the same error derivatives as the Almeida-Pineda version of error backpropagation in a recurrent network if the following conditions hold:
  The difference of the plus and minus phase activation terms in GeneRec, which are updated in separate iterative activation settling phases, can be used to compute a unit’s error term instead of the iterative update of the difference itself, which is what Almeida-Pineda uses.
  The reciprocal weights are symmetric. This enables the activation signals from the output to the hidden units (via the recurrent weights) to reflect the contribution that the hidden units made to the output error (via the forward-going weights).
  The difference of activations in the plus and minus phases is a reasonable approximation to the difference of net inputs times the derivative of the sigmoidal activation function. Note that this only affects the overall magnitude of the weight derivatives, not their direction.
  
(PINnc89.pdf) 


$$r_x\frac{dx_i}{dt} = -x_i + \sum_j w_{ij} f(x_j) + I_i$$
TODO: Frome where comes this equation? 

There are several ways to guarantee convergence. One way is to
impose structure on the connectivity of the network, such as requiring
the weight matrix to be lower triangular or symmetric. Symmetry, al-
though mathematically elegant, is quite stringent because it constrains
microscopic connectivity by requiring pairs of neurons to be symmetri-
cally connected. A less stringent constraint is to require that the Jacobian
matrix be diagonally dominant. For equation (2.11, the Jacobian matrix
has the form: 

$$L_{ij} = \delta_{ij} - w_{ij}f'(x_j)$$
Guez et al. (1988) 

(PINnc89.pdf) then the gradient descent dynamics will not change the stability of the network. The need
for this assumption can be eliminated by choosing a dynamical system
which admits only stable behavior, even under learning, as was done by
Barhen et al. (1989). TODO look up Barhen et al. (1989) article 

TODO - general theorem concerning staiblity of networks with symmetric weights Cohen and Grossberg 1983 

GLOBAL ASYMPTOTIC STABILITY - the network will settle for any input 
FIXED POINT =similar= MEMORY

(pineda1987) Oscilation (on recurrent BP) could occur when substantial SELF-EXCITATION 


