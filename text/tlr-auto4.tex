%Experimental Results and Analysis – in this section you should show the quantitative results – charts and tables. Analyze the results by explaining and highlighting what is important on them in terms of your goals and what is bad. You should explain the strange results too.

%V ďalšej časti prezentujte vlastný prínos a vlastné výsledky porovnajte s výsledkami iných. Charakterizujte použité metódy.
%Vyhýbajte sa používaniu žargónu.
%Používajte starú múdrosť: 1 obrázok je viac než 1000 slov.

\subsection{4-2-4 Encoder} 
\label{sec:results-auto4} 

%==================================================================
\paragraph{Introduction.} 
In this section we inspect TLR\ref{sec:our-tlr} performance for broad range of parameters $\lambda_h$ and $\lambda_v$. The network architecture is 4-2-4 what allows us to run plethora of simulations. There were two kinds of simulations. One being \emph{two dimensional maps (TDM)} where on the $x$ and $y$ axis $\lambda_v$ and $\lambda_h$ were plotted color was used for the $z$ value. Second being \emph{timelines} where a best configuration found by TDM was inspected. For TDM we run about 200-500 networks for each ($\lambda_v$, $\lambda_h$) and for timelines about 5000-10000 runs. After the simulation ended we took the average for each parameter setting. The networks were trained while $patSucc^F \neq 0$ or $epoch < MAX\_EPOCH$ where $MAX\_EPOCH$ was set to 100,000 in TDM and 1,000,000 in timelines. Note that most of the plots are in \emph{logarithmic} scale. 

%============================================================
\subsubsection{Comparison} 
\label{sec:tlr-auto4-cmp} 

In the following table\ref{tab:results-cmp-auto4} we can see the comparison of the most important models which we analysed on the \emph{4-2-4 encoder} task. We achieved an improvement of BAL $patSucc^F$ from $62.7\%$ to $93.1\%$ by using two different learning rates \ref{sec:our-tlr}. This result was improved further to $99.86\%$ by preselecting networks based on initial weights \ref{sec:sim-exp-candidates}. This proved that hidden distance and representation convexity are important attributes of BAL \ref{sec:results-candidates}. 

On the other hand, many of the analysed models achieved poorly. Notably we tried modified GeneRec learning rules \ref{sec:models-generec-modifications} on BAL, calling this model \emph{BAL GeneRec Learning Rules (BAL GLR)}. This lead to no results. Also \emph{BAL-recirc} \ref{sec:our-bal-recirc} being the combination of BAL and GeneRec achieved worse than BAL. We experimented with \emph{momentum} in section~\ref{sec:results-momentum}, symmetric version of BAL in section~\ref{sec:our-bal-sym} and other but with no significant improvement. 

\begin{table}[H] 
  \centering
    \begin{tabular}{|l|l|l|l|l|l|}
    \hline
    Algorithm (section)&$\lambda_h$&$\lambda_v$&$patSucc^F$ &Epochs\\ %&SEM(success) \\
    \hline
    BP (\ref{sec:models-bp}) &2.4 &2.4 &100&60\\ %&5.1\\
    \hline
    GR (\ref{sec:models-generec}) &0.6 &0.6 &90&418\\ %&28\\
    \hline
    GR Sym (\ref{eq:models-generec-learning-rule-sym}) &1.4 &1.4 &56&88\\ %&2.9\\
    \hline
    GR Mid (\ref{eq:models-generec-learning-rule-mid}) &2.4 &2.4 &92&60\\ %&3.4\\
    \hline
    CHL (\ref{eq:models-chl}) &1.2 &1.2 &56&77\\ %&1.8\\
    \hline
    BAL (\ref{sec:models-bal})&0.9 &0.9 &62.7& 5136.11\\ %&2.0e+08\\
    \hline
    BAL TLR (\ref{sec:our-tlr})&0.0002  & 500&93.12&5845.01\\ %&1.52e+08\\
    \hline
    BAL TLR Can (\ref{sec:sim-exp-candidates})&0.0002&500&99.86&150.417\\ %&5,070,000\\
    \hline
    BAL Recirc (\ref{sec:our-bal-recirc})&5&1.2&43.9&2878.92\\ %&4.31e+07\\
    \hline
    BAL GLR (\ref{sec:models-generec-modifications})& any & 0 & 0 & N/A \\ 
    \hline 
    %TODO Symmetric BAL 
    \end{tabular}
  \caption{Comparing performance of different models on the \emph{4-2-4 encoder} task. Data for BP, GR, GR Sym, Gr Mid and CHL are taken from \citet{o1996bio}.} 
  \label{tab:results-cmp-auto4}
\end{table}

Note that when comparing runtime in table~\ref{tab:results-cmp-auto4} based on \emph{epochs} we must be aware of that GeneRec and BAL-recirc epochs take longer than others. That's because the recirculation step~\ref{sec:models-generec-activation} where usually about 5-15 iterations are necessary for activation to settle~\ref{sec:generec-fluctuation}. Thus the 418 epochs of GeneRec are comparable to the 5845 epochs of TLR in terms of compuration time. 

%============================================================
\subsubsection{Two learning rate} 
\label{sec:tlr-auto4} 
\ref{sec:datasets-auto4} 
%======== (3D) L1 x L2 x epochs =========
%======== (3D) L1 x L2 x patSuccF =========
\begin{figure}[H]
  \centering
  \includegraphics[width=0.48\textwidth]{img/tlr-auto4-success.pdf}   
  \includegraphics[width=0.48\textwidth]{img/tlr-auto4-epoch.pdf}     
  \caption{TLR success and convergence on the \emph{4-2-4 Encoder} task with $\sigma = 2.3$ and $\mu = 0.0$ best being $96.5\$$ with setting $\lambda_h=0.0003$ and $\lambda_v=1000.0$.}
  \label{fig:results-tlr-auto4-performance}
\end{figure}

Note the inconsistency with table~\ref{tab:results-cmp-auto4} where 93.12\% success was stated as best and in figure~\ref{fig:results-tlr-auto4-performance} wher 96.5\%. That's because the law of big numbers 

%======== (2D) best TLR on ALL_SUCC x epoch (std-dev) ==========
\begin{figure}[H]
  \centering
  \includegraphics[width=0.48\textwidth]{img/tlr-auto4-best-perf.pdf}   
  \includegraphics[width=0.48\textwidth]{img/tlr-auto4-best-can.pdf}      
  \caption{TLR success evolution for the \emph{4-2-4 Encoder} task. Left without candidate selection and right with candidates selection. } %TODO backward success
  \label{fig:results-tlr-auto4-epoch} 
\end{figure}


%============================================================
\subsubsection{Hidden activations}
\ref{sec:our-hidden-activation}  

%===== hidden activation timelines with commentaries (for TLR, BAL, GeneRec) 
% 2x success, 2x error (wrong settle, divergence) 

\begin{figure}[H]
  \centering
  \includegraphics[width=0.48\textwidth]{img/hid-bal-bad-init.pdf}  
  \includegraphics[width=0.48\textwidth]{img/hid-bal-bad-convex.pdf}  \\
  \includegraphics[width=0.48\textwidth]{img/hid-bal-bad-step.pdf}  
  \includegraphics[width=0.48\textwidth]{img/hid-bal-bad-stagnation.pdf}  \\
  \includegraphics[width=0.48\textwidth]{img/hid-bal-good-init.pdf}  
  \includegraphics[width=0.48\textwidth]{img/hid-bal-good-convex.pdf}  \\
  \includegraphics[width=0.48\textwidth]{img/hid-bal-good-step.pdf}  
  \includegraphics[width=0.48\textwidth]{img/hid-bal-good-stagnation.pdf}  \\ 
  \caption{\emph{BAL} hidden activations on the \emph{4-2-4 encoder}. Top $2\times2$ are {\bf un}successful networks and bottom $2\times2$ successful ones.}
  \label{fig:results-hidden-activations-bal}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.48\textwidth]{img/hid-tlr-bad-static.pdf}  
  \includegraphics[width=0.48\textwidth]{img/hid-tlr-bad-tiny.pdf}  
  \includegraphics[width=0.48\textwidth]{img/hid-tlr-bad-init.pdf}  
  \includegraphics[width=0.48\textwidth]{img/hid-tlr-bad-weird.pdf}  
  \includegraphics[width=0.48\textwidth]{img/hid-tlr-good-static.pdf}  
  \includegraphics[width=0.48\textwidth]{img/hid-tlr-good-tiny.pdf}  
  \includegraphics[width=0.48\textwidth]{img/hid-tlr-good-init.pdf}  
  \includegraphics[width=0.48\textwidth]{img/hid-tlr-good-weird.pdf}  
  \caption{\emph{TLR} hidden activations on the \emph{4-2-4 encoder}. Top $2\times2$ are {\bf un}successful networks and bottom $2\times2$ successful ones.}
  \label{fig:results-hidden-activations-tlr}
\end{figure}

%============================================================
\subsubsection{Features}
\label{sec:results-auto4-bal-matrix-sim}

TODO feature to epoch of best (in worst we will throw away) 

\subsubsection{Other} 

\paragraph{Recirculation BAL.} 
%======== (3D) L1 x L2 x epochs =========
%======== (3D) L1 x L2 x patSuccF =========
\begin{figure}[H]
  \centering
  \includegraphics[width=0.48\textwidth]{img/bal-recirc-auto4-success.pdf}   
  \includegraphics[width=0.48\textwidth]{img/bal-recirc-auto4-epoch.pdf}     
  \caption{BAL-recirc \ref{sec:our-bal-recirc} success and convergence on the \emph{4-2-4 Encoder} task with $\sigma = 2.3$ and $\mu = 0.0$.}
  \label{fig:results-bal-recirc-auto4-performance}
\end{figure}

\paragraph{Symmetric BAL.} 
\label{sec:our-bal-sym} 
\emph{Symmetric BAL (SymBAL)} is inspired by the necessary condition for convergence of GeneRec \citep{o1996bio} we set symmetric weights $W^{IH} = (W^{HI})^T$ and $W^{HO} = (W^{OH})^T$. We found no significant improvement when using this approach. TODO plot. 

\paragraph{GeneRec.} 
%======== (3D) L1 x L2 x epochs =========
%======== (3D) L1 x L2 x patSuccF =========
\begin{figure}[H]
  \centering
  \includegraphics[width=0.48\textwidth]{img/generec-auto4-success.pdf}   
  \includegraphics[width=0.48\textwidth]{img/generec-auto4-epoch.pdf}     
  \caption{GeneRec \ref{sec:models-generec} success and convergence on the \emph{4-2-4 Encoder} task with $\sigma = 2.3$ and $\mu = 0.0$.}
  \label{fig:results-generec-auto4-performance}
\end{figure}


\subsubsection{Conclusion} 
\label{sec:tlr-auto4-conclusion} 

TODO Explain / Make up hypotheses why it behaves as measured (Future work). \\
