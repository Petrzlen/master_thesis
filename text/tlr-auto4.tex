%Experimental Results and Analysis – in this section you should show the quantitative results – charts and tables. Analyze the results by explaining and highlighting what is important on them in terms of your goals and what is bad. You should explain the strange results too.

%V ďalšej časti prezentujte vlastný prínos a vlastné výsledky porovnajte s výsledkami iných. Charakterizujte použité metódy.
%Vyhýbajte sa používaniu žargónu.
%Používajte starú múdrosť: 1 obrázok je viac než 1000 slov.

\subsection{4-2-4 Encoder} 
\label{sec:results-auto4} 

%==================================================================
\paragraph{Introduction.} 
In this section we inspect TLR\ref{sec:our-tlr} performance for broad range of parameters $\lambda_h$ and $\lambda_v$. The network architecture is 4-2-4 what allows us to run plethora of simulations. There were two kinds of simulations. One being \emph{two dimensional maps (TDM)} where on the $x$ and $y$ axis $\lambda_v$ and $\lambda_h$ were plotted color was used for the $z$ value. Second being \emph{timelines} where a best configuration found by TDM was inspected. For TDM we run about 200-500 networks for each ($\lambda_v$, $\lambda_h$) and for timelines about 5000-10000 runs. After the simulation ended we took the average for each parameter setting. The networks were trained while $patSucc^F \neq 0$ or $epoch < MAX\_EPOCH$ where $MAX\_EPOCH$ was set to 100,000 in TDM and 1,000,000 in timelines. Note that most of the plots are in \emph{logarithmic} scale. 

%============================================================
\subsubsection{Comparison} 
\label{sec:tlr-auto4-cmp} 

For all our models \ref{sec:sim-our} and \ref{sec:sim-exp} tested on the 4-2-4 encoder task~\ref{sec:datasets-auto4} TLR~\ref{sec:our-tlr} had the best success rate. 

For TLR, BAL, GeneRec, BP, CHL, other learning rules
\label{sec:our-learning-rules}
It's possible for both BAL \ref{sec:models-bal} and GeneRec \ref{sec:models-generec} to try different learning rules mentioned in \ref{sec:models-generec-modifications}. We will denote such models as \emph{BAL-sym} and \emph{GR-sym} for symmetry perserving rule~\ref{eq:models-generec-learning-rule-sym}; \emph{BAL-mid} and \emph{GR-mid} for midpoint rule~\ref{eq:models-generec-learning-rule-mid} and \emph{BAL-chl} and \emph{GR-chl} for CHL rule~\ref{eq:models-generec-learning-rule-chl} but none of them worked. 

\begin{table}[H] 
  \centering
    \begin{tabular}{|l|l|l|l|l|}
    \hline
    Algorithm&$\lambda$&$patSucc^F$&Epcs&SEM \\
    \hline
    BP&2.4&100&60&5.1\\
    \hline
    AP&2.8&100&54&3.6\\
    \hline
    GR&0.6&90&418&28\\
    \hline
    GR Sym&1.4&56&88&2.9\\
    \hline
    GR Mid&2.4&92&60&3.4\\
    \hline
    CHL&1.2&56&77&1.8\\
    \hline
    BAL&0.9&62.7& 5136.11&2.0e+08\\
    \hline
    BAL TLR&0.0002 | 500&93.12&5845.01&1.52e+08\\
    \hline
    BAL TLR 1000 can&0.0002 | 500&99.86&150.417&5,070,000\\
    \hline
    BAL Recirc&1.2&43.9&2878.92&4.31e+07\\
    \hline
    BAL OLL& any & 0 & N/A \\ 
    \hline 
    \end{tabular}
  \caption{Comparing performance of different models on the \emph{4-2-4 encoder} task.} 
  \label{tab:results-cmp-auto4}
\end{table}

TODO note that one epoch computation is not same for different models, for instance GeneRec needs to settle in the iterative approach 

%============================================================
\subsubsection{Two learning rate} 
\label{sec:tlr-auto4} 
\ref{sec:datasets-auto4} 
%======== (3D) L1 x L2 x epochs =========
%======== (3D) L1 x L2 x patSuccF =========
\begin{figure}[H]
  \centering
  \includegraphics[width=0.48\textwidth]{img/tlr-auto4-success.pdf}   
  \includegraphics[width=0.48\textwidth]{img/tlr-auto4-epoch.pdf}     
  \caption{TLR success and convergence on the \emph{4-2-4 Encoder} task with $\sigma = 2.3$ and $\mu = 0.0$ best being setting $\lambda_h=0.0003$ and $\lambda_v=1000.0$.}
  \label{fig:results-tlr-auto4-performance}
\end{figure}

%======== (2D) best TLR on ALL_SUCC x epoch (std-dev) ==========
\begin{figure}[H]
  \centering
  \includegraphics[width=0.48\textwidth]{img/tlr-auto4-best-perf.pdf}   
  \includegraphics[width=0.48\textwidth]{img/tlr-auto4-best-can.pdf}      
  \caption{TLR success evolution for the \emph{4-2-4 Encoder} task. Left without candidate selection and right with candidates selection. } %TODO backward success
  \label{fig:results-tlr-auto4-epoch} 
\end{figure}


%============================================================
\subsubsection{Hidden activations}
\ref{sec:our-hidden-activation}  

%===== hidden activation timelines with commentaries (for TLR, BAL, GeneRec) 
% 2x success, 2x error (wrong settle, divergence) 

\begin{figure}[H]
  \centering
  \includegraphics[width=0.48\textwidth]{img/hid-bal-bad-init.pdf}  
  \includegraphics[width=0.48\textwidth]{img/hid-bal-bad-convex.pdf}  \\
  \includegraphics[width=0.48\textwidth]{img/hid-bal-bad-step.pdf}  
  \includegraphics[width=0.48\textwidth]{img/hid-bal-bad-stagnation.pdf}  \\
  \includegraphics[width=0.48\textwidth]{img/hid-bal-good-init.pdf}  
  \includegraphics[width=0.48\textwidth]{img/hid-bal-good-convex.pdf}  \\
  \includegraphics[width=0.48\textwidth]{img/hid-bal-good-step.pdf}  
  \includegraphics[width=0.48\textwidth]{img/hid-bal-good-stagnation.pdf}  \\ 
  \caption{\emph{BAL} hidden activations on the \emph{4-2-4 encoder}. Top $2\times2$ are {\bf un}successful networks and bottom $2\times2$ successful ones.}
  \label{fig:results-hidden-activations-bal}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.48\textwidth]{img/hid-tlr-bad-static.pdf}  
  \includegraphics[width=0.48\textwidth]{img/hid-tlr-bad-tiny.pdf}  
  \includegraphics[width=0.48\textwidth]{img/hid-tlr-bad-init.pdf}  
  \includegraphics[width=0.48\textwidth]{img/hid-tlr-bad-weird.pdf}  
  \includegraphics[width=0.48\textwidth]{img/hid-tlr-good-static.pdf}  
  \includegraphics[width=0.48\textwidth]{img/hid-tlr-good-tiny.pdf}  
  \includegraphics[width=0.48\textwidth]{img/hid-tlr-good-init.pdf}  
  \includegraphics[width=0.48\textwidth]{img/hid-tlr-good-weird.pdf}  
  \caption{\emph{TLR} hidden activations on the \emph{4-2-4 encoder}. Top $2\times2$ are {\bf un}successful networks and bottom $2\times2$ successful ones.}
  \label{fig:results-hidden-activations-tlr}
\end{figure}

%============================================================
\subsubsection{Features}
\label{sec:results-auto4-bal-matrix-sim}

TODO feature to epoch of best (in worst we will throw away) 

\subsubsection{Other} 

\paragraph{Recirculation BAL.} 
%======== (3D) L1 x L2 x epochs =========
%======== (3D) L1 x L2 x patSuccF =========
\begin{figure}[H]
  \centering
  \includegraphics[width=0.48\textwidth]{img/bal-recirc-auto4-success.pdf}   
  \includegraphics[width=0.48\textwidth]{img/bal-recirc-auto4-epoch.pdf}     
  \caption{BAL-recirc \ref{sec:our-bal-recirc} success and convergence on the \emph{4-2-4 Encoder} task with $\sigma = 2.3$ and $\mu = 0.0$.}
  \label{fig:results-bal-recirc-auto4-performance}
\end{figure}

\paragraph{Symmetric BAL.} 
\label{sec:our-bal-sym} 
\emph{Symmetric BAL (SymBAL)} is inspired by the necessary condition for convergence of GeneRec \citep{o1996bio} we set symmetric weights $W^{IH} = (W^{HI})^T$ and $W^{HO} = (W^{OH})^T$. We found no significant improvement when using this approach. TODO plot. 

\paragraph{Generec.} 
%======== (3D) L1 x L2 x epochs =========
%======== (3D) L1 x L2 x patSuccF =========
\begin{figure}[H]
  \centering
  \includegraphics[width=0.48\textwidth]{img/generec-auto4-success.pdf}   
  \includegraphics[width=0.48\textwidth]{img/generec-auto4-epoch.pdf}     
  \caption{GeneRec \ref{sec:models-generec} success and convergence on the \emph{4-2-4 Encoder} task with $\sigma = 2.3$ and $\mu = 0.0$.}
  \label{fig:results-generec-auto4-performance}
\end{figure}


\subsubsection{Conclusion} 
\label{sec:tlr-auto4-conclusion} 

TODO Explain / Make up hypotheses why it behaves as measured (Future work). \\
