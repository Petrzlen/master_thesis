%At the end of your thesis you can attach resources such as source code (or something like ASCII code table) that would improve the completeness of your thesis.

\section*{Appendix A - Implementation}
\appendix
\addcontentsline{toc}{section}{Appendix A - Implementation}
\markboth{Appendix A}{}
\label{sec:appendix-impl} 


%TODO crucial parts of the implementation 
%TODO additional tables, measures 

%Implementation â€“ in implementation section you should mention the tools that you use to implement, the target environment (e.g. linux, windows). Limitations (e.g. buffer sizes, connections number).

\subsection*{GeneRec} 
\label{sec:appendix-impl-generec} 

Activation is computed as in \ref{eq:models-generec-activation}. How we implemented the details which are not talked about in \citet{o1996bio}. They are related to BAL \ref{sec:models-bal}. 

\subsubsection*{Bias} 
%TODO reformulate
%TODO add more recent citations 
%TODO add results with / without bias 
%TODO parse references 
The use of such biases in neural networks has been discussed in the context of the fundamental bias/variance tradeoff \citet{geman1992neural}. This tradeoff emphasizes the fact that biases that are appropriate for the task can greatly facilitate learning and generalization by reducing the level of variance, where variance reflects the extent to which parameters are underconstrained by learning, and thus free to vary, causing random errors in generalization. These biases are also known as regularizers \citet{poggio1990networks}. However, inappropriate biases can obviously hurt performance by introducing systematic errors, such that there is no such thing as a single universally beneficial set of biases (Wolpert, 1996a; Wolpert, 1996b). 


\paragraph{Generec-Bias} 

Therefore we tried several settings: 
\begin{itemize} 
\item No bias - two patterns for 4-2-4, three patterns for 4-3-4, all four patterns for 4-4-4
\item Bias only on the input layer:  
4-2-4 best network error = 3.0 (tended to have two very profiled (0.95+, 0, 0, 0) and two very even (0.5, 0.5, 0.5, 0.5)  \\
4-3-4 about 75\% success rate 
\item Bias on both input and hidden layers: 
    4-2-4 0.0 1.0 0.5 0.1 91.74311926605505 100/109, almost no 1.0 errors 
          epochs ranged from 9 to 4500 \\ 
    4-3-4 about 100\% succes rate with about 20 epochs (epoch ranged from 5 to 700) 
\item Bias on all matrices (IH, HO, OH): 
    seems to add a little chaos with a slighty worse performance (but maybe bad sigma)

\end{itemize} 

\subsubsection*{Fluctuation} 
\label{sec:generec-fluctuation}

Asymmetric BIA \ref{sec:our-bal-recirc}.

%\paragraph{Recirculation step} %====================
If no stationary point solution is found after MAX\_ITERATION then we set the result to avarage of the last two activations. This lead to 80\% less fluctuation. But still occured: 
\begin{itemize}
\item Big fluctuation: 0.9591299483462391
\item Not enough iterations: 20
\item Even if MAX\_ITERATION = 200 then max fluctuation still could be arbitrary and oscilating.
\end{itemize} 

%\paragraph{Interesting} %====================
\begin{itemize} 
  \item oscilation in iteration could occur randomly (just in some epochs and it will completely change the network) 
  \item when using averages, it's less probable that a fluctuation will occur 
  \item setting MAX\_iteration much higher doesn't affect performance. About 50 is enough but 20 is not. 
\end{itemize} 
  
%\paragraph{Conclusion} %====================
Symmetric weights haven't helped BAL (35\%) - no param selection
Using bothward GeneRec (60\%) - no param selection 
  Forgot to symmetric init -> no perceivable change 
In both cases (almost) no fluctuation 

Non-symmetric case, fluctuation in about 1/5 cases 
  About 30\% success 
  About 3-33 iterations needed to settle, very network dependent 
  About 50 - 5000 epochs needed 

IDEA: maybe bad implementation of backward
      using iterative activation has almost no reason (bothward is the generec idea) 

%TODO Convergence & Stability & Fluctuation 

\subsubsection*{Initial weight distribution} 
\label{sec:our-sigma} 

Based on \citet{o1996bio} we used the following probability distribution for weight initialization: 

\begin{equation} 
\frac{1}{\sqrt{2\pi \sigma^2} e^{-\frac{(x-\mu)^2}{2\sigma^2}}},
\end{equation} 

where $\sigma$ is one of the network parameters (ref) and $\mu = 0$ as the expected value of the normal distribution. 

Our motivation to try this approach came from the fact that most of the BAL networks were capable to learn the given tasks easily, but others converged to error states (ref). Therefore we believed that there should exists a classifier which separates the successfull weight initialization from the unsuccessfull ones (ref - future work). 

TODO references.
\ref{sec:results-sigma} 

