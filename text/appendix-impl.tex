%At the end of your thesis you can attach resources such as source code (or something like ASCII code table) that would improve the completeness of your thesis.

\section*{Appendix A - Implementation}
\appendix
\addcontentsline{toc}{section}{Appendix A - Implementation}
\markboth{Appendix A}{}
\label{sec:appendix-impl} 

%Implementation â€“ in implementation section you should mention the tools that you use to implement, the target environment (e.g. linux, windows). Limitations (e.g. buffer sizes, connections number).

\subsection*{GeneRec} 
\label{sec:appendix-impl-generec} 

In this section, we discuss how we implemented some of the details which are not talked about in~\citet{o1996bio}. These are related mainly to \emph{BAL-recirc}~(\ref{sec:our-bal-recirc}) and also BAL~(\ref{sec:models-bal}). For how GeneRec works please consult~(\ref{sec:models-generec}).  

\subsubsection*{Bias} 

Most of artificial neural networks come with \emph{bias}es~(\ref{sec:models-perceptron}) which allow arbitrary \emph{separating hyperplane}s. To our knowledge~\citet{o1996bio} talks nothing about biases and therefore, we had to try several setting, namely \emph{no bias}, \emph{only--input bias}, \emph{only--forward bias} and \emph{full bias}. %TODO reformulate the itemize, maybe a table. 

\begin{itemize} 
\item No bias - two patterns for 4-2-4, three patterns for 4-3-4, all four patterns for 4-4-4
\item Bias only on the input layer:  
4-2-4 best network error = 3.0 (tended to have two very profiled (0.95+, 0, 0, 0) and two very even (0.5, 0.5, 0.5, 0.5)  \\
4-3-4 about 75\% success rate 
\item Bias on both input and hidden layers: 
    4-2-4 0.0 1.0 0.5 0.1 91.74311926605505 100/109, almost no 1.0 errors 
          epochs ranged from 9 to 4500 \\ 
    4-3-4 about 100\% succes rate with about 20 epochs (epoch ranged from 5 to 700) 
\item Bias on all matrices (IH, HO, OH): 
    seems to add a little chaos with a slighty worse performance (but maybe bad sigma)

\end{itemize} 

\subsubsection*{Fluctuation} 
\label{sec:generec-fluctuation}

%Asymmetric BIA~(\ref{sec:our-bal-recirc}).
%TODO rewrite \\ 

%\paragraph{Recirculation step} %====================
If no stationary point solution is found after MAX\_ITERATION then we set the result to average of the last two activations. This lead to 80\% less fluctuation. But still occured: 
\begin{itemize}
\item Big fluctuation: 0.9591299483462391
\item Not enough iterations: 20
\item Even if MAX\_ITERATION = 200 then max fluctuation still could be arbitrary and oscilating.
\end{itemize} 

%\paragraph{Interesting} %====================
\begin{itemize} 
  \item oscilation in iteration could occur randomly (just in some epochs and it will completely change the network) 
  \item when using averages, it~is less probable that a fluctuation will occur 
  \item setting MAX\_iteration much higher does no t affect performance. About 50 is enough but 20 is not. 
\end{itemize} 
  
%\paragraph{Conclusion} %====================
Symmetric weights have not helped BAL (35\%) - no param selection \\ 
Using bothward GeneRec (60\%) - no param selection 
  Forgot to symmetric init -> no perceivable change 
In both cases (almost) no fluctuation 

Non-symmetric case, fluctuation in about 1/5 cases 
  About 30\% success \\
  About 3-33 iterations needed to settle, very network dependent  \\
  About 50 - 5000 epochs needed  \\

IDEA: maybe bad implementation of backward
      using iterative activation has almost no reason (bothward is the generec idea) 

\subsubsection*{Initial weight distribution} 
\label{sec:results-sigma} %TODO merge? 
\label{sec:our-sigma} 

Based on~\citet{o1996bio} we used the following probability distribution for weight initialization: 
\begin{equation} 
\frac{1}{\sqrt{2\pi \sigma^2} e^{-\frac{(x-\mu)^2}{2\sigma^2}}} \nonumber,
\end{equation} 
where $\sigma$ is one of the network parameters (ref) and $\mu = 0$ as the expected value of the normal distribution. As our experiemnts with hidden activations suggests~(\ref{sec:our-hidden-activation}), initial weight distribution could be crucial for BAL. Therefore, we had the idea to classify initial weight matrices with labelling them successfull or unsuccessful. This could be implementable for the \emph{4-2-4 encoder}~(\ref{sec:datasets-auto4}) tasks where we would have $4\times(4\times(2+1))=48$ attributes dataset, one for each weight, and one label.



\subsubsection*{Stopping criterion}
\label{sec:our-stopping-criteria} 

