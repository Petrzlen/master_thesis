
\subsubsection{Candidate selection} 

\paragraph{Introduction.} 
Before the training phase we randomly generate $N$ networks from which we select a \emph{best candidate} network based on network features. We used this \emph{candidate selection} approach to test if some particular features have an impact to the overall network performance. When we found such a features we then tried to design a network which was consistent to our motivation (ref) and reinforced that feature. 

\paragraph{Features.}

TODO describe it more
We measured the following features: 
\begin{itemize} 
\item err - current bit error 
\item sigma - initial weight distribution 
\item lambda - learning rate 
\item lambda\_I - in the two lambda model (ref) the setting of the learning rate for $w_{IH}$ and $w_{OH}$, i.e. from the outside-hidden activations. For the rest two weight matrices $w_{HI}$ and $w_{HO}$ the \emph{lambda} was used. 
\item momentum
\item h\_dist - $avg dist(h_i - h_j) i \neq j where h_i is   hidden activation for input i$
\item h\_f\_b\_dist - avg distance between forward and backward activations on hidden layer. Note this feature is only reasonable for the auto-associative tasks. 
\item m\_avg\_w - avg weight of matrixes. 
\item m\_sim - MATRIX\_SIMILARITY - sum of $|a_{ij} - b_{ij}|$ per pairs (HO, HI) and (OH, IH) 
\item first\_second - sum of ratio of $(a_1, a_2)$ where $a_i$ is the $i$-th biggest output 
\item	o\_f\_b\_dist - OUTPUT\_FOR\_BACK\_DIST - avg distance between forward and backward activations on their output layers (between forward layer 2, backward layer 0) 
\item in\_triangle - check if some point is inside a polygon from others (TODO only for hidden size = 2?)
\item fluctuation - when treating the opposite weight matrices (i.e. $w_IH$ to $w_HI$ and $w_HO$ to $w_OH$) as dynamic system as in CHL, GeneRec and RecirculationBal then \emph{fluctuation} measures the last two activations when finding the stacionary point solution iteratively. So this is zero if the fixed point solution is found and not zero if it's not. 

%	//public static  int MEASURE_NOISE_SPAN = 9; 
%	//public static  int MEASURE_MULTIPLY_WEIGHTS = 9; 

\end{itemize} 

\paragraph{Pseudocode.} 
\begin{lstlisting} 
candidates = [generate_network for i in range(0, N)] 
candidate = max(pair(fitness(c),c) for c in candidates) 
train(candidate.c) 
\end{lstlisting} 


\paragraph{Results.} 
If we ignore the \emph{lambda\_I} feature then the most important ones were \emph{h\_dist} and \emph{in\_triangle} which intuitively have similar values. This results was the main reason of the development of the two-lambda model (ref). The following figure shows that when selecting the candidate with the highest \emph{h\_dist} among 100 randomly generated networks it could lead to about 10\% success increase. 

TODO figure candidate selection compare to standard BAL 

Hidden distance (over 70\%) over in triangle (68.3 \%). 
