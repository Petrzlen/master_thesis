\subsubsection{Candidate selection} 
\label{sec:our-candidates} 

\paragraph{Introduction.} 
The \emph{candidate selection} model was used to test and prove if some particular network \emph{features} \ref{sec:our-candidates-features} have an impact to the overall network performance. Only difference between standard BAL \ref{sec:models-BAL} is that before the training phase $N$ networks are randomly generated from which a \emph{best candidate network} is selected based on \emph{feature function} defined as $F: \mbox{network} \mapsto \mathbb{R}$. The feature function was trained on a \emph{feature dataset} containing inficidual features and a binary label \emph{success}. The dataset was generated by running standard BAL, measuring features before the run and adding the success label after the training. The notion of most important features was used to design other models which reinforced these features, e.g. the \emph{Two learning rate} model \ref{sec:our-two-lambdas} for the \emph{h\_f\_b\_b\_dist} feature. 

\paragraph{Features.}
\label{sec:our-candidates-features}

TODO describe it more

We measured the following features: 
\begin{itemize} 
\item err - current bit error 
\item sigma $\sigma$ - initial weight distribution 
\item lambda $\lambda$ or $\lambda_h$ - common learning rate or learning rate for $W^{IH}$ and $W^{OH}$. 
\item lambda\_h $\lambda_v$ - in the Two learning rate model \ref{sec:our-two-lambdas} the value of learning rate for $W^{IH}$ and $W^{OH}$
\item momentum $\mu$ - \ref{sec:our-momentum}
\item h\_dist - $avg dist(h_i - h_j) i \neq j where h_i is   hidden activation for input i$
\item h\_f\_b\_dist - avg distance between forward and backward activations on hidden layer. Note this feature is only reasonable for the auto-associative tasks. 
\item m\_avg\_w - avg weight of matrixes. 
\item m\_sim - MATRIX\_SIMILARITY - sum of $|a_{ij} - b_{ij}|$ per pairs (HO, HI) and (OH, IH) 
\item first\_second - sum of ratio of $(a_1, a_2)$ where $a_i$ is the $i$-th biggest output 
\item	o\_f\_b\_dist - OUTPUT\_FOR\_BACK\_DIST - avg distance between forward and backward activations on their output layers (between forward layer 2, backward layer 0) 
\item in\_triangle - check if some point is inside a polygon from others (TODO only for hidden size = 2?)
\item fluctuation - when treating the opposite weight matrices (i.e. $w_IH$ to $w_HI$ and $w_HO$ to $w_OH$) as dynamic system as in CHL, GeneRec and RecirculationBal then \emph{fluctuation} measures the last two activations when finding the stacionary point solution iteratively. So this is zero if the fixed point solution is found and not zero if it's not. 
\item bit\_succ\_f
\item bit\_succ\_b
\item pat\_succ\_f 
\item pat\_succ\_b

%	//public static  int MEASURE_NOISE_SPAN = 9; 
%	//public static  int MEASURE_MULTIPLY_WEIGHTS = 9; 

\end{itemize} 

\paragraph{Pseudocode.} 
\begin{lstlisting} 
candidates = [generate_network for i in range(0, N)] 
candidate = max(pair(fitness(c),c) for c in candidates) 
train(candidate.c) 
\end{lstlisting} 


\paragraph{Results.} 
If we ignore the \emph{lambda\_I} feature then the most important ones were \emph{h\_dist} and \emph{in\_triangle} which intuitively have similar values. This results was the main reason of the development of the two-lambda model (ref). The following figure shows that when selecting the candidate with the highest \emph{h\_dist} among 100 randomly generated networks it could lead to about 10\% success increase. 

TODO figure candidate selection compare to standard BAL 

Hidden distance (over 70\%) over in triangle (68.3 \%). 
