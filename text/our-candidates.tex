\subsubsection{Candidate selection} 
\label{sec:our-candidates} 

\paragraph{Introduction.} 
The \emph{candidate selection} model was used to test and prove if some particular network \emph{features} \ref{sec:our-candidates-features} have an impact to the overall network performance. Only difference between standard BAL \ref{sec:models-bal} is that before the training phase $N$ networks are randomly generated from which a \emph{best candidate network} is selected based on \emph{feature function} defined as $F: \mbox{network} \mapsto \mathbb{R}$. The feature function was trained on a \emph{feature dataset} containing inficidual features and a binary label \emph{success}. The dataset was generated by running standard BAL, measuring features before the run and adding the success label after the training. The notion of most important features was used to design other models which reinforced these features, e.g. the \emph{Two learning rate} model \ref{sec:our-two-lambdas} for the \emph{h\_f\_b\_b\_dist} feature. 

\begin{figure}[H]
    \begin{lstlisting} 
      candidates = [generate_network for i in range(0, N)] 
      candidate = max(pair(fitness(c),c) for c in candidates) 
      train(candidate.c) 
    \end{lstlisting} 
  \caption{Candidate selection pseudocode.}
  \label{fig:our-candidates-pseudocode} 
\end{figure} 

\paragraph{Features.}
\label{sec:our-candidates-features}

TODO describe it more \\
TODO note when auto / hetero associative \\

We denote $X_I$, $H_I$ and $Y_I$ as the front, hidden and back activation \emph{vector}s for input $I$ (and the corresponding target). For the rest of the notation please consult \ref{tab:models-bal-activation}. We measured the following \emph{features}: 
\begin{itemize} 
\item h\_dist - $avg_{I \neq J}\left(dist(H_I^{+},H_J^{+})\right)$.
\item h\_f\_b\_dist - $avg_{I}\left(dist(H_I^{-},H_I^{+})\right)$.
\item	v\_f\_b\_dist - as h\_f\_b\_dist but for visible layer: $avg_{I}\left(dist(Y_I^{-},X_I^{+})\right)$. 
\item matrix\_w - average weight of matrices $W^{IH}$, $W^{HO}$, $W^{OH}$ and $W^{HI}$. 
\item matrix\_sim - sum of $|a_{ij} - b_{ij}|$ per pairs $(a,b) \in ((W^{IH}, W^{OH}),\, (W^{HI}, W^{HO}))$. 
\item \label{sec:our-in-triangle} in\_triangle - for hidden size equal two: check if hidden representations of inputs form a convex polygon, i.e.~if the hidden representations are lineary separable \ref{sec:models-perceptron}. 
\item fluctuation - when treating the opposing weight matrices, i.e.~$w_IH$ to $w_HI$ and $w_HO$ to $w_OH$, as in GeneRec activation computation \ref{eq:models-generec-activation} then \emph{fluctuation} measures:
\begin{equation}
  \label{eq:our-fluctuation}
  max_{I}\left(
    max_{i \in \mbox{\tiny units}}\left(
      |a_i(t_{\mbox{\tiny stop}}) - a_i(t_{\mbox{\tiny stop}-1})
    \right)|
  \right). 
\end{equation}

%\item err - current bit error 
%\item first\_second - sum of ratio of $(a_1, a_2)$ where $a_i$ is the $i$-th biggest output 
%\item sigma $\sigma$ - initial weight distribution 
%\item lambda $\lambda$ or $\lambda_h$ - common learning rate or learning rate for $W^{IH}$ and $W^{OH}$. 
%\item lambda\_h $\lambda_v$ - in the Two learning rate model \ref{sec:our-two-lambdas} the value of learning rate for $W^{IH}$ and $W^{OH}$
%\item momentum $\mu$ - \ref{sec:our-momentum}

%\label{sec:sim-evaluation-methods} 

%	//public static  int MEASURE_NOISE_SPAN = 9; 
%	//public static  int MEASURE_MULTIPLY_WEIGHTS = 9; 
\end{itemize} 

Also all \emph{parameters} of neural network were included such as $\lambda$, $\lambda_h$, $\lambda_v$ \ref{sec:our-two-lambdas}, momentum $\mu$ \ref{sec:our-momentum} and weight distribution $\sigma$ \ref{sec:our-sigma}. We further analyse the results in \ref{sec:results-candidates} 
