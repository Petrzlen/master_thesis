
\subsection{Datasets}  
\label{sec:sim-data} 

For analysing our versions of BAL~(\ref{sec:models-bal}) we have chosen three datasets on which we tested and compared the performance of our models. 

\subsubsection{4-2-4 Encoder} 
\label{sec:datasets-auto4}


The \emph{4-2-4 econder} task is the simplest dataset we have been working with. It consists of four four--dimensional samples $(1,0,0,0)$, $(0,1,0,0)$, $(0,0,1,0)$ and $(0,0,0,1)$ where each sample is mapped to itself. We use two units on the hidden layer what gives us the 4-2-4 architecture. The 4-2-4 encoder task is a well--known problem and used previously for testing GeneRec~\citep{o1996bio} and BAL~\citep{farkas2013bal}. In the case of BAL only 60--65\% $patSucc^F$ was achieved what leaves window for improvement. We chose this dataset as it~is convenient for testing novel approaches as the learning progress of the network could be checked by hand and eye. 

\subsubsection{Complex binary vector associations} 
\label{sec:datasets-k3}

The \emph{complex binary vector associations (CBVA)} task was used in~\citet{farkas2013bal} and its motivated by the sensory--motor mappings between distributed patterns. The task is to associate between sixteen 16--dimensional vectors which all had 3 active units. There are always 4 distinct overlapping input patterns associated with exactly one output pattern. As there are several possibilities of inputs for each output, then in the \emph{backward} way it~is impossible to achieve perfect $bitSucc^B$ or $patSucc^B$. Therefore, we would expect from the network to give a \emph{blend} of the four input patterns corresponding to one output. 

\begin{figure}[H]
  \centering
  \includegraphics[width=0.98\textwidth]{img/datasets-k3.png} 
  \caption{BAL performance on \emph{CBVA}. Black color stands for target--estimate match, gray for target only and gray with a cross for false--positive estimate~\citep{farkas2013bal}.}
  \label{fig:datasets-k3}
\end{figure}

%tail -n +2 train.csv | sed 's/,/ /g' | awk 'BEGIN{FS=" "}{for(i=2;i<=NF;i++) {printf "%f ", $i / 256.0} printf "\n"}' >> buf.in
%echo "38000 784" > digits.in
%echo "4000 784" > digits.test.in
%tail -n +2 train.csv | sed 's/,/ /g' | awk 'BEGIN{FS=" "}{for(i=0;i<=9;i++) printf("%d ", $1 == i ? 1 : 0); printf "\n"}' > buf.out
%echo "38000 10" > digits.out
%echo "4000 10" > digits.test.out
%head -38000 buf.in >> digits.in 
%tail -4000 buf.in >> digits.test.in 
%head -38000 buf.out >> digits.out
%tail -4000 buf.out >> digits.test.out
%rm buf.* 
\subsubsection{Handwritten digits} 
\label{sec:datasets-digits} 

The well--known MNIST dataset of \emph{handwritten digits}~\citep{digits2014mnist} first analysed by~\citet{lecun1998gradient} consists of 42,000 samples of $28 \times 28$ grayscale images mapped to one digit. We have chosen this dataset for three reasons. First, it~is big and complex enough to test the practicallity of our models. Second, performance of many models is known on this dataset and therefore, we can easily compare performance of our models to these models. And third, we can easily visualize the backward \emph{blend} representations and intuitively confirm if our models perform as expected. These visualisations could be found in Section~\ref{sec:our-backward-repre}. 

\begin{figure}[H]
  \centering
  \includegraphics[width=0.4\textwidth]{img/digits.png} 
  \caption{Samples from the \emph{digits} dataset.}
  \label{fig:datasets-digits}
\end{figure}


