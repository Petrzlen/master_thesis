\subsubsection{Other experiments}

In this section, we describe additional models, model modifications and experiments we proposed and used during our work. We further introduce notation used in the results section. 

\paragraph{Weight initialization classification.} 
\label{sec:our-weight-init-class}
As candidate selection suggested in Section \ref{sec:sim-exp-candidates}, weight initialization could be crucial for the success rate of BAL. To further analyse this hypothesis we propose the following experiment. First we generate $n$ networks $N_i$ with random weights $W_i$, train them and label them $s_i = patSucc^F$. This way we get a dataset $D=(W_i, s_i)$ for which we can fit a model $M$ and use it for prediction of $s_i$. Then we analyse $M$ and try to propose hypothesis for successful weights. Finally, we would modify the weight initialization algorithm that it will support the more successful networks. Note that this experiment was not implemented but we recommend it for future work. 

\label{sec:our-sigma} 

In our work, we used a weight initialization algorithm inspired by~\citet{o1996bio}. For each weight $w_{ij}$ we select randomly a value from the normal probability distribution:
\begin{equation} 
  \frac{1}{\sqrt{2\pi \sigma^2}} \exp{-\frac{(x-\mu)^2}{2\sigma^2}},
  \label{eq:our-sigma} 
\end{equation} 
where $\sigma$, i.e.~\emph{weight initialization constant}, is one of the network parameters and $\mu = 0$ is the expected value of the normal distribution. We showed that $\sigma$ influences the success rate~(\ref{eq:results-candidates-linear-regression}). It is recommended by~\citet{o1996bio} to set $\sigma=\frac{1}{\sqrt{N + 1}}$ where $N$ is the number of units on the input layer including bias.

%==========================================================
\paragraph{Dynamic learning rate.} 
\label{sec:our-dynamic-lambda} 
The idea of \emph{dynamic learning rate (DLR)}~\citep{jacobs1988increased} is to have a separate \emph{learning rate} for each weight $w_{ij}$ which could change in time $t$ denoted as $\lambda_{ij}(t)$. There are several possibilities how to set $\lambda_{ij}(t)$ which are briefly described in Section ~\ref{sec:our-tlr-related-work}. We further tried to develop our own DLR model which would depend on error of the previous epoch. In one of our trials, we set the learning rate to be smaller with smaller error to make BAL settle hidden activations later. Although we were not able to increase the success rate, we admit there is space for further improvement~(\ref{sec:future-work}). This modification was an inspiration for TLR~(\ref{sec:our-tlr}). 

%==========================================================
\paragraph{Batch mode.} 
\label{sec:our-batch-mode}
Before each epoch the training samples are shuffled. But in \emph{batch mode} instead of updating weights after each training sample, the weight changes are accumulated for the whole epoch. With other words, all weight changes are summed for each sample in the training set and then weights are updated in \emph{batch}. One can observe, that after the weights are initialized, the learning algorithm becomes deterministic. This approach could be used to confirm or disprove the importance of weight initialization. We executed several simulations on BAL with \emph{batch} weight update, but it had no significant impact. 

%==========================================================
%\paragraph{Rerun.} This experiment was designed to test if shuffling of samples and weight initialization has effect to network performance. First, $N$ networks are created with random weights~(\ref{sec:our-sigma}) while having same parameters and then saved and trained. Second, the networks are loaded and each network is re--trained $k$-times, thus the name \emph{rerun}. At the end we measure the difference between performance of the $k$ networks and the original network. 

%OPT rerun experiment (it has suspicious results) 

%\begin{lstlisting}
%All bad: 
%err sigma lambda momentum success sample_ratio
%0.0 2.3 0.7 0.0 19.296918767507005 6889/35700
%1.0 2.3 0.7 0.0 68.05602240896359 24296/35700
%2.0 2.3 0.7 0.0 12.644257703081232 4514/35700
%3.0 2.3 0.7 0.0 0.0028011204481792717 1/35700

%All good: 
%err sigma lambda momentum success sample_ratio
%0.0 2.3 0.7 0.0 99.98911353032659 64293/64300
%1.0 2.3 0.7 0.0 0.01088646967340591 7/64300
%\end{lstlisting}

%==========================================================
\paragraph{Dropout.}
\label{sec:sim-our-dropout}
Based on the work of~\citet{hinton2012improving}, we implemented the \emph{dropout} method of learning. The main idea is that in each epoch, we randomly choose half of the hidden layer neurons, which will be ignored for this epoch. With other words, in each epoch a random subset of hidden neurons is chosen to be active, while the other hiddens are ignored. The motivation is to prevent co--adaptation of the hidden layer neurons~\citep{hinton2012improving}. We were not able to train any successful BAL~(\ref{sec:models-bal}) network using dropout on the 4-2-4 encoder~(\ref{sec:datasets-auto4}) and CBVA~(\ref{sec:datasets-k3}) tasks. Therefore we soon dropped the idea, but we admit, that setting other probability $p$ for dropout or applying it on higher dimensional tasks could have a positive impact on success rate. 

%==========================================================
\paragraph{Noise.} 
\label{sec:sim-our-noise} 

Motivated by the \emph{chaotic} behaviour of nature itself we tried adding \emph{random noise} to each weight update. We hoped, that the possible noise could prevent settling of hidden activations to fast~(\ref{sec:our-hidden-activation}). Our simulations of BAL using random noise showed no increase in performance. %This result was backed up by the candidate selection experiment~(\ref{sec:sim-exp-candidates}) wehre the resulting linear regression model \ref{eq:results-candidates-linear-regression} showed no impact on the error. 

%==========================================================
\paragraph{Multilayer GeneRec.}
\label{sec:sim-our-generec-multi} 

We implemented a multi-layer version of GeneRec~(\ref{sec:models-generec}). The recirculation step in the minus phase was extended to $2L-3$ steps, where $L$ is the number of layers. First the propagation of neuron activations goes $L-1$ times forward and next it goes $L-2$ times backward. Then the recirculation step~(\ref{sec:models-generec-activation}) between layers $2,\,3,\,\ldots,\,L$ is executed. Our implementation of multi-layer GeneRec using the 784--300--50--10 architecture achieved 43.22\% success rate on the handwritten digits recognition task~(\ref{sec:datasets-digits}). 
