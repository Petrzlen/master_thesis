\subsubsection{Other}

In this section we describe other models, model modifications and experiments we used in our work. We also introduce notation used in the results section (\ref{sec:results}) and discuss related work. 

%==========================================================
\paragraph{Dynamic learning rate.} 
\label{sec:our-dynamic-lambda} 
The idea of \emph{dynamic learning rate (DLR)} \citep{jacobs1988increased} is to have different \emph{learning rate} $\lambda_{ij}(t)$ for each weight $w_{ij}$ which could change in time $t$, i.e. \emph{epoch}. There are several approaches how to set $\lambda_{ij}(t)$. We briefly introduced them in related work with TLR (\ref{sec:our-tlr-related-work}). We tried to develop our own DLR model which would be dependendent on error of the previous epoch. We set the learning rate to be smaller with smaller error so that BAL will settle the hidden activations later. We were not able to increase performance with this approach but we admit there is lot of space for further experiments (\ref{sec:future-dlr}). This model was inspiration for TLR (\ref{sec:our-tlr}). 


%TODO select some interesting learning rules 
%Dynamic lambda per weight \emph{delta--bar--delta--rule} baseline article \citep{jacobs1988increased}. \\
%Adaptive learning rate, Only sign of weight update \citep{riedmiller1993direct} \\
%Dynamic learning rate \citep{yu1997efficient}, \citep{yu2002backpropagation} \\ 
%Adaptive learning rate BP \citep{behera2006adaptive}, \citep{magoulas1999improving} \\
%Momemntum adaptation \citep{miniani1990acceleration} -> future work 
%Determining learning rate \citep{weir1991method} 


%==========================================================
\paragraph{Batch mode.} Instead of updating weights after each training sample, weight changes are accumulated for the whole epoch. With other words, we sum up all weight changes for each sample and then weights are updated in \emph{batch}. One can observe that shuffling of samples has no effect to \emph{batch} training at all. Therefore after the weights are initialized, the learning algorithm becomes deterministic. This approach could be used to prove or disprove the importance of weight initialization. Running several simulations on BAL with \emph{batch} weight update had no significant impact on performance. 

%==========================================================
\paragraph{Rerun.} This experiment was designed to test if shuffling of samples and weight initialization has effect to network performance. First, $N$ networks are created with random weights~(\ref{sec:our-sigma}) while having same parameters and then saved and trained. Second, the networks are loaded and each network is re--trained $k$-times, thus the name \emph{rerun}. At the end we measure the difference between performance of the $k$ networks and the original network. 

TODO rerun experiment (it has suspicious results) 

%\begin{lstlisting}
%All bad: 
%err sigma lambda momentum success sample_ratio
%0.0 2.3 0.7 0.0 19.296918767507005 6889/35700
%1.0 2.3 0.7 0.0 68.05602240896359 24296/35700
%2.0 2.3 0.7 0.0 12.644257703081232 4514/35700
%3.0 2.3 0.7 0.0 0.0028011204481792717 1/35700

%All good: 
%err sigma lambda momentum success sample_ratio
%0.0 2.3 0.7 0.0 99.98911353032659 64293/64300
%1.0 2.3 0.7 0.0 0.01088646967340591 7/64300
%\end{lstlisting}

%==========================================================
\paragraph{Dropout.}
Based on the work of \citet{hinton2012improving} we implemented the \emph{dropout} method of learning. The main ideas is that in each epoch we randomly choose half of the hidden layer neurons which will be ignored for this epoch. In other words in each epoch a random subset of hidden neurons is chosen to use while the others are ignored. The motivation behind is to prevent co--adaption of the hidden layer neurons \citep{hinton2012improving}. Our simulations with BAL (\ref{sec:models-bal}) on 4-2-4 encoder (\ref{sec:datasets-auto4}) and CBVA (\ref{sec:datasets-k3}) tasks were not able to learn the task. We dropped the idea soon but we admit that setting other probability of dropout $p$ or applying it on high--dimensional tasks could have a positive impact. 

%==========================================================
\paragraph{Noise.} 

Motivated by the \emph{chaotic} behaviour of nature itself we tried adding \emph{random noise} to each weight update. We hoped that the possible noise could prevent settling of hidden activations to fast (\ref{sec:our-hidden-activation}). Our simulation of BAL (\ref{sec:models-bal}) with added random noise showed no performance increase. This result was backed up by the candidate selection experiment (\ref{sec:sim-exp-candidates}) wehre the resulting linear regression model \ref{eq:results-candidates-linear-regression} showed no impact on the error. 

%==========================================================
\paragraph{Multilayer GeneRec.}

We implemented a multilayer version of GeneRec (\ref{sec:models-generec}). The recirculation step in the minus phase was extended. First it goes two times forward and next it goes two times backward while the activation from input is constant. Our implementation of multilayer Generec achieved 42\% success rate with 784-300-50-10 architecture on the hand-written digit recognition task (\ref{sec:datasets-digits}). 
