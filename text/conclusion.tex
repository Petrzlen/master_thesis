% ==================== 16. Conclusion ========================
% This chapter should conclude on your contribution. It should highlight the key results from the research work. In this section, you should avoid mentioning new terms and statements not discussed throughout the main text. Also general aspects of the research work shouldn’t be repeated here. Conclusion shouldn’t be the abstract written in past tense. The conclusion should derive the important facts out of your work and results that you obtained.
%=== ZAVER ====
%\begin{itemize} 
%\item   toto je otvorene
%\item   toto vyzera tazko
%\item   tymto by sme sa zaoberali dalej (Future work) 
%\end{itemize} 

\section*{Conclusion}
\markboth{CONCLUSION}{}
\addcontentsline{toc}{section}{Conclusion}
\label{sec:conclusion} 

In our work, we derived TLR, a modification of BAL, which increased the success rate of BAL from 62.7\% to 93.1\% on the 4-2-4 encoder task. We observed that BAL converges rapidly to the state, when the backward and forward activations become the same. This has become our primary hypothesis to explain why BAL had problems learning the 4-2-4 encoder task. Our hypothesis was proven by candidate selection, which selected the network with more distant hidden activations. This increased the success rate from 93.1\% to 99.84\% and reduced the needed epochs from 5845 to 150. Then we applied TLR on the handwritten digit recognition task with architecture 784-300-10. Although TLR still has a performance gap on backpropagation, it achieved far better success rate than original BAL. %(TODO sim) 

We experimented with many different modifications of BAL, notably BAL-recirc with 43.2\%, other GeneRec learning rules with 0\%, dropout with 0\% and multilayer GeneRec with 42\% success rate. Standard modifications, such as batch training mode or adding momentum, showed no tendency in increasing the success rate of TLR. We admit that there is a space for improvement on these approaches.

\label{sec:future-work}
%What left unfinished from your work? What are you future plans to develop better your work?
Our work opened several ways to continue analysis of BAL. For instance, it is possible to create a dataset where the initial weights are the training features for successful networks, as discussed in section~\ref{sec:our-weight-init-class}. Other possibility is to use four learning rates, one for each matrix, or dynamic learning rates for each connection as outlined in section~\ref{sec:our-dynamic-lambda}. Furthermore, we recommend analysing backward activations which showed counterintuitive behaviour in figure~\ref{fig:results-tlr-auto4-epoch}. Finally, TLR introduced one more parameter which needs to be set up, therefore a method for finding best pair of $\lambda_v$ and $\lambda_h$ would be usefull. 

%TODO candidate selection on higher dimensions 
%TODO symmetric BAL 

