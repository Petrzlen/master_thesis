\section*{Conclusion}
\markboth{CONCLUSION}{}
\addcontentsline{toc}{section}{Conclusion}
\label{sec:conclusion} 

Still, many work should be done. 


==ZAVER 
\begin{itemize} 
\item   spravili sme toto toto \\ 
hidden activations, tlr \ref{sec:tlr-auto4-hypothesis} 
We could introduce a hypothesis why TLR outperforms BAL on the \emph{4-2-4 encoder task}. The hypothesis is suggested by sections on hidden distances~(\ref{sec:tlr-auto4-hidden}) and~(\ref{sec:our-hidden-activation}) and on candidate selection~(\ref{fig:results-tlr-auto4-epoch}). The reason is that the hidden activations settle to fast. This is explained by the fact that forward and backward hidden activations become same to fast and moreove, the weight initialization could help this. These issues are solved by $\lambda_h \ll \lambda_h$ which adds epochs to the training phase and by candidate selection which prevents initializing hidden activations close to each other. \\
\\
half line hypothesis 
\item   toto je otvorene
\item   toto vyzera tazko
\item   tymto by sme sa zaoberali dalej (Future work) 

\end{itemize} 

%What left unfinished from your work? What are you future plans to develop better your work?

\subsection*{Future Work} 
Weight initialization: train on the dataset (initial weights + success)~(\ref{sec:results-sigma}) 
i.e. ako rozlisit uspesne a neuspesne od inicializacie (v batch mode), t.j. binarny klasifikator na good/bad vah . 

Try TLR also with BP.  \\ 

\label{sec:future-dlr} 
Dynamic learning rate~(\ref{sec:our-dynamic-lambda}). 

Tricks with BP~\citep{lecun2012efficient}. 

Try using also backward activations.


