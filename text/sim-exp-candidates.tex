\subsubsection{Candidate selection} 
\label{sec:sim-exp-candidates}

\paragraph{Introduction.} 
The \emph{candidate selection} model was used to test and prove if some particular network \emph{features}~(\ref{sec:our-candidates-features}) have an impact to the overall network performance. The only difference between standard BAL~(\ref{sec:models-bal}) is that before the training phase $N$ networks are randomly generated from which a \emph{best candidate network} is selected based on \emph{feature function} defined as $F: \mbox{network} \mapsto \mathbb{R}$. The feature function was trained on a \emph{feature dataset} containing individual features and a binary label \emph{success}. The dataset was generated by running standard BAL, measuring features before the run and adding the success label after the training. The notion of most important features was used to design other models which reinforced these features, e.g. TLR for the $dist_{hid}^{FB}$ feature. 

\begin{algorithm}[H]
%  \KwData{$candidates$ }
  \begin{algorithmic}
    \State $candidate \gets (\infty, null)$
    \For{$i = 1$ to $N$} 
      \State $gn \gets generate\_network$
      \State $can \gets (fitness(gn), gn)$
      \If{$can < candidate$}
        \State $candidate \gets can$
      \EndIf
    \EndFor
  \caption{Candidate selection pseudocode.}
  \label{alg:our-candidates-pseudocode} 
  \end{algorithmic}
\end{algorithm} 

\paragraph{Features.}
\label{sec:our-candidates-features}

We denote $X_I$, $H_I$ and $Y_I$ as the front, hidden and back activation \emph{vector}s for input $I$ (and the corresponding target). For the rest of the notation please consult \ref{tab:models-bal-activation}. We measured the following \emph{features}: 

\begin{itemize} 
\item $dist_{H}$ (real) -- the average distance between all hidden activations of the inputs, i.e.~$avg_{I \neq J}\left(dist(H_I^{+},H_J^{+})\right)$. %TODO forward / backward! 

\label{sec:our-dist-h-fb}
\item $dist_{H}^{FB}$ (real) -- the average distance between corresponding forward and backward hidden activations, i.e.~$avg_{I}\left(dist(H_I^{-},H_I^{+})\right)$.

\item	$dist_{O}^{FB}$ (real)-- the average distance between corresponding forward and backward output activations, i.e.~  $avg_{I}\left(dist(Y_I^{-},X_I^{+})\right)$. Note that this feature is only relevant for auto--associative tasks such as 4-2-4 encoder~(\ref{sec:datasets-auto4}). 

\item $matrix\_weight$ (real) -- average weight of the network, i.e. average value of all weight matrices $W^{IH}$, $W^{HO}$, $W^{OH}$ and $W^{HI}$. Note that each matrix value has the same impact to \emph{matrix\_weight}. 

\item $matrix\_sim$ (real) -- similarity of the opposite matrices, i.e~sum of $|A_{ij} - B_{ij}|$ per pairs $(A,B) \in ((W^{IH}, W^{OH}),\, (W^{HI}, W^{HO}))$. Note that this feature is only relevant for auto--associative tasks such as 4-2-4 encoder~(\ref{sec:datasets-auto4}) and especially BAL~(\ref{sec:models-bal}) where this similarity tends to 0 as analysed in~(\ref{sec:results-auto4-bal-matrix-sim}). 

\label{sec:our-in-triangle}
\item $in\_triangle$ (bool) -- check if hidden activations of inputs  $H_I^{+}$ form a convex polygon, i.e.~if the hidden activation are lineary separable~(\ref{sec:models-perceptron}). Consult figure~\ref{fig:results-hidden-activations-bal} for examples of convex and non--convex hidden activations. Note that this was implemented only for hidden size equal to two. %TODO add stats on the implication 

\label{eq:our-fluctuation}
\item $fluctuation$ (real) -- the maximal difference between the last two unit activations when using the \emph{iterative method} for activation computation \ref{eq:models-generec-activation}. With other words, let $a_i(t)$ be the activation of unit $i$ in iteration $t$ and $T$ be number of iterations. Then $fluctuation$ is i.e.~$w_IH$ to $w_HI$ and $w_HO$ to $w_OH$, as in GeneRec activation computation \ref{eq:models-generec-activation} then $fluctuation$ measures $max_i|a_i(T-1) - a_i(T)|$. So if $fluctuation \approx 0$ then the iterative method was successful and all activations settled.
\end{itemize} 

Also all \emph{parameters} of TLR were included such as $\lambda_v$, $\lambda_h$~(\ref{sec:our-tlr}), momentum $\mu$~(\ref{sec:our-momentum}) and weight distribution $\sigma$~(\ref{sec:our-sigma}). Then we trained a linear regression model over the network measures with label $1-bitSucc^F = bitErr^F$. In equation~(\ref{eq:results-candidates-linear-regression}) the resulting linear model for feature subset is shown. %%%TODO 

\begin{align} 
\label{eq:results-candidates-linear-regression} 
bitErr^F &= 
- 0.328 \times dist_{H}
+ 0.140 \times dist_{H}^{FB}
- 0.100 \times dist_{O}^{FB} \nonumber \\
&+ 0.019 \times matrix\_sim
- 0.127 \times \sigma
+ 0.000 \times matrix\_weight
+ 3.610
\end{align}  

\paragraph{Results.}
\label{sec:results-candidates} 

\ref{fig:results-tlr-auto4-epoch} 


