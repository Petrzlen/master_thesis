\subsubsection{Hopfield networks}
\label{sec:theory-hopfield}

\citet{hopfield1984neurons} introduced a network with arbitrary connections defined only by one weight matrix $W$. Some of the units are chosen as the \emph{input units} which have stable activations for a given input pattern. We can treat a Hopfield network as a recurrent neural network. A Hopfield network comes with a continuous energy function for which usually function~(\ref{eq:theory-hopfield-energy}) is chosen: 
\begin{equation}
  \label{eq:theory-hopfield-energy}
  E = -\frac{1}{2}\sum_i\sum_ja_iw_{ij}a_j,
\end{equation} 
where $a_i$ is the activation of the $i$-th unit. The aim of the network is to settle the activations so that $E$ settles in a global minima. Activation for the $i$-th unit is computed based on the following differential equation~\citep{hopfield1984neurons}: 
\begin{equation}
  \label{eq:theory-hopfield-activation}
  \frac{\partial a_i}{\partial t} = \alpha(-a_i + f_i(\eta_i)),
\end{equation} 
where $a^T = [a_1,\ldots,a_n]$ is the activation vector, $f_i$ is bounded, monotically increasing, differentiable activation function. \citet{hopfield1984neurons} proved for equation~(\ref{eq:theory-hopfield-activation}) that if the weights are symmetric, i.e. $w_{ij} = w_{ji}$, the activations will settle in the minimal error state defined in equation~(\ref{eq:theory-hopfield-activation}). This learning rule is typically used in \emph{interactive activation networks} studied by~\citet{grossberg1978theory} and~\citet{mcclelland1981interactive}. 

