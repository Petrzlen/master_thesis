% The theory and concepts of your work. For example, if you work on compiler you can mention about how compiler works without getting much into detail.

\newcommand{\argmin}{\operatornamewithlimits{arg\,min}}
\newcommand{\Bx}{{\bf x}}
\newcommand{\By}{{\bf y}}
\newcommand{\Bh}{{\bf h}}
\newcommand{\Bw}{{\bf w}}
\newcommand{\Bc}{{\bf c}}

In this section we will describe the basics of artificial neural networks. We will also introduce the notation used in this work. Note that the definitions and notations vary through the literature. We use the one which the author is familiar with. For the reader who is comfortable with this topic we recommend to skip this section and go to related models \ref{overview-models}. 

\input{theory-perceptron} 
\label{sec:perceptron} 

%=============================================================
\subsubsection{Multilayer Feedworward Networks} 
\label{sec:multilayer} 

We will define multilayer feedforward networks as in \citet{haykin1994neural}. First, we define a \emph{layered} neural network where neurons are organised to form layers. In the simplest version we have an \emph{input layer} of source nodes and an \emph{output layer} which is formed by aforementioned perceptrons. In other words this is a \emph{feedforward} or \emph{acyclic} type of network as the \emph{activation}, i.e. outputs of the neurons are computed from the input to the output layer and never \emph{backwards}. 

Multilayer neural network has one or more \emph{hidden layers} in addition to the input and ouput layer as shown on figure~\ref{fig:multilayer}: 

\begin{figure}[h]
  \centering
  \includegraphics[width=0.6\textwidth]{img/multilayer.pdf}    
  \caption{Fully connected feedforward multilayer with one hidden layer and one output layer. The source nodes supply the activation pattern, i.e. input vector, which is applied to second layer neurons. The output signal of the hidden layer is used as the input for the output layer. As shown by \citet{cybenko1989approximation} the three layer network is an universal approximator continuous functions on compact subsets of $\mathbb{R}^n$.} 
  \label{fig:multilayer}
\end{figure}

There exists several methods for training multilayer networks. First, we will describe the most common Backpropagation in \ref{models-bp} and then methods related to our work such as CHL \ref{models-chl}, GeneRec \ref{models-generec} and BAL \ref{models-bal}. 
